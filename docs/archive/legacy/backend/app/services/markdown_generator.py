"""
Markdown çŸ¥è­˜åº«ç”Ÿæˆå™¨
å°‡å·²æ‰¹å‡†çš„å°è©±è½‰æ›ç‚º Markdown æª”æ¡ˆ
"""
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
from collections import defaultdict


class MarkdownGenerator:
    """Markdown çŸ¥è­˜åº«ç”Ÿæˆå™¨"""

    def __init__(self, output_dir: str = "../knowledge-base"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def generate_from_conversations(
        self,
        conversations: List[Dict[str, Any]],
        category: Optional[str] = None
    ) -> Dict[str, str]:
        """
        å¾å°è©±åˆ—è¡¨ç”Ÿæˆ Markdown æª”æ¡ˆ

        Args:
            conversations: å°è©±è³‡æ–™åˆ—è¡¨
            category: æŒ‡å®šåˆ†é¡ï¼ˆNone = æŒ‰åˆ†é¡è‡ªå‹•åˆ†çµ„ï¼‰

        Returns:
            {
                "ç”¢å“åŠŸèƒ½.md": "/path/to/ç”¢å“åŠŸèƒ½.md",
                "æŠ€è¡“æ”¯æ´.md": "/path/to/æŠ€è¡“æ”¯æ´.md",
                ...
            }
        """
        if category:
            # ç”Ÿæˆå–®ä¸€åˆ†é¡æª”æ¡ˆ
            filtered = [c for c in conversations if c.get("primary_category") == category]
            if filtered:
                file_path = self._generate_category_file(category, filtered)
                return {f"{category}.md": str(file_path)}
            return {}
        else:
            # æŒ‰åˆ†é¡åˆ†çµ„ç”Ÿæˆå¤šå€‹æª”æ¡ˆ
            grouped = self._group_by_category(conversations)
            result = {}

            for cat, convs in grouped.items():
                if cat:  # å¿½ç•¥ç„¡åˆ†é¡çš„
                    file_path = self._generate_category_file(cat, convs)
                    result[f"{cat}.md"] = str(file_path)

            return result

    def _group_by_category(
        self,
        conversations: List[Dict[str, Any]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """æŒ‰åˆ†é¡åˆ†çµ„å°è©±"""
        grouped = defaultdict(list)

        for conv in conversations:
            category = conv.get("primary_category", "å…¶ä»–")
            grouped[category].append(conv)

        return dict(grouped)

    def _generate_category_file(
        self,
        category: str,
        conversations: List[Dict[str, Any]]
    ) -> Path:
        """ç”Ÿæˆå–®ä¸€åˆ†é¡çš„ Markdown æª”æ¡ˆ"""
        # æª”æ¡ˆè·¯å¾‘
        file_path = self.output_dir / f"{category}.md"

        # æ”¶é›†å…ƒæ•¸æ“š
        tags = set()
        for conv in conversations:
            tags.update(conv.get("tags", []))

        # ç”Ÿæˆ Markdown å…§å®¹
        content = self._format_markdown(
            category=category,
            conversations=conversations,
            tags=list(tags)
        )

        # å¯«å…¥æª”æ¡ˆ
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)

        return file_path

    def _format_markdown(
        self,
        category: str,
        conversations: List[Dict[str, Any]],
        tags: List[str]
    ) -> str:
        """æ ¼å¼åŒ– Markdown å…§å®¹"""
        lines = []

        # æ¨™é¡Œ
        lines.append(f"# {category}\n")

        # å…ƒæ•¸æ“š
        lines.append("> **å…ƒæ•¸æ“š**")
        lines.append(f"> - æ›´æ–°æ—¥æœŸï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"> - è³‡æ–™ç­†æ•¸ï¼š{len(conversations)}")
        lines.append(f"> - ä¾†æºï¼šLINE å°è©±æ•´ç†")
        if tags:
            lines.append(f"> - æ¨™ç±¤ï¼š{', '.join(tags)}")
        lines.append("")
        lines.append("---\n")

        # å…§å®¹
        for i, conv in enumerate(conversations, 1):
            # æå–æ¸…ç†å¾Œçš„å…§å®¹
            qa = self._extract_qa(conv)

            if qa:
                # Q&A æ ¼å¼
                lines.append(f"## {i}. {qa['question']}\n")
                lines.append(qa["answer"])
                lines.append("")

                # ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœ‰ï¼‰
                if qa.get("context"):
                    lines.append(f"> **ä¸Šä¸‹æ–‡**ï¼š{qa['context']}")
                    lines.append("")

                # æ¨™ç±¤
                if conv.get("tags"):
                    lines.append(f"**æ¨™ç±¤**ï¼š{', '.join(conv['tags'])}")

                # ä¾†æºè³‡è¨Š
                lines.append(f"**ä¾†æº**ï¼šå°è©± ID `{conv.get('id')}`  ")
                if conv.get("quality_score"):
                    lines.append(f"**å“è³ªåˆ†æ•¸**ï¼š{conv['quality_score']}/10  ")
                if conv.get("confidence_score"):
                    lines.append(f"**ä¿¡å¿ƒåº¦**ï¼š{conv['confidence_score']:.2f}  ")

                lines.append("")
                lines.append("---\n")

        return "\n".join(lines)

    def _extract_qa(self, conversation: Dict[str, Any]) -> Optional[Dict[str, str]]:
        """å¾å°è©±ä¸­æå– Q&A"""
        # å„ªå…ˆä½¿ç”¨ AI æ¸…ç†å¾Œçš„å…§å®¹
        processed = conversation.get("processed_content", {})
        cleaned = processed.get("cleaned", {})

        if cleaned:
            return {
                "question": cleaned.get("question", ""),
                "answer": cleaned.get("answer", ""),
                "context": cleaned.get("context", "")
            }

        # å‚™ç”¨ï¼šå¾åŸå§‹å°è©±æå–
        raw = conversation.get("raw_content", {})
        messages = raw.get("messages", [])

        if not messages:
            return None

        # ç°¡å–®æå–ï¼šç¬¬ä¸€æ¢ç‚ºå•é¡Œï¼Œå…¶ä»–ç‚ºç­”æ¡ˆ
        question = messages[0].get("message", "") if messages else ""
        answers = [m.get("message", "") for m in messages[1:]]
        answer = "\n\n".join(answers)

        return {
            "question": question,
            "answer": answer,
            "context": ""
        }

    def generate_index(self, file_paths: Dict[str, str]) -> Path:
        """
        ç”Ÿæˆç´¢å¼•æª”æ¡ˆï¼ˆREADME.mdï¼‰

        Args:
            file_paths: {åˆ†é¡: æª”æ¡ˆè·¯å¾‘}

        Returns:
            ç´¢å¼•æª”æ¡ˆè·¯å¾‘
        """
        index_path = self.output_dir / "README.md"

        lines = [
            "# AIChatbot çŸ¥è­˜åº«\n",
            f"> æ›´æ–°æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
            "## ğŸ“š åˆ†é¡ç›®éŒ„\n"
        ]

        # åˆ—å‡ºæ‰€æœ‰åˆ†é¡
        for category, path in sorted(file_paths.items()):
            filename = os.path.basename(path)
            lines.append(f"- [{category}](./{filename})")

        lines.append("\n---\n")
        lines.append("## ğŸ“– ä½¿ç”¨èªªæ˜\n")
        lines.append("æœ¬çŸ¥è­˜åº«ç”± AIChatbot å¾Œå°ç®¡ç†ç³»çµ±è‡ªå‹•ç”Ÿæˆã€‚")
        lines.append("å…§å®¹ä¾†æºæ–¼ LINE å°è©±è¨˜éŒ„ï¼Œç¶“é AI è™•ç†å’Œäººå·¥å¯©æ ¸ã€‚\n")

        lines.append("### è³‡æ–™æ ¼å¼\n")
        lines.append("æ¯å€‹åˆ†é¡æª”æ¡ˆåŒ…å«ï¼š")
        lines.append("- å•é¡Œå’Œç­”æ¡ˆï¼ˆQ&A æ ¼å¼ï¼‰")
        lines.append("- ç›¸é—œæ¨™ç±¤")
        lines.append("- å“è³ªåˆ†æ•¸å’Œä¿¡å¿ƒåº¦")
        lines.append("- ä¾†æºè¿½æº¯è³‡è¨Š\n")

        lines.append("### æ›´æ–°é »ç‡\n")
        lines.append("- å»ºè­°ï¼šæ¯é€±æ›´æ–°ä¸€æ¬¡")
        lines.append("- æˆ–ç•¶æœ‰æ–°çš„å·²æ‰¹å‡†å°è©±æ™‚æ‰‹å‹•è§¸ç™¼\n")

        content = "\n".join(lines)

        with open(index_path, "w", encoding="utf-8") as f:
            f.write(content)

        return index_path

    def cleanup_old_files(self, keep_categories: List[str]):
        """
        æ¸…ç†ä¸å†éœ€è¦çš„èˆŠæª”æ¡ˆ

        Args:
            keep_categories: è¦ä¿ç•™çš„åˆ†é¡åˆ—è¡¨
        """
        keep_files = {f"{cat}.md" for cat in keep_categories}
        keep_files.add("README.md")
        keep_files.add("_meta.json")

        # åˆªé™¤ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­çš„ .md æª”æ¡ˆ
        for file_path in self.output_dir.glob("*.md"):
            if file_path.name not in keep_files:
                file_path.unlink()
                print(f"å·²åˆªé™¤èˆŠæª”æ¡ˆ: {file_path.name}")


# ========== ä½¿ç”¨ç¯„ä¾‹ ==========

if __name__ == "__main__":
    # æ¸¬è©¦è³‡æ–™
    test_conversations = [
        {
            "id": "test-001",
            "primary_category": "ç”¢å“åŠŸèƒ½",
            "tags": ["ç™»å…¥", "å¸³æˆ¶"],
            "quality_score": 8,
            "confidence_score": 0.92,
            "processed_content": {
                "cleaned": {
                    "question": "å¦‚ä½•é‡è¨­å¯†ç¢¼ï¼Ÿ",
                    "answer": "é‡è¨­å¯†ç¢¼çš„æ­¥é©Ÿå¦‚ä¸‹ï¼š\n1. é»é¸å³ä¸Šè§’ã€è¨­å®šã€\n2. é¸æ“‡ã€å¸³æˆ¶å®‰å…¨ã€\n3. é»æ“Šã€è®Šæ›´å¯†ç¢¼ã€\n4. è¼¸å…¥æ–°å¯†ç¢¼ä¸¦ç¢ºèª",
                    "context": "å°è©±æ™‚é–“ï¼š2024-01-15 14:30"
                }
            }
        },
        {
            "id": "test-002",
            "primary_category": "ç”¢å“åŠŸèƒ½",
            "tags": ["åŠŸèƒ½", "ä½¿ç”¨"],
            "quality_score": 9,
            "confidence_score": 0.95,
            "processed_content": {
                "cleaned": {
                    "question": "å¦‚ä½•ä¸Šå‚³æª”æ¡ˆï¼Ÿ",
                    "answer": "ä¸Šå‚³æª”æ¡ˆå¾ˆç°¡å–®ï¼š\n1. é»é¸ã€ä¸Šå‚³ã€æŒ‰éˆ•\n2. é¸æ“‡æª”æ¡ˆï¼ˆæ”¯æ´ PDF, JPG, PNGï¼‰\n3. é»æ“Šã€ç¢ºèªä¸Šå‚³ã€å³å¯",
                    "context": ""
                }
            }
        },
        {
            "id": "test-003",
            "primary_category": "æŠ€è¡“æ”¯æ´",
            "tags": ["éŒ¯èª¤", "æ’é™¤"],
            "quality_score": 7,
            "confidence_score": 0.85,
            "processed_content": {
                "cleaned": {
                    "question": "ç„¡æ³•ç™»å…¥æ€éº¼è¾¦ï¼Ÿ",
                    "answer": "è«‹å˜—è©¦ä»¥ä¸‹æ–¹æ³•ï¼š\n1. æª¢æŸ¥å¸³è™Ÿå¯†ç¢¼æ˜¯å¦æ­£ç¢º\n2. æ¸…é™¤ç€è¦½å™¨å¿«å–\n3. å˜—è©¦é‡è¨­å¯†ç¢¼\n4. å¦‚ä»ç„¡æ³•è§£æ±ºï¼Œè«‹è¯ç¹«å®¢æœ",
                    "context": "å¸¸è¦‹å•é¡Œ"
                }
            }
        }
    ]

    # ç”Ÿæˆ Markdown
    generator = MarkdownGenerator(output_dir="./test_knowledge_base")

    # ç”Ÿæˆæ‰€æœ‰åˆ†é¡æª”æ¡ˆ
    file_paths = generator.generate_from_conversations(test_conversations)

    print("=== ç”Ÿæˆçš„æª”æ¡ˆ ===")
    for category, path in file_paths.items():
        print(f"{category}: {path}")

    # ç”Ÿæˆç´¢å¼•
    index_path = generator.generate_index(file_paths)
    print(f"\nç´¢å¼•æª”æ¡ˆ: {index_path}")

    print("\nâœ… Markdown ç”Ÿæˆå®Œæˆï¼")
