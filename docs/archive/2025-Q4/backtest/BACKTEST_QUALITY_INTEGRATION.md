# å›æ¸¬ç³»çµ±å“è³ªå¢å¼·æ•´åˆæ–¹æ¡ˆ

**ç›®æ¨™ï¼š** å°‡ `test_scoring_quality.py` çš„é€²éšè©•ä¼°åŠŸèƒ½æ•´åˆåˆ°ç¾æœ‰çš„ `/backtest` å›æ¸¬ç³»çµ±ä¸­

---

## ğŸ“‹ ç¾ç‹€åˆ†æ

### ç¾æœ‰å›æ¸¬ç³»çµ±

**ä½ç½®ï¼š** `scripts/knowledge_extraction/backtest_framework.py`

**è©•ä¼°æ–¹å¼ï¼š**
```python
def evaluate_answer(self, test_scenario, system_response):
    # 1. åˆ†é¡åŒ¹é…æª¢æŸ¥ (0.3åˆ†)
    category_match = (expected_category == actual_intent)

    # 2. é—œéµå­—è¦†è“‹ç‡ (0.4åˆ†)
    keyword_ratio = matched_keywords / total_keywords

    # 3. ä¿¡å¿ƒåº¦æª¢æŸ¥ (0.3åˆ†)
    confidence >= 0.7

    # ç¸½åˆ† >= 0.6 è¦–ç‚ºé€šé
```

**å„ªé»ï¼š**
- âœ… å¿«é€ŸåŸ·è¡Œ
- âœ… ä¸éœ€è¦é¡å¤– API å‘¼å«
- âœ… æ”¯æ´å¤šæ„åœ–æª¢æ¸¬
- âœ… æœ‰ç›´è§€çš„å„ªåŒ–å»ºè­°

**é™åˆ¶ï¼š**
- âš ï¸ ç„¡æ³•è©•ä¼°ç­”æ¡ˆçœŸå¯¦å“è³ª
- âš ï¸ é—œéµå­—åŒ¹é…éæ–¼ç°¡å–®
- âš ï¸ ç„¡æ³•è©•ä¼°å®Œæ•´æ€§
- âš ï¸ ç„¡æ³•è©•ä¼°æ’åºå“è³ª (NDCG)

---

### æ–°å»ºå“è³ªæ¸¬è©¦ç³»çµ±

**ä½ç½®ï¼š** `test_scoring_quality.py`

**è©•ä¼°æ–¹å¼ï¼š**
```python
async def evaluate_answer_quality_with_llm(
    self, question, answer, expected_intent
):
    # ä½¿ç”¨ GPT-4o-mini è©•ä¼°ï¼š
    # 1. ç›¸é—œæ€§ (Relevance): 1-5
    # 2. å®Œæ•´æ€§ (Completeness): 1-5
    # 3. æº–ç¢ºæ€§ (Accuracy): 1-5
    # 4. æ„åœ–åŒ¹é… (Intent Match): 1-5
    # 5. ç¶œåˆè©•åˆ† (Overall): 1-5

# NDCG æ’åºå“è³ªè©•ä¼°
ndcg = calculate_ndcg(results, relevance_scores, k=3)
```

**å„ªé»ï¼š**
- âœ… çœŸå¯¦è©•ä¼°ç­”æ¡ˆå“è³ª
- âœ… æä¾›è©³ç´°ç†ç”±
- âœ… è¨ˆç®— NDCG æ’åºæŒ‡æ¨™
- âœ… è­˜åˆ¥ã€Œæ’åºæ‚–è«–ã€å•é¡Œ

**é™åˆ¶ï¼š**
- âš ï¸ éœ€è¦ OpenAI API è²»ç”¨
- âš ï¸ åŸ·è¡Œæ™‚é–“è¼ƒé•·
- âš ï¸ éœ€è¦æ§åˆ¶é€Ÿç‡é™åˆ¶

---

## ğŸ¯ æ•´åˆæ–¹æ¡ˆ

### **æ–¹æ¡ˆ Aï¼šé›™æ¨¡å¼å›æ¸¬ï¼ˆæ¨è–¦ï¼‰**

åœ¨ç¾æœ‰å›æ¸¬ä¸­æ–°å¢**è©³ç´°æ¨¡å¼**é¸é …ï¼š

```python
class BacktestFramework:
    def __init__(
        self,
        base_url: str = "http://localhost:8100",
        vendor_id: int = 1,
        quality_mode: str = "basic"  # â† æ–°å¢åƒæ•¸
    ):
        self.quality_mode = quality_mode
        # basic: ç¾æœ‰è©•ä¼°
        # detailed: LLM å“è³ªè©•ä¼°
        # hybrid: å…©è€…çµåˆ
```

#### åŸ·è¡Œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å›æ¸¬é–‹å§‹                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸºç¤è©•ä¼° (Basic)                     â”‚
â”‚ â€¢ åˆ†é¡åŒ¹é…                           â”‚
â”‚ â€¢ é—œéµå­—è¦†è“‹                          â”‚
â”‚ â€¢ ä¿¡å¿ƒåº¦æª¢æŸ¥                          â”‚
â”‚ â€¢ åŸ·è¡Œé€Ÿåº¦å¿«                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
      [è³ªé‡æ¨¡å¼é¸æ“‡]
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚
        â†“               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Basic Mode   â”‚  â”‚ Detailed Mode   â”‚
â”‚ å®Œæˆ         â”‚  â”‚ ç¹¼çºŒ LLM è©•ä¼°    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ LLM å“è³ªè©•ä¼°         â”‚
              â”‚ â€¢ ç›¸é—œæ€§ (1-5)       â”‚
              â”‚ â€¢ å®Œæ•´æ€§ (1-5)       â”‚
              â”‚ â€¢ æº–ç¢ºæ€§ (1-5)       â”‚
              â”‚ â€¢ æ„åœ–åŒ¹é… (1-5)     â”‚
              â”‚ â€¢ NDCG è¨ˆç®—         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ å®Œæ•´å ±å‘Š             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **å¯¦æ–½ç´°ç¯€**

#### 1. æ“´å±• `backtest_framework.py`

```python
# æ–°å¢æ–¹æ³•
async def evaluate_answer_quality_detailed(
    self,
    test_scenario: Dict,
    system_response: Dict
) -> Dict:
    """ä½¿ç”¨ LLM é€²è¡Œè©³ç´°å“è³ªè©•ä¼°"""

    # 1. åŸ·è¡ŒåŸºç¤è©•ä¼°ï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
    basic_eval = self.evaluate_answer(test_scenario, system_response)

    # 2. å¦‚æœæ˜¯ detailed æˆ– hybrid æ¨¡å¼ï¼ŒåŸ·è¡Œ LLM è©•ä¼°
    if self.quality_mode in ['detailed', 'hybrid']:
        question = test_scenario.get('test_question', '')
        answer = system_response.get('answer', '')
        expected_intent = test_scenario.get('expected_category', '')

        # å‘¼å« LLM è©•ä¼°
        quality_eval = await self.llm_evaluate_answer(
            question, answer, expected_intent
        )

        # 3. æ•´åˆè©•ä¼°çµæœ
        return {
            'basic_eval': basic_eval,
            'quality_eval': quality_eval,
            'overall_score': self._calculate_hybrid_score(basic_eval, quality_eval),
            'passed': self._determine_pass_status(basic_eval, quality_eval)
        }

    return {'basic_eval': basic_eval, 'passed': basic_eval['passed']}

async def llm_evaluate_answer(
    self,
    question: str,
    answer: str,
    expected_intent: str
) -> Dict:
    """ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆå“è³ª"""

    prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å•ç­”çš„å“è³ªï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€ä½³ï¼‰ï¼š

å•é¡Œï¼š{question}
é æœŸæ„åœ–ï¼š{expected_intent}
ç­”æ¡ˆï¼š{answer}

è«‹å¾ä»¥ä¸‹ç¶­åº¦è©•åˆ†ï¼š
1. ç›¸é—œæ€§ (Relevance): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”å•é¡Œï¼Ÿ
2. å®Œæ•´æ€§ (Completeness): ç­”æ¡ˆæ˜¯å¦å®Œæ•´æ¶µè“‹å•é¡Œæ‰€å•ï¼Ÿ
3. æº–ç¢ºæ€§ (Accuracy): ç­”æ¡ˆå…§å®¹æ˜¯å¦æº–ç¢ºå¯é ï¼Ÿ
4. æ„åœ–åŒ¹é… (Intent Match): ç­”æ¡ˆæ˜¯å¦ç¬¦åˆé æœŸæ„åœ–ï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
{{
    "relevance": <1-5>,
    "completeness": <1-5>,
    "accuracy": <1-5>,
    "intent_match": <1-5>,
    "overall": <1-5>,
    "reasoning": "ç°¡çŸ­èªªæ˜è©•åˆ†ç†ç”±"
}}"""

    try:
        response = self.openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.3
        )

        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸  LLM è©•ä¼°å¤±æ•—: {e}")
        return {
            'relevance': 0,
            'completeness': 0,
            'accuracy': 0,
            'intent_match': 0,
            'overall': 0,
            'reasoning': f"è©•ä¼°å¤±æ•—: {str(e)}"
        }

def _calculate_hybrid_score(self, basic_eval: Dict, quality_eval: Dict) -> float:
    """è¨ˆç®—æ··åˆè©•åˆ†"""
    # æ¬Šé‡ï¼šåŸºç¤è©•åˆ† 40%ï¼ŒLLM è©•åˆ† 60%
    basic_score = basic_eval['score']
    quality_score = quality_eval['overall'] / 5.0  # æ¨™æº–åŒ–åˆ° 0-1

    return 0.4 * basic_score + 0.6 * quality_score

def _determine_pass_status(self, basic_eval: Dict, quality_eval: Dict) -> bool:
    """åˆ¤å®šæ˜¯å¦é€šé"""
    hybrid_score = self._calculate_hybrid_score(basic_eval, quality_eval)

    # æ··åˆæ¨¡å¼ï¼šä»»ä¸€æ–¹å¼é€šéå³å¯ï¼Œä½†ç¶œåˆåˆ†æ•¸ä½æ–¼ 0.5 å‰‡å¤±æ•—
    if self.quality_mode == 'hybrid':
        return (basic_eval['passed'] or quality_eval['overall'] >= 3) and hybrid_score >= 0.5

    # è©³ç´°æ¨¡å¼ï¼šä»¥ LLM è©•ä¼°ç‚ºæº–
    if self.quality_mode == 'detailed':
        return quality_eval['overall'] >= 3 and quality_eval['completeness'] >= 3

    # åŸºç¤æ¨¡å¼ï¼šç¾æœ‰é‚è¼¯
    return basic_eval['passed']
```

#### 2. æ–°å¢ NDCG è¨ˆç®—

```python
def calculate_ndcg_for_results(self, results: List[Dict]) -> Dict:
    """è¨ˆç®—æ‰€æœ‰æ¸¬è©¦çš„å¹³å‡ NDCG"""

    import math

    def calculate_ndcg(relevance_scores: List[float], k: int = 3) -> float:
        """è¨ˆç®—å–®å€‹æ¸¬è©¦çš„ NDCG@K"""
        if not relevance_scores or len(relevance_scores) == 0:
            return 0.0

        # DCG
        dcg = 0.0
        for i, score in enumerate(relevance_scores[:k], 1):
            dcg += (2 ** score - 1) / math.log2(i + 1)

        # IDCG
        ideal_scores = sorted(relevance_scores, reverse=True)[:k]
        idcg = 0.0
        for i, score in enumerate(ideal_scores, 1):
            idcg += (2 ** score - 1) / math.log2(i + 1)

        return dcg / idcg if idcg > 0 else 0.0

    # è¨ˆç®—æ¯å€‹æ¸¬è©¦çš„ NDCG
    ndcg_scores = []
    for result in results:
        if 'quality_eval' in result:
            # ä½¿ç”¨ LLM è©•ä¼°çš„ç›¸é—œæ€§åˆ†æ•¸
            relevance = result['quality_eval'].get('relevance', 0)
            ndcg_scores.append(relevance)

    if ndcg_scores:
        avg_ndcg = sum(ndcg_scores) / len(ndcg_scores) / 5.0  # æ¨™æº–åŒ–åˆ° 0-1
        return {
            'avg_ndcg': avg_ndcg,
            'count': len(ndcg_scores)
        }

    return {'avg_ndcg': 0.0, 'count': 0}
```

#### 3. æ›´æ–°å ±å‘Šç”Ÿæˆ

```python
def generate_report(self, results: List[Dict], output_path: str):
    """ç”Ÿæˆå¢å¼·ç‰ˆå›æ¸¬å ±å‘Š"""

    # ... ç¾æœ‰çµ±è¨ˆ ...

    # å¦‚æœæœ‰ LLM è©•ä¼°ï¼Œè¨ˆç®—é¡å¤–æŒ‡æ¨™
    has_quality_eval = any('quality_eval' in r for r in results)

    if has_quality_eval:
        # è¨ˆç®—å“è³ªåˆ†æ•¸
        quality_scores = [
            r['quality_eval'] for r in results
            if 'quality_eval' in r
        ]

        avg_relevance = sum(q['relevance'] for q in quality_scores) / len(quality_scores)
        avg_completeness = sum(q['completeness'] for q in quality_scores) / len(quality_scores)
        avg_accuracy = sum(q['accuracy'] for q in quality_scores) / len(quality_scores)
        avg_intent_match = sum(q['intent_match'] for q in quality_scores) / len(quality_scores)
        avg_overall = sum(q['overall'] for q in quality_scores) / len(quality_scores)

        # è¨ˆç®— NDCG
        ndcg_stats = self.calculate_ndcg_for_results(results)

        # è¼¸å‡ºåˆ°å ±å‘Š
        f.write("\n" + "="*60 + "\n")
        f.write("å“è³ªè©•ä¼°çµ±è¨ˆ (LLM)\n")
        f.write("="*60 + "\n")
        f.write(f"å¹³å‡ç›¸é—œæ€§ï¼š{avg_relevance:.2f}/5\n")
        f.write(f"å¹³å‡å®Œæ•´æ€§ï¼š{avg_completeness:.2f}/5\n")
        f.write(f"å¹³å‡æº–ç¢ºæ€§ï¼š{avg_accuracy:.2f}/5\n")
        f.write(f"å¹³å‡æ„åœ–åŒ¹é…ï¼š{avg_intent_match:.2f}/5\n")
        f.write(f"å¹³å‡ç¶œåˆè©•åˆ†ï¼š{avg_overall:.2f}/5\n")
        f.write(f"å¹³å‡ NDCGï¼š{ndcg_stats['avg_ndcg']:.3f}\n\n")
```

---

### **å‰ç«¯æ•´åˆ**

#### æ›´æ–° `BacktestView.vue`

```vue
<!-- æ–°å¢å“è³ªåˆ†æ•¸é¡¯ç¤º -->
<div v-if="statistics.quality_enabled" class="quality-stats">
  <h3>ğŸ“Š ç­”æ¡ˆå“è³ªçµ±è¨ˆ</h3>
  <div class="quality-grid">
    <div class="quality-item">
      <span class="label">ç›¸é—œæ€§</span>
      <span class="value">{{ statistics.avg_relevance }}/5</span>
    </div>
    <div class="quality-item">
      <span class="label">å®Œæ•´æ€§</span>
      <span class="value">{{ statistics.avg_completeness }}/5</span>
    </div>
    <div class="quality-item">
      <span class="label">æº–ç¢ºæ€§</span>
      <span class="value">{{ statistics.avg_accuracy }}/5</span>
    </div>
    <div class="quality-item">
      <span class="label">NDCG</span>
      <span class="value">{{ statistics.avg_ndcg }}</span>
    </div>
  </div>
</div>

<!-- åœ¨è©³æƒ… Modal ä¸­é¡¯ç¤ºå“è³ªè©•åˆ† -->
<div v-if="selectedResult.quality_eval" class="detail-section quality-section">
  <h3>ğŸ¯ LLM å“è³ªè©•ä¼°</h3>
  <table class="quality-table">
    <tr>
      <td>ç›¸é—œæ€§ (Relevance)</td>
      <td>
        <span class="score-stars">{{ getStars(selectedResult.quality_eval.relevance) }}</span>
        {{ selectedResult.quality_eval.relevance }}/5
      </td>
    </tr>
    <tr>
      <td>å®Œæ•´æ€§ (Completeness)</td>
      <td>
        <span class="score-stars">{{ getStars(selectedResult.quality_eval.completeness) }}</span>
        {{ selectedResult.quality_eval.completeness }}/5
      </td>
    </tr>
    <tr>
      <td>æº–ç¢ºæ€§ (Accuracy)</td>
      <td>
        <span class="score-stars">{{ getStars(selectedResult.quality_eval.accuracy) }}</span>
        {{ selectedResult.quality_eval.accuracy }}/5
      </td>
    </tr>
    <tr>
      <td>æ„åœ–åŒ¹é… (Intent Match)</td>
      <td>
        <span class="score-stars">{{ getStars(selectedResult.quality_eval.intent_match) }}</span>
        {{ selectedResult.quality_eval.intent_match }}/5
      </td>
    </tr>
  </table>
  <div class="quality-reasoning">
    <strong>è©•åˆ†ç†ç”±ï¼š</strong>
    <p>{{ selectedResult.quality_eval.reasoning }}</p>
  </div>
</div>
```

```javascript
methods: {
  getStars(score) {
    const fullStars = Math.floor(score);
    const halfStar = score % 1 >= 0.5 ? 1 : 0;
    const emptyStars = 5 - fullStars - halfStar;

    return 'â˜…'.repeat(fullStars) +
           (halfStar ? 'â˜†' : '') +
           'â˜†'.repeat(emptyStars);
  }
}
```

---

## ğŸš€ å¯¦æ–½æ­¥é©Ÿ

### éšæ®µ 1ï¼šå¾Œç«¯æ•´åˆï¼ˆ1-2 å¤©ï¼‰

1. âœ… åœ¨ `backtest_framework.py` ä¸­æ–°å¢ `quality_mode` åƒæ•¸
2. âœ… å¯¦ä½œ `llm_evaluate_answer()` æ–¹æ³•
3. âœ… å¯¦ä½œ `calculate_ndcg()` æ–¹æ³•
4. âœ… æ›´æ–° `generate_report()` è¼¸å‡ºå“è³ªæŒ‡æ¨™
5. âœ… æ–°å¢ç’°å¢ƒè®Šæ•¸æ§åˆ¶ï¼š
   ```bash
   BACKTEST_QUALITY_MODE=hybrid  # basic, detailed, hybrid
   ```

### éšæ®µ 2ï¼šAPI æ“´å±•ï¼ˆåŠå¤©ï¼‰

åœ¨ `knowledge-admin/backend/app.py` ä¸­ï¼š

```python
@app.get("/api/backtest/results")
async def get_backtest_results(
    status_filter: str = "all",
    limit: int = 50,
    offset: int = 0
):
    # ... ç¾æœ‰é‚è¼¯ ...

    # æ–°å¢å“è³ªçµ±è¨ˆ
    if has_quality_data:
        statistics['quality_enabled'] = True
        statistics['avg_relevance'] = calculate_avg('relevance')
        statistics['avg_completeness'] = calculate_avg('completeness')
        statistics['avg_accuracy'] = calculate_avg('accuracy')
        statistics['avg_ndcg'] = calculate_ndcg()

    return {
        'results': results,
        'total': total,
        'statistics': statistics
    }
```

### éšæ®µ 3ï¼šå‰ç«¯æ›´æ–°ï¼ˆ1 å¤©ï¼‰

1. âœ… æ›´æ–° `BacktestView.vue` é¡¯ç¤ºå“è³ªçµ±è¨ˆå¡ç‰‡
2. âœ… åœ¨æ¸¬è©¦è©³æƒ… Modal ä¸­é¡¯ç¤º LLM è©•åˆ†
3. âœ… æ–°å¢æ˜Ÿç´šè©•åˆ†é¡¯ç¤º
4. âœ… æ–°å¢å“è³ªè¶¨å‹¢åœ–è¡¨ï¼ˆå¯é¸ï¼‰

### éšæ®µ 4ï¼šæ¸¬è©¦èˆ‡å„ªåŒ–ï¼ˆåŠå¤©ï¼‰

1. âœ… åŸ·è¡Œå°è¦æ¨¡æ¸¬è©¦ï¼ˆ10-20 æ¢ï¼‰
2. âœ… é©—è­‰è©•åˆ†æº–ç¢ºæ€§
3. âœ… èª¿æ•´æ¬Šé‡å’Œé–¾å€¼
4. âœ… å„ªåŒ–åŸ·è¡Œé€Ÿåº¦ï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰

---

## ğŸ“ˆ é æœŸæ•ˆæœ

### åŸºç¤æ¨¡å¼ (Basic)
- **åŸ·è¡Œæ™‚é–“ï¼š** 50 æ¢æ¸¬è©¦ ~3-5 åˆ†é˜
- **æˆæœ¬ï¼š** $0
- **é©ç”¨å ´æ™¯ï¼š** æ—¥å¸¸å›æ¸¬ã€CI/CD

### è©³ç´°æ¨¡å¼ (Detailed)
- **åŸ·è¡Œæ™‚é–“ï¼š** 50 æ¢æ¸¬è©¦ ~10-15 åˆ†é˜
- **æˆæœ¬ï¼š** ~$0.50-1.00 (ä½¿ç”¨ GPT-4o-mini)
- **é©ç”¨å ´æ™¯ï¼š** æ·±åº¦åˆ†æã€ç³»çµ±å„ªåŒ–

### æ··åˆæ¨¡å¼ (Hybrid) - **æ¨è–¦**
- **åŸ·è¡Œæ™‚é–“ï¼š** 50 æ¢æ¸¬è©¦ ~8-12 åˆ†é˜
- **æˆæœ¬ï¼š** ~$0.50-1.00
- **é©ç”¨å ´æ™¯ï¼š** å¹³è¡¡é€Ÿåº¦å’Œå“è³ª

---

## ğŸ’° æˆæœ¬ä¼°ç®—

**ä½¿ç”¨ GPT-4o-mini**ï¼š
- è¼¸å…¥ï¼š~300 tokens/å•é¡Œ
- è¼¸å‡ºï¼š~150 tokens/å›ç­”
- å–®æ¬¡è©•ä¼°ï¼š~$0.0006 (300 Ã— 0.15 + 150 Ã— 0.6) / 1M
- 100 æ¢æ¸¬è©¦ï¼š~$0.06
- 500 æ¢æ¸¬è©¦ï¼š~$0.30

**éå¸¸åˆ’ç®—ï¼** âœ…

---

## ğŸ¯ é—œéµå„ªå‹¢

1. **å‘å¾Œç›¸å®¹** - ç¾æœ‰å›æ¸¬ä»å¯æ­£å¸¸é‹ä½œ
2. **æ¼¸é€²å¼æ¡ç”¨** - å¯é¸æ“‡å•Ÿç”¨è©³ç´°è©•ä¼°
3. **æˆæœ¬å¯æ§** - åªåœ¨éœ€è¦æ™‚ä½¿ç”¨ LLM
4. **æ·±åº¦æ´å¯Ÿ** - çœŸå¯¦è©•ä¼°ç­”æ¡ˆå“è³ª
5. **å¯æ“ä½œ** - æä¾›æ˜ç¢ºçš„å„ªåŒ–æ–¹å‘

---

## ğŸ“š ç›¸é—œæ–‡æª”

- [è©•åˆ†å“è³ªåˆ†æå ±å‘Š](./SCORING_QUALITY_ANALYSIS.md)
- [å›æ¸¬æ¡†æ¶åŸå§‹ç¢¼](../scripts/knowledge_extraction/backtest_framework.py)
- [å‰ç«¯å›æ¸¬ä»‹é¢](../knowledge-admin/frontend/src/views/BacktestView.vue)

---

**ä¸‹ä¸€æ­¥ï¼š** å¯¦æ–½éšæ®µ 1 - å¾Œç«¯æ•´åˆ

**é è¨ˆå®Œæˆæ™‚é–“ï¼š** 2-3 å¤©
