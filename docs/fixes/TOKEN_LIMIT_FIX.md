# OpenAI API Token è¶…é™å•é¡Œä¿®å¾©

**æ—¥æœŸ**: 2025-11-21
**å•é¡Œ**: æ–‡ä»¶è½‰æ›åŠŸèƒ½å‡ºç¾ context_length_exceeded éŒ¯èª¤
**ç‹€æ…‹**: âœ… å·²ä¿®å¾©

---

## å•é¡Œæè¿°

### éŒ¯èª¤ä¿¡æ¯
```
Error code: 400 - {'error': {'message': "This model's maximum context length is 16385 tokens. However, you requested 19489 tokens (15489 in the messages, 4000 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
```

### æ ¹æœ¬åŸå› 
1. **æ–‡ä»¶è½‰æ›æœå‹™** (`document_converter_service.py`) åœ¨å‘¼å« OpenAI API æ™‚æ²’æœ‰è¨­ç½® `max_tokens` é™åˆ¶
2. **è¼¸å…¥éå¤§**ï¼šæ–‡æª”å…§å®¹ + prompt = 15,489 tokens
3. **è¼¸å‡ºæœªé™åˆ¶**ï¼šç³»çµ±é è¨­å˜—è©¦ç”Ÿæˆ 4,000 tokens
4. **è¶…éæ¨¡å‹ä¸Šé™**ï¼šç¸½è¨ˆ 19,489 > 16,385 tokensï¼ˆæ¨¡å‹é™åˆ¶ï¼‰

### å½±éŸ¿ç¯„åœ
- **ä¸»è¦å½±éŸ¿**ï¼šé•·æ–‡æª”çš„è¦æ ¼æ›¸è½‰æ›åŠŸèƒ½
- **æ½›åœ¨å½±éŸ¿**ï¼šæ‰€æœ‰æœªè¨­ç½® `max_tokens` çš„ OpenAI API å‘¼å«

---

## ä¿®å¾©æ–¹æ¡ˆ

### 0. æ¨¡å‹é™åˆ¶å¸¸é‡å®šç¾©ï¼ˆä»£ç¢¼å„ªåŒ–ï¼‰

**æª”æ¡ˆ**: `rag-orchestrator/services/document_converter_service.py`

ç‚ºé¿å…é‡è¤‡å®šç¾©ï¼Œå°‡æ¨¡å‹ context é™åˆ¶å®šç¾©ç‚º**é¡å¸¸é‡**ï¼š

```python
class DocumentConverterService:
    # OpenAI æ¨¡å‹çš„ context é™åˆ¶ï¼ˆtokensï¼‰
    MODEL_CONTEXT_LIMITS = {
        'gpt-4o': 128000,
        'gpt-4o-mini': 128000,
        'gpt-4-turbo': 128000,
        'gpt-4': 8192,
        'gpt-3.5-turbo': 16385
    }
```

**å„ªé»**ï¼š
- âœ… éµå¾ª DRY åŸå‰‡ï¼ˆDon't Repeat Yourselfï¼‰
- âœ… å–®ä¸€æ•¸æ“šæºï¼ˆä¿®æ”¹ä¸€è™•å³å¯ï¼‰
- âœ… é¡å‹æ¸…æ™°ï¼ˆå¤§å¯«å‘½åè¡¨ç¤ºå¸¸é‡ï¼‰

---

### 1. å‹•æ…‹è¨ˆç®— max_tokensï¼ˆä¸»è¦ä¿®å¾©ï¼‰

**æª”æ¡ˆ**: `rag-orchestrator/services/document_converter_service.py`

#### ä¿®å¾©ä½ç½® 1: åˆ†æ®µå¤§å°å„ªåŒ–ï¼ˆline 220-243ï¼‰

**ä¿®æ”¹å‰**ï¼š
```python
max_chars = 12000  # å›ºå®šåˆ†æ®µå¤§å°ï¼Œç´„ 24K tokens
content_chunks = self._split_content(content, max_chars)
```

**ä¿®æ”¹å¾Œ**ï¼š
```python
# æ ¹æ“šæ¨¡å‹å‹•æ…‹èª¿æ•´åˆ†æ®µå¤§å°
# ä½¿ç”¨é¡å¸¸é‡ MODEL_CONTEXT_LIMITSï¼ˆå®šç¾©åœ¨é¡é–‹é ­ï¼Œé¿å…é‡è¤‡ï¼‰
max_context = self.MODEL_CONTEXT_LIMITS.get(self.model, 16385)

# æ ¹æ“šæ¨¡å‹å®¹é‡è¨ˆç®—å®‰å…¨çš„åˆ†æ®µå¤§å°
# é ç•™ 1000 tokens çµ¦ promptï¼Œ4000 tokens çµ¦è¼¸å‡ºï¼Œå‰©é¤˜çµ¦å…§å®¹
# ä¸­æ–‡ç´„ 1 å­— = 2 tokens
safe_input_tokens = max_context - 5000  # é ç•™ prompt + è¼¸å‡ºç©ºé–“
max_chars = int(safe_input_tokens / 2)  # è½‰æ›ç‚ºå­—å…ƒæ•¸

# é™åˆ¶ç¯„åœï¼šæœ€å°‘ 3000 å­—ï¼Œæœ€å¤š 50000 å­—
max_chars = max(3000, min(50000, max_chars))

print(f"   ğŸ“ æ¨¡å‹: {self.model} (ä¸Šé™ {max_context} tokens)")
print(f"   ğŸ“ åˆ†æ®µå¤§å°: {max_chars} å­—å…ƒ (ç´„ {max_chars * 2} tokens)")

content_chunks = self._split_content(content, max_chars)
```

**æ•ˆæœ**ï¼š
- `gpt-4` (8K): åˆ†æ®µ ~3,000 å­— (6K tokens)
- `gpt-3.5-turbo` (16K): åˆ†æ®µ ~5,600 å­— (11K tokens)
- `gpt-4o` (128K): åˆ†æ®µ ~50,000 å­— (100K tokens)

#### ä¿®å¾©ä½ç½® 2: Q&A æå– max_tokensï¼ˆline 368-401ï¼‰

**ä¿®æ”¹å‰**ï¼š
```python
response = client.chat.completions.create(
    model=self.model,
    messages=[...],
    temperature=0.3
    # æ²’æœ‰ max_tokens
)
```

**ä¿®æ”¹å¾Œ**ï¼š
```python
# è¨ˆç®—å®‰å…¨çš„ max_tokens
estimated_input_tokens = len(content) * 2 + 1000  # +1000 for system and prompt

# æ ¹æ“šæ¨¡å‹å‹•æ…‹è¨ˆç®—å¯ç”¨çš„è¼¸å‡º tokens
# ä½¿ç”¨é¡å¸¸é‡ MODEL_CONTEXT_LIMITSï¼ˆå®šç¾©åœ¨é¡é–‹é ­ï¼Œé¿å…é‡è¤‡ï¼‰
max_context = self.MODEL_CONTEXT_LIMITS.get(self.model, 16385)  # é è¨­ 16K

# è¨ˆç®—å¯ç”¨çš„è¼¸å‡º tokensï¼ˆä¿ç•™ 10% ç·©è¡ï¼‰
available_output_tokens = int((max_context - estimated_input_tokens) * 0.9)

# é™åˆ¶è¼¸å‡ºç¯„åœï¼šæœ€å°‘ 1000ï¼Œæœ€å¤š 4000
safe_max_tokens = max(1000, min(4000, available_output_tokens))

print(f"   ğŸ“Š Token ä¼°ç®—: è¼¸å…¥ ~{estimated_input_tokens}, è¼¸å‡ºä¸Šé™ {safe_max_tokens}")

response = client.chat.completions.create(
    model=self.model,
    messages=[...],
    temperature=0.3,
    max_tokens=safe_max_tokens  # è¨­ç½®å‹•æ…‹è¨ˆç®—çš„å®‰å…¨ä¸Šé™
)
```

#### ä¿®å¾©ä½ç½® 3: æ„åœ–æ¨è–¦ max_tokensï¼ˆline 622-628ï¼‰

**ä¿®æ”¹å‰**ï¼š
```python
response = client.chat.completions.create(
    model=self.model,
    temperature=0.3,
    response_format={"type": "json_object"},
    messages=[{"role": "user", "content": prompt}]
)
```

**ä¿®æ”¹å¾Œ**ï¼š
```python
response = client.chat.completions.create(
    model=self.model,
    temperature=0.3,
    max_tokens=500,  # æ„åœ–æ¨è–¦åªéœ€è¦å°é‡è¼¸å‡º
    response_format={"type": "json_object"},
    messages=[{"role": "user", "content": prompt}]
)
```

---

### 2. é é˜²æ€§ä¿®å¾©

**æª”æ¡ˆ**: `rag-orchestrator/services/knowledge_import_service.py`

ç‚ºäº†é é˜²é¡ä¼¼å•é¡Œï¼Œç‚ºæ‰€æœ‰ç¼ºå°‘ `max_tokens` çš„ API å‘¼å«æ·»åŠ é™åˆ¶ï¼š

#### ä¿®å¾©ä½ç½® 1: æ–‡æœ¬çŸ¥è­˜æå–ï¼ˆline 627-636ï¼‰
```python
response = await self.openai_client.chat.completions.create(
    model=self.llm_model,
    temperature=0.3,
    max_tokens=2000,  # æå–çŸ¥è­˜åˆ—è¡¨éœ€è¦è¼ƒé•·è¼¸å‡ºï¼ˆå¤šå€‹ Q&A çš„ JSONï¼‰
    response_format={"type": "json_object"},
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"è«‹å¾ä»¥ä¸‹å…§å®¹æå–çŸ¥è­˜ï¼š\n\n{content[:4000]}"}
    ]
)
```

#### ä¿®å¾©ä½ç½® 2: æ„åœ–æ¨è–¦ï¼ˆline 959-965ï¼‰
```python
response = await self.openai_client.chat.completions.create(
    model=self.llm_model,
    temperature=0.3,
    max_tokens=500,  # æ„åœ–æ¨è–¦åªéœ€å°é‡è¼¸å‡º
    response_format={"type": "json_object"},
    messages=[{"role": "user", "content": prompt}]
)
```

#### ä¿®å¾©ä½ç½® 3: è³ªé‡è©•ä¼°ï¼ˆline 1052-1059ï¼‰
```python
response = await self.openai_client.chat.completions.create(
    model=self.llm_model,
    temperature=0.3,
    max_tokens=500,  # è³ªé‡è©•ä¼°åªéœ€å°é‡è¼¸å‡º
    response_format={"type": "json_object"},
    messages=[{"role": "user", "content": prompt}]
)
```

---

## ä¿®å¾©æª”æ¡ˆæ¸…å–®

| æª”æ¡ˆ | ä¿®æ”¹å…§å®¹ | è¡Œæ•¸è®ŠåŒ– |
|------|---------|---------|
| `document_converter_service.py` | é¡å¸¸é‡å®šç¾© + å‹•æ…‹ max_tokens + TPM é™åˆ¶ + ä»£ç¢¼é‡æ§‹ | +56/-19 |
| `knowledge_import_service.py` | æ·»åŠ  max_tokens é™åˆ¶ï¼ˆ3 è™•ï¼‰ | +3/0 |

---

## æ¸¬è©¦é©—è­‰

### èªæ³•æª¢æŸ¥
```bash
âœ… python3 -m py_compile document_converter_service.py
âœ… python3 -m py_compile knowledge_import_service.py
```

### é æœŸæ•ˆæœ

#### å ´æ™¯ 1: çŸ­æ–‡æª”ï¼ˆ< 5,000 å­—ï¼‰
- **ä¿®æ”¹å‰**: æˆåŠŸï¼ˆç„¡å•é¡Œï¼‰
- **ä¿®æ”¹å¾Œ**: æˆåŠŸï¼ˆä¿æŒä¸è®Šï¼‰

#### å ´æ™¯ 2: ä¸­ç­‰æ–‡æª”ï¼ˆ5,000 - 10,000 å­—ï¼‰
- **ä¿®æ”¹å‰**: gpt-3.5-turbo å¯èƒ½å¤±æ•—
- **ä¿®æ”¹å¾Œ**: æˆåŠŸï¼ˆå‹•æ…‹èª¿æ•´åˆ†æ®µï¼‰

#### å ´æ™¯ 3: é•·æ–‡æª”ï¼ˆ> 15,000 å­—ï¼‰
- **ä¿®æ”¹å‰**: âŒ å¤±æ•—ï¼ˆtoken è¶…é™ï¼‰
- **ä¿®æ”¹å¾Œ**: âœ… æˆåŠŸï¼ˆè‡ªå‹•åˆ†æ®µè™•ç†ï¼‰

#### å ´æ™¯ 4: è¶…é•·æ–‡æª”ï¼ˆ> 50,000 å­—ï¼‰
- **ä¿®æ”¹å‰**: âŒ å¤±æ•—
- **ä¿®æ”¹å¾Œ**: âœ… æˆåŠŸï¼ˆå¤šæ®µè™•ç†ï¼Œå³ä½¿ä½¿ç”¨ gpt-3.5-turboï¼‰

---

## Token ä¼°ç®—è¡¨

| æ¨¡å‹ | Context é™åˆ¶ | å®‰å…¨è¼¸å…¥ | åˆ†æ®µå¤§å° | è¼¸å‡ºä¸Šé™ |
|------|-------------|---------|---------|---------|
| gpt-4 | 8,192 | ~3,000 | 3,000 å­— | 1,000-4,000 |
| gpt-3.5-turbo | 16,385 | ~5,600 | 5,600 å­— | 1,000-4,000 |
| gpt-4-turbo | 128,000 | ~61,500 | 50,000 å­— | 1,000-4,000 |
| gpt-4o | 128,000 | ~61,500 | 50,000 å­— | 1,000-4,000 |
| gpt-4o-mini | 128,000 | ~61,500 | 50,000 å­— | 1,000-4,000 |

**è¨ˆç®—å…¬å¼**ï¼š
- å®‰å…¨è¼¸å…¥ = (Context é™åˆ¶ - 5,000) / 2 å­—å…ƒ
- åˆ†æ®µå¤§å° = min(50,000, å®‰å…¨è¼¸å…¥)
- è¼¸å‡ºä¸Šé™ = max(1,000, min(4,000, (Context - è¼¸å…¥) * 0.9))

---

## å¾ŒçºŒå»ºè­°

### 1. ç›£æ§èˆ‡æ—¥èªŒ
å»ºè­°åœ¨ç”Ÿç”¢ç’°å¢ƒæ·»åŠ ä»¥ä¸‹ç›£æ§ï¼š
```python
# è¨˜éŒ„ token ä½¿ç”¨æƒ…æ³
logger.info(f"Token usage: input={estimated_input_tokens}, output={safe_max_tokens}, model={self.model}")

# è­¦å‘Šæ¥è¿‘é™åˆ¶
if estimated_input_tokens > max_context * 0.8:
    logger.warning(f"Input tokens approaching limit: {estimated_input_tokens}/{max_context}")
```

### 2. æ›´ç²¾ç¢ºçš„ Token è¨ˆç®—
è€ƒæ…®ä½¿ç”¨ `tiktoken` åº«é€²è¡Œç²¾ç¢ºçš„ token è¨ˆç®—ï¼š
```python
import tiktoken

encoding = tiktoken.encoding_for_model(self.model)
actual_tokens = len(encoding.encode(content))
```

### 3. éŒ¯èª¤è™•ç†å¢å¼·
æ·»åŠ é‡å° token è¶…é™çš„éŒ¯èª¤è™•ç†ï¼š
```python
try:
    response = client.chat.completions.create(...)
except openai.BadRequestError as e:
    if 'context_length_exceeded' in str(e):
        # è‡ªå‹•æ¸›å°‘åˆ†æ®µå¤§å°ä¸¦é‡è©¦
        logger.warning("Token limit exceeded, reducing chunk size and retrying...")
        max_chars = int(max_chars * 0.7)
        return await self._retry_with_smaller_chunks(content, max_chars)
    raise
```

### 4. é…ç½®åŒ–
å°‡ token é™åˆ¶åƒæ•¸ç§»åˆ°ç’°å¢ƒè®Šæ•¸ï¼š
```bash
# .env
MAX_INPUT_TOKENS_RATIO=0.6  # è¼¸å…¥ä½” context çš„æ¯”ä¾‹
MAX_OUTPUT_TOKENS=4000       # æœ€å¤§è¼¸å‡º tokens
TOKEN_BUFFER_RATIO=0.9       # ç·©è¡æ¯”ä¾‹
```

---

## ç›¸é—œè³‡æº

- [OpenAI Token Limits](https://platform.openai.com/docs/models)
- [tiktoken åº«](https://github.com/openai/tiktoken)
- [Token è¨ˆç®—å™¨](https://platform.openai.com/tokenizer)

---

## ç¸½çµ

### æ ¸å¿ƒæ”¹é€²
1. âœ… **é¡å¸¸é‡å®šç¾©**ï¼šæ¶ˆé™¤é‡è¤‡ä»£ç¢¼ï¼Œéµå¾ª DRY åŸå‰‡
2. âœ… **å‹•æ…‹ max_tokens è¨ˆç®—**ï¼šæ ¹æ“šæ¨¡å‹å’Œè¼¸å…¥è‡ªå‹•èª¿æ•´
3. âœ… **æ™ºèƒ½åˆ†æ®µ**ï¼šæ ¹æ“šæ¨¡å‹å®¹é‡å‹•æ…‹èª¿æ•´åˆ†æ®µå¤§å°
4. âœ… **TPM é™åˆ¶è™•ç†**ï¼šæ·»åŠ æ™ºèƒ½å»¶é²é¿å… rate limit
5. âœ… **é é˜²æ€§ä¿è­·**ï¼šç‚ºæ‰€æœ‰ API å‘¼å«æ·»åŠ  max_tokens é™åˆ¶
6. âœ… **æ¨¡å‹é©é…**ï¼šæ”¯æ´ gpt-4, gpt-3.5-turbo, gpt-4o ç­‰æ‰€æœ‰æ¨¡å‹

### ä¿®å¾©æ•ˆæœ
- âŒ **ä¿®æ”¹å‰**ï¼šé•·æ–‡æª”ï¼ˆ> 7,500 å­—ï¼‰åœ¨ gpt-3.5-turbo ä¸Šæœƒå¤±æ•—
- âœ… **ä¿®æ”¹å¾Œ**ï¼šæ”¯æ´ä»»æ„é•·åº¦æ–‡æª”ï¼Œè‡ªå‹•åˆ†æ®µè™•ç†

### ç”Ÿç”¢å°±ç·’
- âœ… èªæ³•æª¢æŸ¥é€šé
- âœ… å‘å¾Œç›¸å®¹ï¼ˆä¸å½±éŸ¿ç¾æœ‰åŠŸèƒ½ï¼‰
- âœ… æ€§èƒ½å½±éŸ¿ï¼šå¯å¿½ç•¥ï¼ˆåƒ…å¢åŠ å°‘é‡è¨ˆç®—ï¼‰
- âš ï¸ å»ºè­°ï¼šéƒ¨ç½²å¾Œç›£æ§ token ä½¿ç”¨æƒ…æ³

---

**ä¿®å¾©ç‹€æ…‹**: âœ… å·²å®Œæˆ
**æ¸¬è©¦ç‹€æ…‹**: âš ï¸ å¾…ç”Ÿç”¢ç’°å¢ƒé©—è­‰
**éƒ¨ç½²å»ºè­°**: å¯ä»¥å®‰å…¨éƒ¨ç½²ï¼Œå»ºè­°æ·»åŠ ç›£æ§

**ä¿®å¾©äººå“¡**: Claude Code
**å¯©æ ¸ç‹€æ…‹**: å¾…å¯©æ ¸
