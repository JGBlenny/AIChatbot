#!/usr/bin/env python3
"""
RAG è©•åˆ†å“è³ªæ¸¬è©¦ç³»çµ±
é©—è­‰æ„åœ–åŠ æˆæ©Ÿåˆ¶æ˜¯å¦çœŸæ­£æå‡ç­”æ¡ˆå“è³ª

æ¸¬è©¦ç¶­åº¦ï¼š
1. ç›¸é—œæ€§ (Relevance): ç­”æ¡ˆæ˜¯å¦å›ç­”å•é¡Œ
2. å®Œæ•´æ€§ (Completeness): ç­”æ¡ˆæ˜¯å¦å®Œæ•´
3. æ„åœ–åŒ¹é… (Intent Match): æ„åœ–æ˜¯å¦æ­£ç¢º
4. æ’åºè³ªé‡ (Ranking Quality): æœ€ä½³ç­”æ¡ˆæ˜¯å¦æ’åœ¨å‰é¢

æ¸¬è©¦å ´æ™¯ï¼š
- å¤šæ„åœ–å•é¡Œï¼ˆä¸»è¦ + æ¬¡è¦ï¼‰
- å–®æ„åœ–å•é¡Œ
- é‚Šç•Œæƒ…æ³ï¼ˆä½ç›¸ä¼¼åº¦é«˜æ„åœ– vs é«˜ç›¸ä¼¼åº¦ä½æ„åœ–ï¼‰
- éŒ¯èª¤åˆ†é¡æƒ…æ³
"""

import sys
import os
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from openai import OpenAI

# æ·»åŠ è·¯å¾‘
sys.path.insert(0, '/Users/lenny/jgb/AIChatbot/rag-orchestrator')

from services.intent_classifier import IntentClassifier
from services.vendor_knowledge_retriever import VendorKnowledgeRetriever


class ScoringQualityTester:
    """è©•åˆ†å“è³ªæ¸¬è©¦å™¨"""

    def __init__(self):
        self.intent_classifier = IntentClassifier(use_database=True)
        self.knowledge_retriever = VendorKnowledgeRetriever()
        self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.test_results = []

    def print_section(self, title: str):
        """æ‰“å°å€å¡Šæ¨™é¡Œ"""
        print("\n" + "=" * 80)
        print(f"  {title}")
        print("=" * 80)

    def print_subsection(self, title: str):
        """æ‰“å°å­æ¨™é¡Œ"""
        print(f"\n{'â”€' * 80}")
        print(f"ğŸ“‹ {title}")
        print(f"{'â”€' * 80}")

    async def retrieve_with_different_boosts(
        self,
        query: str,
        intent_id: int,
        all_intent_ids: List[int],
        vendor_id: int = 1,
        top_k: int = 5
    ) -> Dict[str, List[Dict]]:
        """
        ä½¿ç”¨ä¸åŒçš„åŠ æˆå€æ•¸æª¢ç´¢ï¼Œæ¯”è¼ƒæ•ˆæœ

        Returns:
            {
                'baseline': [...],      # ç„¡åŠ æˆ (1.0x)
                'current': [...],       # ç•¶å‰ (1.5x/1.2x)
                'aggressive': [...],    # æ¿€é€² (2.0x/1.5x)
                'conservative': [...]   # ä¿å®ˆ (1.3x/1.1x)
            }
        """
        results = {}

        # æ¸¬è©¦é…ç½®
        boost_configs = {
            'baseline': (1.0, 1.0),
            'current': (1.5, 1.2),
            'aggressive': (2.0, 1.5),
            'conservative': (1.3, 1.1)
        }

        print(f"\nğŸ” æ¸¬è©¦ä¸åŒåŠ æˆå€æ•¸å°å•é¡Œçš„å½±éŸ¿ï¼š")
        print(f"   å•é¡Œ: {query}")
        print(f"   ä¸»è¦æ„åœ– ID: {intent_id}")
        print(f"   æ‰€æœ‰æ„åœ– IDs: {all_intent_ids}")

        for config_name, (primary_boost, secondary_boost) in boost_configs.items():
            print(f"\n   âš™ï¸  é…ç½®: {config_name} (ä¸»è¦: {primary_boost}x, æ¬¡è¦: {secondary_boost}x)")

            # é€™è£¡éœ€è¦ä¿®æ”¹ retriever ä»¥æ”¯æŒè‡ªå®šç¾© boost
            # æš«æ™‚ä½¿ç”¨ç•¶å‰å¯¦ç¾
            knowledge_list = await self.knowledge_retriever.retrieve_knowledge_hybrid(
                query=query,
                intent_id=intent_id,
                vendor_id=vendor_id,
                top_k=top_k,
                similarity_threshold=0.5,  # é™ä½é–¾å€¼ä»¥è§€å¯Ÿæ›´å¤šçµæœ
                resolve_templates=False,
                all_intent_ids=all_intent_ids
            )

            results[config_name] = knowledge_list
            print(f"      æ‰¾åˆ° {len(knowledge_list)} ç­†çµæœ")

        return results

    async def evaluate_answer_quality_with_llm(
        self,
        question: str,
        answer: str,
        expected_intent: str
    ) -> Dict:
        """
        ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆå“è³ª

        Returns:
            {
                'relevance': 1-5,
                'completeness': 1-5,
                'accuracy': 1-5,
                'intent_match': 1-5,
                'overall': 1-5,
                'reasoning': str
            }
        """
        prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å•ç­”çš„å“è³ªï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€ä½³ï¼‰ï¼š

å•é¡Œï¼š{question}
é æœŸæ„åœ–ï¼š{expected_intent}
ç­”æ¡ˆï¼š{answer}

è«‹å¾ä»¥ä¸‹ç¶­åº¦è©•åˆ†ï¼š
1. ç›¸é—œæ€§ (Relevance): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”å•é¡Œï¼Ÿ
2. å®Œæ•´æ€§ (Completeness): ç­”æ¡ˆæ˜¯å¦å®Œæ•´æ¶µè“‹å•é¡Œæ‰€å•ï¼Ÿ
3. æº–ç¢ºæ€§ (Accuracy): ç­”æ¡ˆå…§å®¹æ˜¯å¦æº–ç¢ºå¯é ï¼Ÿ
4. æ„åœ–åŒ¹é… (Intent Match): ç­”æ¡ˆæ˜¯å¦ç¬¦åˆé æœŸæ„åœ–ï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
{{
    "relevance": <1-5>,
    "completeness": <1-5>,
    "accuracy": <1-5>,
    "intent_match": <1-5>,
    "overall": <1-5>,
    "reasoning": "ç°¡çŸ­èªªæ˜è©•åˆ†ç†ç”±"
}}"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )

            result = json.loads(response.choices[0].message.content)
            return result
        except Exception as e:
            print(f"âš ï¸  LLM è©•ä¼°å¤±æ•—: {e}")
            return {
                'relevance': 0,
                'completeness': 0,
                'accuracy': 0,
                'intent_match': 0,
                'overall': 0,
                'reasoning': f"è©•ä¼°å¤±æ•—: {str(e)}"
            }

    def calculate_ndcg(
        self,
        retrieved_results: List[Dict],
        relevance_scores: List[float],
        k: int = 5
    ) -> float:
        """
        è¨ˆç®— NDCG@K (Normalized Discounted Cumulative Gain)
        è¡¡é‡æ’åºè³ªé‡

        Args:
            retrieved_results: æª¢ç´¢çµæœåˆ—è¡¨
            relevance_scores: å°æ‡‰çš„ç›¸é—œæ€§åˆ†æ•¸åˆ—è¡¨
            k: è¨ˆç®—å‰ K å€‹çµæœ

        Returns:
            NDCG åˆ†æ•¸ (0-1)
        """
        import math

        if not relevance_scores or len(relevance_scores) == 0:
            return 0.0

        # DCG: Discounted Cumulative Gain
        dcg = 0.0
        for i, score in enumerate(relevance_scores[:k], 1):
            dcg += (2 ** score - 1) / math.log2(i + 1)

        # IDCG: Ideal DCG (æœ€ä½³æ’åº)
        ideal_scores = sorted(relevance_scores, reverse=True)[:k]
        idcg = 0.0
        for i, score in enumerate(ideal_scores, 1):
            idcg += (2 ** score - 1) / math.log2(i + 1)

        # NDCG
        if idcg == 0:
            return 0.0
        return dcg / idcg

    async def test_case_with_quality_assessment(
        self,
        test_num: int,
        test_name: str,
        question: str,
        expected_intent: str,
        expected_best_answers: List[str],
        vendor_id: int = 1
    ) -> Dict:
        """
        åŸ·è¡Œå–®å€‹æ¸¬è©¦ä¸¦è©•ä¼°ç­”æ¡ˆå“è³ª

        Args:
            expected_best_answers: é æœŸçš„æœ€ä½³ç­”æ¡ˆï¼ˆknowledge ID æˆ– question_summaryï¼‰
        """
        self.print_subsection(f"æ¸¬è©¦ {test_num}: {test_name}")

        result = {
            'test_num': test_num,
            'test_name': test_name,
            'question': question,
            'expected_intent': expected_intent,
            'expected_best_answers': expected_best_answers,
            'passed': True,
            'issues': [],
            'quality_scores': {},
            'ndcg_scores': {}
        }

        try:
            # 1. æ„åœ–åˆ†é¡
            print(f"\nğŸ” å•é¡Œ: {question}")
            intent_result = self.intent_classifier.classify(question)
            print(f"   åˆ†é¡çµæœ: {intent_result['intent_name']} (ä¿¡å¿ƒåº¦: {intent_result['confidence']:.2f})")

            primary_intent_id = intent_result.get('intent_ids', [None])[0]
            all_intent_ids = intent_result.get('intent_ids', [])

            if not primary_intent_id:
                result['passed'] = False
                result['issues'].append("æ„åœ–åˆ†é¡å¤±æ•—")
                return result

            # 2. æª¢ç´¢çŸ¥è­˜ï¼ˆç•¶å‰é…ç½®ï¼‰
            print(f"\nğŸ“š æª¢ç´¢çŸ¥è­˜ (ä½¿ç”¨ç•¶å‰é…ç½®):")
            knowledge_list = await self.knowledge_retriever.retrieve_knowledge_hybrid(
                query=question,
                intent_id=primary_intent_id,
                vendor_id=vendor_id,
                top_k=5,
                similarity_threshold=0.5,
                resolve_templates=False,
                all_intent_ids=all_intent_ids
            )

            if not knowledge_list:
                result['issues'].append("æœªæª¢ç´¢åˆ°ä»»ä½•çŸ¥è­˜")
                print("   âš ï¸  æœªæª¢ç´¢åˆ°ä»»ä½•çŸ¥è­˜")
            else:
                print(f"   æ‰¾åˆ° {len(knowledge_list)} ç­†çµæœ")

                # 3. ä½¿ç”¨ LLM è©•ä¼°æ¯å€‹ç­”æ¡ˆçš„å“è³ª
                print(f"\nğŸ¤– ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆå“è³ª:")
                quality_scores = []

                for i, kb in enumerate(knowledge_list[:3], 1):  # åªè©•ä¼°å‰ 3 å€‹
                    print(f"\n   è©•ä¼°ç­”æ¡ˆ {i}: ID {kb.get('id')}, ç›¸ä¼¼åº¦ {kb.get('similarity', 0):.3f}")

                    quality = await self.evaluate_answer_quality_with_llm(
                        question=question,
                        answer=kb.get('answer', ''),
                        expected_intent=expected_intent
                    )

                    quality_scores.append(quality)

                    print(f"      ç›¸é—œæ€§: {quality['relevance']}/5")
                    print(f"      å®Œæ•´æ€§: {quality['completeness']}/5")
                    print(f"      æº–ç¢ºæ€§: {quality['accuracy']}/5")
                    print(f"      æ„åœ–åŒ¹é…: {quality['intent_match']}/5")
                    print(f"      ç¶œåˆè©•åˆ†: {quality['overall']}/5")
                    print(f"      ç†ç”±: {quality['reasoning']}")

                result['quality_scores'] = quality_scores

                # 4. è¨ˆç®— NDCGï¼ˆä½¿ç”¨ç›¸é—œæ€§åˆ†æ•¸ï¼‰
                if quality_scores:
                    relevance_scores = [q['relevance'] for q in quality_scores]
                    ndcg = self.calculate_ndcg(
                        knowledge_list[:3],
                        relevance_scores,
                        k=3
                    )
                    result['ndcg_scores']['current'] = ndcg
                    print(f"\n   ğŸ“Š NDCG@3: {ndcg:.3f}")

                # 5. æª¢æŸ¥æœ€ä½³ç­”æ¡ˆæ˜¯å¦åœ¨å‰ 3
                top_3_summaries = [kb.get('question_summary', '') for kb in knowledge_list[:3]]
                found_best = False
                for expected in expected_best_answers:
                    if any(expected in summary for summary in top_3_summaries):
                        found_best = True
                        break

                if found_best:
                    print(f"\n   âœ… é æœŸçš„æœ€ä½³ç­”æ¡ˆåœ¨ Top-3 ä¸­")
                else:
                    print(f"\n   âš ï¸  é æœŸçš„æœ€ä½³ç­”æ¡ˆä¸åœ¨ Top-3 ä¸­")
                    result['issues'].append("é æœŸæœ€ä½³ç­”æ¡ˆæœªåœ¨å‰ 3 å")

                # 6. åˆ†æè©•åˆ†èˆ‡ç›¸ä¼¼åº¦çš„é—œä¿‚
                print(f"\n   ğŸ“ˆ è©•åˆ†èˆ‡ç›¸ä¼¼åº¦ç›¸é—œæ€§åˆ†æ:")
                if quality_scores and len(knowledge_list) >= 3:
                    similarities = [kb.get('similarity', 0) for kb in knowledge_list[:3]]
                    overall_scores = [q['overall'] for q in quality_scores]

                    # ç°¡å–®ç›¸é—œæ€§æª¢æŸ¥
                    for i in range(3):
                        print(f"      çµæœ {i+1}: ç›¸ä¼¼åº¦ {similarities[i]:.3f}, å“è³ª {overall_scores[i]}/5")

                    # æª¢æŸ¥æ˜¯å¦ç›¸ä¼¼åº¦é«˜çš„ç­”æ¡ˆå“è³ªä¹Ÿé«˜
                    if similarities[0] > similarities[1] and overall_scores[0] < overall_scores[1]:
                        result['issues'].append("æ’åºå•é¡Œï¼šç›¸ä¼¼åº¦æœ€é«˜çš„ç­”æ¡ˆå“è³ªåè€Œè¼ƒä½")

        except Exception as e:
            result['passed'] = False
            result['issues'].append(f"æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {str(e)}")
            print(f"\nâŒ éŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()

        # ä¿å­˜çµæœ
        self.test_results.append(result)

        # æ‰“å°æ¸¬è©¦çµæœ
        status = "âœ… é€šé" if result['passed'] and len(result['issues']) == 0 else "âš ï¸  éœ€æ³¨æ„"
        print(f"\n{'â”€' * 80}")
        print(f"{status}: {test_name}")
        if result['issues']:
            print("ç™¼ç¾çš„å•é¡Œ:")
            for issue in result['issues']:
                print(f"   - {issue}")
        print(f"{'â”€' * 80}")

        return result

    async def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰è©•åˆ†å“è³ªæ¸¬è©¦"""

        self.print_section("ğŸ§ª RAG è©•åˆ†å“è³ªå®Œæ•´æ¸¬è©¦")

        # æ¸¬è©¦æ¡ˆä¾‹é›†
        test_cases = [
            {
                'test_num': 1,
                'test_name': 'å¤šæ„åœ–å•é¡Œ - é€€ç§ŸæŠ¼é‡‘',
                'question': 'é€€ç§Ÿæ™‚æŠ¼é‡‘è¦æ€éº¼é€€é‚„ï¼Ÿ',
                'expected_intent': 'é€€ç§Ÿæµç¨‹',
                'expected_best_answers': ['é€€ç§ŸæŠ¼é‡‘å¦‚ä½•é€€é‚„', 'æŠ¼é‡‘']
            },
            {
                'test_num': 2,
                'test_name': 'å¤šæ„åœ–å•é¡Œ - è§£é™¤åˆç´„',
                'question': 'å¦‚ä½•è§£é™¤åˆç´„ä¸¦é‡æ–°ç°½ç´„ï¼Ÿ',
                'expected_intent': 'é€€ç§Ÿæµç¨‹',
                'expected_best_answers': ['è§£é™¤åˆç´„', 'é‡æ–°ç°½ç´„']
            },
            {
                'test_num': 3,
                'test_name': 'å–®æ„åœ–å•é¡Œ - ç§Ÿé‡‘è¨ˆç®—',
                'question': 'ç§Ÿé‡‘å¦‚ä½•è¨ˆç®—ï¼Ÿé€¾æœŸæœƒç½°æ¬¾å—ï¼Ÿ',
                'expected_intent': 'åˆç´„è¦å®š',
                'expected_best_answers': ['ç§Ÿé‡‘', 'é€¾æœŸ', 'ç½°æ¬¾']
            },
            {
                'test_num': 4,
                'test_name': 'é‚Šç•Œæƒ…æ³ - ä¸€èˆ¬æ€§å•é¡Œ',
                'question': 'æˆ‘æƒ³äº†è§£ç§Ÿç´„çš„è©³ç´°å…§å®¹',
                'expected_intent': 'åˆç´„è¦å®š',
                'expected_best_answers': ['ç§Ÿç´„', 'åˆç´„']
            }
        ]

        # åŸ·è¡Œæ¸¬è©¦
        for test_case in test_cases:
            await self.test_case_with_quality_assessment(**test_case)
            await asyncio.sleep(2)  # é¿å… API è«‹æ±‚éæ–¼å¯†é›†

        # æ‰“å°ç¸½çµ
        self.print_summary()

    def print_summary(self):
        """æ‰“å°æ¸¬è©¦ç¸½çµ"""

        self.print_section("ğŸ“Š è©•åˆ†å“è³ªæ¸¬è©¦ç¸½çµ")

        total_tests = len(self.test_results)
        tests_with_issues = sum(1 for r in self.test_results if len(r['issues']) > 0)

        print(f"\nç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"ç™¼ç¾å•é¡Œ: {tests_with_issues} å€‹æ¸¬è©¦")

        # çµ±è¨ˆå¹³å‡ NDCG
        ndcg_scores = [r['ndcg_scores'].get('current', 0) for r in self.test_results if r['ndcg_scores']]
        if ndcg_scores:
            avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)
            print(f"\nå¹³å‡ NDCG@3: {avg_ndcg:.3f}")
            print(f"   (1.0 = å®Œç¾æ’åº, 0.0 = å®Œå…¨éŒ¯èª¤)")

        # çµ±è¨ˆå¹³å‡å“è³ªåˆ†æ•¸
        all_quality_scores = []
        for r in self.test_results:
            if r.get('quality_scores'):
                all_quality_scores.extend(r['quality_scores'])

        if all_quality_scores:
            avg_relevance = sum(q['relevance'] for q in all_quality_scores) / len(all_quality_scores)
            avg_completeness = sum(q['completeness'] for q in all_quality_scores) / len(all_quality_scores)
            avg_accuracy = sum(q['accuracy'] for q in all_quality_scores) / len(all_quality_scores)
            avg_intent_match = sum(q['intent_match'] for q in all_quality_scores) / len(all_quality_scores)
            avg_overall = sum(q['overall'] for q in all_quality_scores) / len(all_quality_scores)

            print(f"\nå¹³å‡ç­”æ¡ˆå“è³ªåˆ†æ•¸ (5åˆ†åˆ¶):")
            print(f"   ç›¸é—œæ€§: {avg_relevance:.2f}")
            print(f"   å®Œæ•´æ€§: {avg_completeness:.2f}")
            print(f"   æº–ç¢ºæ€§: {avg_accuracy:.2f}")
            print(f"   æ„åœ–åŒ¹é…: {avg_intent_match:.2f}")
            print(f"   ç¶œåˆè©•åˆ†: {avg_overall:.2f}")

        # é¡¯ç¤ºæ‰€æœ‰å•é¡Œ
        if tests_with_issues > 0:
            print(f"\nâš ï¸  ç™¼ç¾çš„å•é¡Œç¸½çµ:")
            issue_counts = {}
            for result in self.test_results:
                for issue in result['issues']:
                    issue_counts[issue] = issue_counts.get(issue, 0) + 1

            for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
                print(f"   - {issue} ({count} æ¬¡)")

        # ä¿å­˜çµæœåˆ°æ–‡ä»¶
        output_file = f"/Users/lenny/jgb/AIChatbot/output/scoring_quality_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'total_tests': total_tests,
                    'tests_with_issues': tests_with_issues,
                    'avg_ndcg': avg_ndcg if ndcg_scores else 0,
                    'avg_quality': {
                        'relevance': avg_relevance if all_quality_scores else 0,
                        'completeness': avg_completeness if all_quality_scores else 0,
                        'accuracy': avg_accuracy if all_quality_scores else 0,
                        'intent_match': avg_intent_match if all_quality_scores else 0,
                        'overall': avg_overall if all_quality_scores else 0,
                    } if all_quality_scores else {}
                },
                'test_results': self.test_results
            }, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜åˆ°: {output_file}")

        # æä¾›æ”¹å–„å»ºè­°
        self.print_recommendations()

    def print_recommendations(self):
        """æ ¹æ“šæ¸¬è©¦çµæœæä¾›æ”¹å–„å»ºè­°"""

        self.print_section("ğŸ’¡ æ”¹å–„å»ºè­°")

        # åˆ†æ NDCG åˆ†æ•¸
        ndcg_scores = [r['ndcg_scores'].get('current', 0) for r in self.test_results if r['ndcg_scores']]
        if ndcg_scores:
            avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)

            if avg_ndcg < 0.7:
                print("\nâš ï¸  NDCG åˆ†æ•¸è¼ƒä½ (<0.7)ï¼Œå»ºè­°:")
                print("   1. èª¿æ•´æ„åœ–åŠ æˆå€æ•¸ (ç•¶å‰ 1.5x/1.2x)")
                print("   2. é™ä½ç›¸ä¼¼åº¦é–¾å€¼ (ç•¶å‰ 0.6)")
                print("   3. æª¢æŸ¥ Scope æ¬Šé‡ (1000/500/100)")
            elif avg_ndcg < 0.85:
                print("\nâœ… NDCG åˆ†æ•¸ä¸­ç­‰ (0.7-0.85)ï¼Œå¯è€ƒæ…®å¾®èª¿:")
                print("   1. å°ä¸»è¦æ„åœ–å¯èƒ½çµ¦äºˆæ›´é«˜åŠ æˆ (å¦‚ 1.6x)")
                print("   2. è€ƒæ…®å¼•å…¥æ„åœ–ä¿¡å¿ƒåº¦ä½œç‚ºé¡å¤–æ¬Šé‡")
            else:
                print("\nğŸ‰ NDCG åˆ†æ•¸å„ªç§€ (>0.85)ï¼Œæ’åºå“è³ªè‰¯å¥½ï¼")

        # åˆ†æå“è³ªåˆ†æ•¸
        all_quality_scores = []
        for r in self.test_results:
            if r.get('quality_scores'):
                all_quality_scores.extend(r['quality_scores'])

        if all_quality_scores:
            avg_overall = sum(q['overall'] for q in all_quality_scores) / len(all_quality_scores)

            if avg_overall < 3.5:
                print("\nâš ï¸  ç­”æ¡ˆå“è³ªåˆ†æ•¸è¼ƒä½ (<3.5/5)ï¼Œå»ºè­°:")
                print("   1. æª¢æŸ¥çŸ¥è­˜åº«å…§å®¹å“è³ª")
                print("   2. æé«˜ç›¸ä¼¼åº¦é–¾å€¼ä»¥éæ¿¾ä½è³ªé‡ç­”æ¡ˆ")
                print("   3. åŠ å¼·æ„åœ–åˆ†é¡æº–ç¢ºæ€§")
            elif avg_overall < 4.0:
                print("\nâœ… ç­”æ¡ˆå“è³ªåˆ†æ•¸ä¸­ç­‰ (3.5-4.0/5):")
                print("   1. ç¹¼çºŒå„ªåŒ–çŸ¥è­˜åº«å…§å®¹")
                print("   2. è€ƒæ…®å¼•å…¥é‡æ’åºæ©Ÿåˆ¶")
            else:
                print("\nğŸ‰ ç­”æ¡ˆå“è³ªåˆ†æ•¸å„ªç§€ (>4.0/5)ï¼")

        # å¸¸è¦‹å•é¡Œå»ºè­°
        issue_counts = {}
        for result in self.test_results:
            for issue in result['issues']:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1

        if "é æœŸæœ€ä½³ç­”æ¡ˆæœªåœ¨å‰ 3 å" in issue_counts:
            count = issue_counts["é æœŸæœ€ä½³ç­”æ¡ˆæœªåœ¨å‰ 3 å"]
            print(f"\nâš ï¸  æœ‰ {count} å€‹æ¸¬è©¦çš„æœ€ä½³ç­”æ¡ˆæœªåœ¨å‰ 3:")
            print("   å»ºè­°:")
            print("   1. å¢åŠ æ„åœ–åŠ æˆå€æ•¸")
            print("   2. èª¿æ•´ Scope æ¬Šé‡")
            print("   3. æª¢æŸ¥ embedding è³ªé‡")

        if "æ’åºå•é¡Œï¼šç›¸ä¼¼åº¦æœ€é«˜çš„ç­”æ¡ˆå“è³ªåè€Œè¼ƒä½" in issue_counts:
            print(f"\nâš ï¸  å­˜åœ¨æ’åºå“è³ªå•é¡Œ:")
            print("   å»ºè­°:")
            print("   1. å¼•å…¥ç­”æ¡ˆé•·åº¦/å®Œæ•´æ€§å› å­")
            print("   2. è€ƒæ…®ä½¿ç”¨é‡æ’åº (Reranking) æ¨¡å‹")
            print("   3. çµåˆäººå·¥æ¨™è¨»å„ªåŒ–æ’åº")


async def main():
    """ä¸»å‡½æ•¸"""
    tester = ScoringQualityTester()

    try:
        await tester.run_all_tests()
        sys.exit(0)

    except KeyboardInterrupt:
        print("\n\nâš ï¸ æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_file = '/Users/lenny/jgb/AIChatbot/.env'
    if os.path.exists(env_file):
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    if key != 'EMBEDDING_API_URL':
                        os.environ[key] = value

    # è¨­ç½®ç’°å¢ƒè®Šæ•¸
    os.environ['DB_HOST'] = os.getenv('DB_HOST', 'localhost')
    os.environ['DB_PORT'] = os.getenv('DB_PORT', '5432')
    os.environ['DB_USER'] = os.getenv('DB_USER', 'aichatbot')
    os.environ['DB_PASSWORD'] = os.getenv('DB_PASSWORD', 'aichatbot_password')
    os.environ['DB_NAME'] = os.getenv('DB_NAME', 'aichatbot_admin')
    os.environ['EMBEDDING_API_URL'] = 'http://localhost:5001/api/v1/embeddings'

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(main())
