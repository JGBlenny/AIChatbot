#!/usr/bin/env python3
"""
å®Œæ•´å°è©±æµç¨‹æ•ˆèƒ½æ¸¬è©¦

âš ï¸ è­¦å‘Šï¼šæ­¤æ¸¬è©¦ä½¿ç”¨å·²å»¢æ£„çš„ /api/v1/chat ç«¯é»
   - æ­¤ç«¯é»å°‡åœ¨ 2026-01-01 ç§»é™¤
   - å»ºè­°é·ç§»åˆ° /api/v1/message æˆ– /api/v1/chat/stream
   - è©³è¦‹: docs/api/CHAT_API_MIGRATION_GUIDE.md

æ¸¬è©¦ç¯„åœï¼š
1. æ„åœ–åˆ†é¡
2. çŸ¥è­˜æª¢ç´¢ï¼ˆRAGï¼‰
3. LLM ç­”æ¡ˆç”Ÿæˆ
4. ä¿¡å¿ƒåº¦è©•ä¼°
5. æœªé‡æ¸…å•é¡Œè¨˜éŒ„
6. ç«¯åˆ°ç«¯ç¸½æ™‚é–“

æ¸¬è©¦æƒ…å¢ƒï¼š
- é«˜ä¿¡å¿ƒåº¦å•é¡Œï¼ˆæœ‰æ˜ç¢ºçŸ¥è­˜ï¼‰
- ä¸­ç­‰ä¿¡å¿ƒåº¦å•é¡Œï¼ˆéƒ¨åˆ†ç›¸é—œçŸ¥è­˜ï¼‰
- ä½ä¿¡å¿ƒåº¦å•é¡Œï¼ˆç„¡ç›¸é—œçŸ¥è­˜ï¼‰
- è¤‡é›œå•é¡Œï¼ˆéœ€è¦ SOP + åƒæ•¸æ³¨å…¥ï¼‰

TODO: å°‡æ¸¬è©¦é·ç§»åˆ° /api/v1/message ç«¯é»
"""
import asyncio
import httpx
import time
import statistics
from typing import List, Dict
from datetime import datetime
import json

# API é…ç½®
RAG_API_BASE = "http://localhost:8100"
VENDOR_ID = 1  # æ¸¬è©¦ç”¨æ¥­è€… ID

# æ¸¬è©¦å•é¡Œé›†åˆ
TEST_QUESTIONS = {
    "high_confidence": [
        "ç§Ÿé‡‘æ¯å€‹æœˆå¹¾è™Ÿè¦ç¹³ï¼Ÿ",
        "æˆ‘æƒ³é¤Šå¯µç‰©å¯ä»¥å—ï¼Ÿ",
        "é€€ç§Ÿéœ€è¦æå‰å¤šä¹…å‘ŠçŸ¥ï¼Ÿ",
        "æˆ¿ç§ŸåŒ…å«å“ªäº›è²»ç”¨ï¼Ÿ",
        "ç§Ÿç´„åˆ°æœŸå¦‚ä½•çºŒç´„ï¼Ÿ"
    ],
    "medium_confidence": [
        "é›»è²»æ˜¯æ€éº¼ç®—çš„ï¼Ÿ",
        "å†·æ°£å£äº†æ€éº¼è¾¦ï¼Ÿ",
        "å¯ä»¥æå‰è§£ç´„å—ï¼Ÿ",
        "æˆ¿é–“å¯ä»¥é‡˜é‡˜å­å—ï¼Ÿ",
        "åœè»Šä½æ€éº¼æ”¶è²»ï¼Ÿ"
    ],
    "low_confidence": [
        "é™„è¿‘æœ‰ä»€éº¼å¥½åƒçš„é¤å»³ï¼Ÿ",
        "é€™å€‹ç¤¾å€å®‰å…¨å—ï¼Ÿ",
        "æˆ‘çš„é„°å±…å¾ˆåµæ€éº¼è¾¦ï¼Ÿ",
        "ä»Šå¤©å¤©æ°£å¦‚ä½•ï¼Ÿ",
        "ä½ èƒ½å¹«æˆ‘è¨‚æŠ«è–©å—ï¼Ÿ"
    ],
    "complex": [
        "æˆ‘çš„ç§Ÿé‡‘æ˜ç´°æ˜¯ä»€éº¼ï¼Ÿ",  # éœ€è¦ API æ•´åˆ
        "å¹«æˆ‘æŸ¥è©¢æˆ‘çš„ç¹³è²»è¨˜éŒ„",  # éœ€è¦åƒæ•¸æ³¨å…¥
        "æˆ‘çš„åˆç´„ä»€éº¼æ™‚å€™åˆ°æœŸï¼Ÿ",  # éœ€è¦ç§Ÿå®¢è­˜åˆ¥
    ]
}


class PerformanceTester:
    """æ•ˆèƒ½æ¸¬è©¦å™¨"""

    def __init__(self):
        self.results = []
        self.client = None

    async def __aenter__(self):
        """é€²å…¥ä¸Šä¸‹æ–‡æ™‚å‰µå»º HTTP å®¢æˆ¶ç«¯"""
        self.client = httpx.AsyncClient(timeout=60.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡æ™‚é—œé–‰å®¢æˆ¶ç«¯"""
        if self.client:
            await self.client.close()

    async def test_single_question(
        self,
        question: str,
        category: str,
        repeat: int = 1
    ) -> Dict:
        """æ¸¬è©¦å–®å€‹å•é¡Œ"""
        times = []
        responses = []

        for i in range(repeat):
            start_time = time.time()

            try:
                response = await self.client.post(
                    f"{RAG_API_BASE}/api/v1/chat",
                    json={
                        "question": question,
                        "vendor_id": VENDOR_ID,
                        "user_role": "customer",
                        "user_id": f"perf_test_{i}"
                    }
                )

                elapsed = (time.time() - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’

                if response.status_code == 200:
                    data = response.json()
                    times.append(elapsed)
                    responses.append(data)
                else:
                    print(f"   âŒ è«‹æ±‚å¤±æ•—: {response.status_code}")
                    times.append(elapsed)
                    responses.append(None)

            except Exception as e:
                elapsed = (time.time() - start_time) * 1000
                print(f"   âŒ ç•°å¸¸: {e}")
                times.append(elapsed)
                responses.append(None)

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        if times:
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            median_time = statistics.median(times)
            stdev_time = statistics.stdev(times) if len(times) > 1 else 0
        else:
            avg_time = min_time = max_time = median_time = stdev_time = 0

        # å¾éŸ¿æ‡‰ä¸­æå–è©³ç´°æ™‚é–“åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        breakdown = None
        if responses and responses[0]:
            last_response = responses[0]
            breakdown = {
                "processing_time_ms": last_response.get("processing_time_ms", 0),
                "confidence_score": last_response.get("confidence_score", 0),
                "confidence_level": last_response.get("confidence_level", "unknown"),
                "requires_human": last_response.get("requires_human", False),
                "intent_type": last_response.get("intent", {}).get("intent_type", "unknown"),
                "retrieved_docs_count": len(last_response.get("retrieved_docs", []))
            }

        result = {
            "question": question,
            "category": category,
            "repeat_count": repeat,
            "avg_time_ms": round(avg_time, 2),
            "min_time_ms": round(min_time, 2),
            "max_time_ms": round(max_time, 2),
            "median_time_ms": round(median_time, 2),
            "stdev_ms": round(stdev_time, 2),
            "breakdown": breakdown,
            "success_rate": sum(1 for r in responses if r) / len(responses) * 100
        }

        self.results.append(result)
        return result

    async def run_tests(self, repeat_per_question: int = 3):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        print("=" * 80)
        print("RAG å°è©±æµç¨‹å®Œæ•´æ•ˆèƒ½æ¸¬è©¦")
        print("=" * 80)
        print(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"API ç«¯é»: {RAG_API_BASE}")
        print(f"æ¥­è€… ID: {VENDOR_ID}")
        print(f"æ¯å€‹å•é¡Œé‡è¤‡æ¬¡æ•¸: {repeat_per_question}")
        print("=" * 80)

        total_tests = sum(len(questions) for questions in TEST_QUESTIONS.values())
        current_test = 0

        for category, questions in TEST_QUESTIONS.items():
            print(f"\nğŸ“‚ æ¸¬è©¦é¡åˆ¥: {category.upper()}")
            print("-" * 80)

            for question in questions:
                current_test += 1
                print(f"\n[{current_test}/{total_tests}] æ¸¬è©¦å•é¡Œ: {question}")

                result = await self.test_single_question(
                    question,
                    category,
                    repeat=repeat_per_question
                )

                # é¡¯ç¤ºå³æ™‚çµæœ
                print(f"   å¹³å‡æ™‚é–“: {result['avg_time_ms']:.2f} ms")
                print(f"   ç¯„åœ: {result['min_time_ms']:.2f} - {result['max_time_ms']:.2f} ms")

                if result['breakdown']:
                    bd = result['breakdown']
                    print(f"   ä¿¡å¿ƒåº¦: {bd['confidence_score']:.2f} ({bd['confidence_level']})")
                    print(f"   æ„åœ–é¡å‹: {bd['intent_type']}")
                    print(f"   æª¢ç´¢æ–‡æª”æ•¸: {bd['retrieved_docs_count']}")
                    print(f"   éœ€è¦äººå·¥: {'æ˜¯' if bd['requires_human'] else 'å¦'}")

                # é¿å…éå¿«è«‹æ±‚
                await asyncio.sleep(0.5)

    def generate_report(self) -> Dict:
        """ç”Ÿæˆæ•ˆèƒ½å ±å‘Š"""
        if not self.results:
            return {"error": "æ²’æœ‰æ¸¬è©¦çµæœ"}

        # æŒ‰é¡åˆ¥åˆ†çµ„çµ±è¨ˆ
        category_stats = {}
        for category in TEST_QUESTIONS.keys():
            category_results = [r for r in self.results if r['category'] == category]

            if category_results:
                avg_times = [r['avg_time_ms'] for r in category_results]
                category_stats[category] = {
                    "count": len(category_results),
                    "avg_time_ms": round(statistics.mean(avg_times), 2),
                    "min_time_ms": round(min(avg_times), 2),
                    "max_time_ms": round(max(avg_times), 2),
                    "median_time_ms": round(statistics.median(avg_times), 2),
                    "success_rate": round(
                        statistics.mean([r['success_rate'] for r in category_results]), 2
                    )
                }

        # å…¨å±€çµ±è¨ˆ
        all_avg_times = [r['avg_time_ms'] for r in self.results]
        global_stats = {
            "total_tests": len(self.results),
            "overall_avg_ms": round(statistics.mean(all_avg_times), 2),
            "overall_min_ms": round(min(all_avg_times), 2),
            "overall_max_ms": round(max(all_avg_times), 2),
            "overall_median_ms": round(statistics.median(all_avg_times), 2),
            "overall_stdev_ms": round(statistics.stdev(all_avg_times), 2),
            "success_rate": round(
                statistics.mean([r['success_rate'] for r in self.results]), 2
            )
        }

        # ä¿¡å¿ƒåº¦åˆ†å¸ƒ
        confidence_distribution = {}
        for result in self.results:
            if result['breakdown']:
                level = result['breakdown']['confidence_level']
                confidence_distribution[level] = confidence_distribution.get(level, 0) + 1

        # è­˜åˆ¥æ…¢æŸ¥è©¢ï¼ˆå¤§æ–¼ P95ï¼‰
        p95_time = sorted(all_avg_times)[int(len(all_avg_times) * 0.95)]
        slow_queries = [
            {
                "question": r['question'],
                "avg_time_ms": r['avg_time_ms'],
                "category": r['category']
            }
            for r in self.results
            if r['avg_time_ms'] > p95_time
        ]

        return {
            "test_summary": {
                "test_time": datetime.now().isoformat(),
                "api_endpoint": RAG_API_BASE,
                "vendor_id": VENDOR_ID
            },
            "global_stats": global_stats,
            "category_stats": category_stats,
            "confidence_distribution": confidence_distribution,
            "slow_queries": {
                "p95_threshold_ms": round(p95_time, 2),
                "count": len(slow_queries),
                "queries": slow_queries
            },
            "detailed_results": self.results
        }

    def print_report(self, report: Dict):
        """æ‰“å°å ±å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
        print("=" * 80)

        # å…¨å±€çµ±è¨ˆ
        print("\nğŸŒ å…¨å±€çµ±è¨ˆ")
        print("-" * 80)
        gs = report['global_stats']
        print(f"ç¸½æ¸¬è©¦æ•¸: {gs['total_tests']}")
        print(f"æˆåŠŸç‡: {gs['success_rate']}%")
        print(f"å¹³å‡éŸ¿æ‡‰æ™‚é–“: {gs['overall_avg_ms']:.2f} ms")
        print(f"ä¸­ä½æ•¸æ™‚é–“: {gs['overall_median_ms']:.2f} ms")
        print(f"æœ€å°æ™‚é–“: {gs['overall_min_ms']:.2f} ms")
        print(f"æœ€å¤§æ™‚é–“: {gs['overall_max_ms']:.2f} ms")
        print(f"æ¨™æº–å·®: {gs['overall_stdev_ms']:.2f} ms")

        # é¡åˆ¥çµ±è¨ˆ
        print("\nğŸ“‚ å„é¡åˆ¥çµ±è¨ˆ")
        print("-" * 80)
        for category, stats in report['category_stats'].items():
            print(f"\n{category.upper()}:")
            print(f"  æ¸¬è©¦æ•¸: {stats['count']}")
            print(f"  å¹³å‡æ™‚é–“: {stats['avg_time_ms']:.2f} ms")
            print(f"  ç¯„åœ: {stats['min_time_ms']:.2f} - {stats['max_time_ms']:.2f} ms")
            print(f"  æˆåŠŸç‡: {stats['success_rate']}%")

        # ä¿¡å¿ƒåº¦åˆ†å¸ƒ
        print("\nğŸ¯ ä¿¡å¿ƒåº¦åˆ†å¸ƒ")
        print("-" * 80)
        for level, count in report['confidence_distribution'].items():
            percentage = (count / gs['total_tests']) * 100
            print(f"{level}: {count} ({percentage:.1f}%)")

        # æ…¢æŸ¥è©¢
        print("\nâš ï¸  æ…¢æŸ¥è©¢ (P95)")
        print("-" * 80)
        print(f"P95 é–¾å€¼: {report['slow_queries']['p95_threshold_ms']:.2f} ms")
        print(f"æ…¢æŸ¥è©¢æ•¸é‡: {report['slow_queries']['count']}")
        for sq in report['slow_queries']['queries']:
            print(f"  - [{sq['category']}] {sq['question']}: {sq['avg_time_ms']:.2f} ms")

        # æ•ˆèƒ½è©•ç´š
        print("\nâ­ æ•ˆèƒ½è©•ç´š")
        print("-" * 80)
        avg_ms = gs['overall_avg_ms']
        if avg_ms < 1000:
            grade = "A (å„ªç§€)"
            emoji = "ğŸŸ¢"
        elif avg_ms < 2000:
            grade = "B (è‰¯å¥½)"
            emoji = "ğŸŸ¡"
        elif avg_ms < 3000:
            grade = "C (å°šå¯)"
            emoji = "ğŸŸ "
        else:
            grade = "D (éœ€æ”¹é€²)"
            emoji = "ğŸ”´"

        print(f"{emoji} å¹³å‡éŸ¿æ‡‰æ™‚é–“ {avg_ms:.2f} ms - è©•ç´š: {grade}")

        # å»ºè­°
        print("\nğŸ’¡ å„ªåŒ–å»ºè­°")
        print("-" * 80)
        if avg_ms > 2000:
            print("  - å¹³å‡éŸ¿æ‡‰æ™‚é–“è¼ƒé•·ï¼Œå»ºè­°æª¢æŸ¥ LLM API å»¶é²")
        if gs['overall_stdev_ms'] > 500:
            print("  - éŸ¿æ‡‰æ™‚é–“æ³¢å‹•è¼ƒå¤§ï¼Œå»ºè­°æª¢æŸ¥æœå‹™ç©©å®šæ€§")
        if report['slow_queries']['count'] > 0:
            print(f"  - ç™¼ç¾ {report['slow_queries']['count']} å€‹æ…¢æŸ¥è©¢ï¼Œå»ºè­°é‡å°æ€§å„ªåŒ–")

        print("\n" + "=" * 80)


async def main():
    """ä¸»å‡½æ•¸"""
    async with PerformanceTester() as tester:
        # åŸ·è¡Œæ¸¬è©¦
        await tester.run_tests(repeat_per_question=3)

        # ç”Ÿæˆå ±å‘Š
        report = tester.generate_report()

        # æ‰“å°å ±å‘Š
        tester.print_report(report)

        # ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶
        output_file = f"/tmp/chat_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜è‡³: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
