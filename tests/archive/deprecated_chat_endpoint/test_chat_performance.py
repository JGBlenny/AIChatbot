#!/usr/bin/env python3
"""
ÂÆåÊï¥Â∞çË©±ÊµÅÁ®ãÊïàËÉΩÊ∏¨Ë©¶

‚ö†Ô∏è Ë≠¶ÂëäÔºöÊ≠§Ê∏¨Ë©¶‰ΩøÁî®Â∑≤Âª¢Ê£ÑÁöÑ /api/v1/chat Á´ØÈªû
   - Ê≠§Á´ØÈªûÂ∞áÂú® 2026-01-01 ÁßªÈô§
   - Âª∫Ë≠∞ÈÅ∑ÁßªÂà∞ /api/v1/message Êàñ /api/v1/chat/stream
   - Ë©≥Ë¶ã: docs/api/CHAT_API_MIGRATION_GUIDE.md

Ê∏¨Ë©¶ÁØÑÂúçÔºö
1. ÊÑèÂúñÂàÜÈ°û
2. Áü•Ë≠òÊ™¢Á¥¢ÔºàRAGÔºâ
3. LLM Á≠îÊ°àÁîüÊàê
4. ‰ø°ÂøÉÂ∫¶Ë©ï‰º∞
5. Êú™ÈáêÊ∏ÖÂïèÈ°åË®òÈåÑ
6. Á´ØÂà∞Á´ØÁ∏ΩÊôÇÈñì

Ê∏¨Ë©¶ÊÉÖÂ¢ÉÔºö
- È´ò‰ø°ÂøÉÂ∫¶ÂïèÈ°åÔºàÊúâÊòéÁ¢∫Áü•Ë≠òÔºâ
- ‰∏≠Á≠â‰ø°ÂøÉÂ∫¶ÂïèÈ°åÔºàÈÉ®ÂàÜÁõ∏ÈóúÁü•Ë≠òÔºâ
- ‰Ωé‰ø°ÂøÉÂ∫¶ÂïèÈ°åÔºàÁÑ°Áõ∏ÈóúÁü•Ë≠òÔºâ
- Ë§áÈõúÂïèÈ°åÔºàÈúÄË¶Å SOP + ÂèÉÊï∏Ê≥®ÂÖ•Ôºâ

TODO: Â∞áÊ∏¨Ë©¶ÈÅ∑ÁßªÂà∞ /api/v1/message Á´ØÈªû
"""
import asyncio
import httpx
import time
import statistics
from typing import List, Dict
from datetime import datetime
import json

# API ÈÖçÁΩÆ
RAG_API_BASE = "http://localhost:8100"
VENDOR_ID = 1  # Ê∏¨Ë©¶Áî®Ê•≠ËÄÖ ID

# Ê∏¨Ë©¶ÂïèÈ°åÈõÜÂêà
TEST_QUESTIONS = {
    "high_confidence": [
        "ÁßüÈáëÊØèÂÄãÊúàÂπæËôüË¶ÅÁπ≥Ôºü",
        "ÊàëÊÉ≥È§äÂØµÁâ©ÂèØ‰ª•ÂóéÔºü",
        "ÈÄÄÁßüÈúÄË¶ÅÊèêÂâçÂ§ö‰πÖÂëäÁü•Ôºü",
        "ÊàøÁßüÂåÖÂê´Âì™‰∫õË≤ªÁî®Ôºü",
        "ÁßüÁ¥ÑÂà∞ÊúüÂ¶Ç‰ΩïÁ∫åÁ¥ÑÔºü"
    ],
    "medium_confidence": [
        "ÈõªË≤ªÊòØÊÄéÈ∫ºÁÆóÁöÑÔºü",
        "ÂÜ∑Ê∞£Â£û‰∫ÜÊÄéÈ∫ºËæ¶Ôºü",
        "ÂèØ‰ª•ÊèêÂâçËß£Á¥ÑÂóéÔºü",
        "ÊàøÈñìÂèØ‰ª•ÈáòÈáòÂ≠êÂóéÔºü",
        "ÂÅúËªä‰ΩçÊÄéÈ∫ºÊî∂Ë≤ªÔºü"
    ],
    "low_confidence": [
        "ÈôÑËøëÊúâ‰ªÄÈ∫ºÂ•ΩÂêÉÁöÑÈ§êÂª≥Ôºü",
        "ÈÄôÂÄãÁ§æÂçÄÂÆâÂÖ®ÂóéÔºü",
        "ÊàëÁöÑÈÑ∞Â±ÖÂæàÂêµÊÄéÈ∫ºËæ¶Ôºü",
        "‰ªäÂ§©Â§©Ê∞£Â¶Ç‰ΩïÔºü",
        "‰Ω†ËÉΩÂπ´ÊàëË®ÇÊä´Ëñ©ÂóéÔºü"
    ],
    "complex": [
        "ÊàëÁöÑÁßüÈáëÊòéÁ¥∞ÊòØ‰ªÄÈ∫ºÔºü",  # ÈúÄË¶Å API Êï¥Âêà
        "Âπ´ÊàëÊü•Ë©¢ÊàëÁöÑÁπ≥Ë≤ªË®òÈåÑ",  # ÈúÄË¶ÅÂèÉÊï∏Ê≥®ÂÖ•
        "ÊàëÁöÑÂêàÁ¥Ñ‰ªÄÈ∫ºÊôÇÂÄôÂà∞ÊúüÔºü",  # ÈúÄË¶ÅÁßüÂÆ¢Ë≠òÂà•
    ]
}


class PerformanceTester:
    """ÊïàËÉΩÊ∏¨Ë©¶Âô®"""

    def __init__(self):
        self.results = []
        self.client = None

    async def __aenter__(self):
        """ÈÄ≤ÂÖ•‰∏ä‰∏ãÊñáÊôÇÂâµÂª∫ HTTP ÂÆ¢Êà∂Á´Ø"""
        self.client = httpx.AsyncClient(timeout=60.0)
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ÈÄÄÂá∫‰∏ä‰∏ãÊñáÊôÇÈóúÈñâÂÆ¢Êà∂Á´Ø"""
        if self.client:
            await self.client.close()

    async def test_single_question(
        self,
        question: str,
        category: str,
        repeat: int = 1
    ) -> Dict:
        """Ê∏¨Ë©¶ÂñÆÂÄãÂïèÈ°å"""
        times = []
        responses = []

        for i in range(repeat):
            start_time = time.time()

            try:
                response = await self.client.post(
                    f"{RAG_API_BASE}/api/v1/chat",
                    json={
                        "question": question,
                        "vendor_id": VENDOR_ID,
                        "user_role": "customer",
                        "user_id": f"perf_test_{i}"
                    }
                )

                elapsed = (time.time() - start_time) * 1000  # ËΩâÊèõÁÇ∫ÊØ´Áßí

                if response.status_code == 200:
                    data = response.json()
                    times.append(elapsed)
                    responses.append(data)
                else:
                    print(f"   ‚ùå Ë´ãÊ±ÇÂ§±Êïó: {response.status_code}")
                    times.append(elapsed)
                    responses.append(None)

            except Exception as e:
                elapsed = (time.time() - start_time) * 1000
                print(f"   ‚ùå Áï∞Â∏∏: {e}")
                times.append(elapsed)
                responses.append(None)

        # Ë®àÁÆóÁµ±Ë®àÊï∏Êìö
        if times:
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            median_time = statistics.median(times)
            stdev_time = statistics.stdev(times) if len(times) > 1 else 0
        else:
            avg_time = min_time = max_time = median_time = stdev_time = 0

        # ÂæûÈüøÊáâ‰∏≠ÊèêÂèñË©≥Á¥∞ÊôÇÈñìÂàÜËß£ÔºàÂ¶ÇÊûúÂèØÁî®Ôºâ
        breakdown = None
        if responses and responses[0]:
            last_response = responses[0]
            breakdown = {
                "processing_time_ms": last_response.get("processing_time_ms", 0),
                "confidence_score": last_response.get("confidence_score", 0),
                "confidence_level": last_response.get("confidence_level", "unknown"),
                "requires_human": last_response.get("requires_human", False),
                "intent_type": last_response.get("intent", {}).get("intent_type", "unknown"),
                "retrieved_docs_count": len(last_response.get("retrieved_docs", []))
            }

        result = {
            "question": question,
            "category": category,
            "repeat_count": repeat,
            "avg_time_ms": round(avg_time, 2),
            "min_time_ms": round(min_time, 2),
            "max_time_ms": round(max_time, 2),
            "median_time_ms": round(median_time, 2),
            "stdev_ms": round(stdev_time, 2),
            "breakdown": breakdown,
            "success_rate": sum(1 for r in responses if r) / len(responses) * 100
        }

        self.results.append(result)
        return result

    async def run_tests(self, repeat_per_question: int = 3):
        """Âü∑Ë°åÊâÄÊúâÊ∏¨Ë©¶"""
        print("=" * 80)
        print("RAG Â∞çË©±ÊµÅÁ®ãÂÆåÊï¥ÊïàËÉΩÊ∏¨Ë©¶")
        print("=" * 80)
        print(f"Ê∏¨Ë©¶ÊôÇÈñì: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"API Á´ØÈªû: {RAG_API_BASE}")
        print(f"Ê•≠ËÄÖ ID: {VENDOR_ID}")
        print(f"ÊØèÂÄãÂïèÈ°åÈáçË§áÊ¨°Êï∏: {repeat_per_question}")
        print("=" * 80)

        total_tests = sum(len(questions) for questions in TEST_QUESTIONS.values())
        current_test = 0

        for category, questions in TEST_QUESTIONS.items():
            print(f"\nüìÇ Ê∏¨Ë©¶È°ûÂà•: {category.upper()}")
            print("-" * 80)

            for question in questions:
                current_test += 1
                print(f"\n[{current_test}/{total_tests}] Ê∏¨Ë©¶ÂïèÈ°å: {question}")

                result = await self.test_single_question(
                    question,
                    category,
                    repeat=repeat_per_question
                )

                # È°ØÁ§∫Âç≥ÊôÇÁµêÊûú
                print(f"   Âπ≥ÂùáÊôÇÈñì: {result['avg_time_ms']:.2f} ms")
                print(f"   ÁØÑÂúç: {result['min_time_ms']:.2f} - {result['max_time_ms']:.2f} ms")

                if result['breakdown']:
                    bd = result['breakdown']
                    print(f"   ‰ø°ÂøÉÂ∫¶: {bd['confidence_score']:.2f} ({bd['confidence_level']})")
                    print(f"   ÊÑèÂúñÈ°ûÂûã: {bd['intent_type']}")
                    print(f"   Ê™¢Á¥¢ÊñáÊ™îÊï∏: {bd['retrieved_docs_count']}")
                    print(f"   ÈúÄË¶Å‰∫∫Â∑•: {'ÊòØ' if bd['requires_human'] else 'Âê¶'}")

                # ÈÅøÂÖçÈÅéÂø´Ë´ãÊ±Ç
                await asyncio.sleep(0.5)

    def generate_report(self) -> Dict:
        """ÁîüÊàêÊïàËÉΩÂ†±Âëä"""
        if not self.results:
            return {"error": "Ê≤íÊúâÊ∏¨Ë©¶ÁµêÊûú"}

        # ÊåâÈ°ûÂà•ÂàÜÁµÑÁµ±Ë®à
        category_stats = {}
        for category in TEST_QUESTIONS.keys():
            category_results = [r for r in self.results if r['category'] == category]

            if category_results:
                avg_times = [r['avg_time_ms'] for r in category_results]
                category_stats[category] = {
                    "count": len(category_results),
                    "avg_time_ms": round(statistics.mean(avg_times), 2),
                    "min_time_ms": round(min(avg_times), 2),
                    "max_time_ms": round(max(avg_times), 2),
                    "median_time_ms": round(statistics.median(avg_times), 2),
                    "success_rate": round(
                        statistics.mean([r['success_rate'] for r in category_results]), 2
                    )
                }

        # ÂÖ®Â±ÄÁµ±Ë®à
        all_avg_times = [r['avg_time_ms'] for r in self.results]
        global_stats = {
            "total_tests": len(self.results),
            "overall_avg_ms": round(statistics.mean(all_avg_times), 2),
            "overall_min_ms": round(min(all_avg_times), 2),
            "overall_max_ms": round(max(all_avg_times), 2),
            "overall_median_ms": round(statistics.median(all_avg_times), 2),
            "overall_stdev_ms": round(statistics.stdev(all_avg_times), 2),
            "success_rate": round(
                statistics.mean([r['success_rate'] for r in self.results]), 2
            )
        }

        # ‰ø°ÂøÉÂ∫¶ÂàÜÂ∏É
        confidence_distribution = {}
        for result in self.results:
            if result['breakdown']:
                level = result['breakdown']['confidence_level']
                confidence_distribution[level] = confidence_distribution.get(level, 0) + 1

        # Ë≠òÂà•ÊÖ¢Êü•Ë©¢ÔºàÂ§ßÊñº P95Ôºâ
        p95_time = sorted(all_avg_times)[int(len(all_avg_times) * 0.95)]
        slow_queries = [
            {
                "question": r['question'],
                "avg_time_ms": r['avg_time_ms'],
                "category": r['category']
            }
            for r in self.results
            if r['avg_time_ms'] > p95_time
        ]

        return {
            "test_summary": {
                "test_time": datetime.now().isoformat(),
                "api_endpoint": RAG_API_BASE,
                "vendor_id": VENDOR_ID
            },
            "global_stats": global_stats,
            "category_stats": category_stats,
            "confidence_distribution": confidence_distribution,
            "slow_queries": {
                "p95_threshold_ms": round(p95_time, 2),
                "count": len(slow_queries),
                "queries": slow_queries
            },
            "detailed_results": self.results
        }

    def print_report(self, report: Dict):
        """ÊâìÂç∞Â†±Âëä"""
        print("\n" + "=" * 80)
        print("üìä ÊïàËÉΩÊ∏¨Ë©¶Â†±Âëä")
        print("=" * 80)

        # ÂÖ®Â±ÄÁµ±Ë®à
        print("\nüåê ÂÖ®Â±ÄÁµ±Ë®à")
        print("-" * 80)
        gs = report['global_stats']
        print(f"Á∏ΩÊ∏¨Ë©¶Êï∏: {gs['total_tests']}")
        print(f"ÊàêÂäüÁéá: {gs['success_rate']}%")
        print(f"Âπ≥ÂùáÈüøÊáâÊôÇÈñì: {gs['overall_avg_ms']:.2f} ms")
        print(f"‰∏≠‰ΩçÊï∏ÊôÇÈñì: {gs['overall_median_ms']:.2f} ms")
        print(f"ÊúÄÂ∞èÊôÇÈñì: {gs['overall_min_ms']:.2f} ms")
        print(f"ÊúÄÂ§ßÊôÇÈñì: {gs['overall_max_ms']:.2f} ms")
        print(f"Ê®ôÊ∫ñÂ∑Æ: {gs['overall_stdev_ms']:.2f} ms")

        # È°ûÂà•Áµ±Ë®à
        print("\nüìÇ ÂêÑÈ°ûÂà•Áµ±Ë®à")
        print("-" * 80)
        for category, stats in report['category_stats'].items():
            print(f"\n{category.upper()}:")
            print(f"  Ê∏¨Ë©¶Êï∏: {stats['count']}")
            print(f"  Âπ≥ÂùáÊôÇÈñì: {stats['avg_time_ms']:.2f} ms")
            print(f"  ÁØÑÂúç: {stats['min_time_ms']:.2f} - {stats['max_time_ms']:.2f} ms")
            print(f"  ÊàêÂäüÁéá: {stats['success_rate']}%")

        # ‰ø°ÂøÉÂ∫¶ÂàÜÂ∏É
        print("\nüéØ ‰ø°ÂøÉÂ∫¶ÂàÜÂ∏É")
        print("-" * 80)
        for level, count in report['confidence_distribution'].items():
            percentage = (count / gs['total_tests']) * 100
            print(f"{level}: {count} ({percentage:.1f}%)")

        # ÊÖ¢Êü•Ë©¢
        print("\n‚ö†Ô∏è  ÊÖ¢Êü•Ë©¢ (P95)")
        print("-" * 80)
        print(f"P95 ÈñæÂÄº: {report['slow_queries']['p95_threshold_ms']:.2f} ms")
        print(f"ÊÖ¢Êü•Ë©¢Êï∏Èáè: {report['slow_queries']['count']}")
        for sq in report['slow_queries']['queries']:
            print(f"  - [{sq['category']}] {sq['question']}: {sq['avg_time_ms']:.2f} ms")

        # ÊïàËÉΩË©ïÁ¥ö
        print("\n‚≠ê ÊïàËÉΩË©ïÁ¥ö")
        print("-" * 80)
        avg_ms = gs['overall_avg_ms']
        if avg_ms < 1000:
            grade = "A (ÂÑ™ÁßÄ)"
            emoji = "üü¢"
        elif avg_ms < 2000:
            grade = "B (ËâØÂ•Ω)"
            emoji = "üü°"
        elif avg_ms < 3000:
            grade = "C (Â∞öÂèØ)"
            emoji = "üü†"
        else:
            grade = "D (ÈúÄÊîπÈÄ≤)"
            emoji = "üî¥"

        print(f"{emoji} Âπ≥ÂùáÈüøÊáâÊôÇÈñì {avg_ms:.2f} ms - Ë©ïÁ¥ö: {grade}")

        # Âª∫Ë≠∞
        print("\nüí° ÂÑ™ÂåñÂª∫Ë≠∞")
        print("-" * 80)
        if avg_ms > 2000:
            print("  - Âπ≥ÂùáÈüøÊáâÊôÇÈñìËºÉÈï∑ÔºåÂª∫Ë≠∞Ê™¢Êü• LLM API Âª∂ÈÅ≤")
        if gs['overall_stdev_ms'] > 500:
            print("  - ÈüøÊáâÊôÇÈñìÊ≥¢ÂãïËºÉÂ§ßÔºåÂª∫Ë≠∞Ê™¢Êü•ÊúçÂãôÁ©©ÂÆöÊÄß")
        if report['slow_queries']['count'] > 0:
            print(f"  - ÁôºÁèæ {report['slow_queries']['count']} ÂÄãÊÖ¢Êü•Ë©¢ÔºåÂª∫Ë≠∞ÈáùÂ∞çÊÄßÂÑ™Âåñ")

        print("\n" + "=" * 80)


async def main():
    """‰∏ªÂáΩÊï∏"""
    async with PerformanceTester() as tester:
        # Âü∑Ë°åÊ∏¨Ë©¶
        await tester.run_tests(repeat_per_question=3)

        # ÁîüÊàêÂ†±Âëä
        report = tester.generate_report()

        # ÊâìÂç∞Â†±Âëä
        tester.print_report(report)

        # ‰øùÂ≠òÂ†±ÂëäÂà∞Êñá‰ª∂
        output_file = f"/tmp/chat_performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nüìÑ Ë©≥Á¥∞Â†±ÂëäÂ∑≤‰øùÂ≠òËá≥: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())
