#!/usr/bin/env python3
"""
LLM Provider é›†æˆæ¸¬è©¦
é©—è­‰ä¿®æ”¹å¾Œçš„æœå‹™(IntentClassifier, LLMAnswerOptimizer)æ˜¯å¦æ­£å¸¸é‹ä½œ
"""
import os
import sys

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(project_root, 'rag-orchestrator'))

from services.intent_classifier import IntentClassifier
from services.llm_answer_optimizer import LLMAnswerOptimizer


def test_intent_classifier():
    """æ¸¬è©¦ IntentClassifier æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 1: IntentClassifier with LLM Provider")
    print("=" * 60)

    try:
        # åˆå§‹åŒ–åˆ†é¡å™¨(æœƒè‡ªå‹•ä½¿ç”¨ LLM Provider)
        classifier = IntentClassifier(use_database=False)
        print("âœ… IntentClassifier åˆå§‹åŒ–æˆåŠŸ")
        print(f"   ä½¿ç”¨ Provider: {classifier.llm_provider.provider_name}")

        # æ¸¬è©¦åˆ†é¡
        test_question = "ç§Ÿé‡‘ä»€éº¼æ™‚å€™è¦ç¹³ç´ï¼Ÿ"
        print(f"\næ¸¬è©¦å•é¡Œ: {test_question}")

        result = classifier.classify(test_question)

        print(f"\nğŸ“ åˆ†é¡çµæœ:")
        print(f"   æ„åœ–: {result.get('intent_name', 'N/A')}")
        print(f"   ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.2f}")
        print(f"   é—œéµè©: {result.get('keywords', [])}")

        # é©—è­‰çµæœçµæ§‹
        assert 'intent_name' in result, "ç¼ºå°‘ intent_name"
        assert 'confidence' in result, "ç¼ºå°‘ confidence"

        print("\nâœ… IntentClassifier æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"\nâŒ IntentClassifier æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_llm_answer_optimizer():
    """æ¸¬è©¦ LLMAnswerOptimizer æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 2: LLMAnswerOptimizer with LLM Provider")
    print("=" * 60)

    try:
        # åˆå§‹åŒ–å„ªåŒ–å™¨(æœƒè‡ªå‹•ä½¿ç”¨ LLM Provider)
        optimizer = LLMAnswerOptimizer()
        print("âœ… LLMAnswerOptimizer åˆå§‹åŒ–æˆåŠŸ")
        print(f"   ä½¿ç”¨ Provider: {optimizer.llm_provider.provider_name}")

        # æ¸¬è©¦ç­”æ¡ˆå„ªåŒ–
        test_question = "ç§Ÿé‡‘æ€éº¼ç¹³ï¼Ÿ"
        test_search_results = [{
            'question_summary': 'ç§Ÿé‡‘ç¹³ç´æ–¹å¼',
            'content': 'ç§Ÿé‡‘æ–¼æ¯æœˆ5æ—¥å‰ç¹³ç´ï¼Œå¯é€ééŠ€è¡Œè½‰å¸³æˆ–ç¾é‡‘æ”¯ä»˜ã€‚',
            'similarity': 0.85,
            'title': 'ç§Ÿé‡‘ç¹³ç´æ–¹å¼'
        }]
        test_intent_info = {
            'intent_name': 'å¸³å‹™æŸ¥è©¢',
            'intent_type': 'knowledge',
            'description': 'ç§Ÿé‡‘ç¹³ç´ç›¸é—œå•é¡Œ',
            'confidence': 0.9
        }

        print(f"\næ¸¬è©¦å•é¡Œ: {test_question}")
        print(f"æœå°‹çµæœæ•¸: {len(test_search_results)}")

        # æ¸¬è©¦ç­”æ¡ˆå„ªåŒ–
        result = optimizer.optimize_answer(
            question=test_question,
            search_results=test_search_results,
            confidence_level='high',
            intent_info=test_intent_info,
            confidence_score=0.85
        )

        optimized = result.get('optimized_answer', '')
        tokens = result.get('optimization_stats', {}).get('total_tokens', 0)

        print(f"\nğŸ“ å„ªåŒ–çµæœ:")
        print(f"   å„ªåŒ–ç­”æ¡ˆ: {optimized[:100]}...")
        print(f"   ä½¿ç”¨ tokens: {tokens}")

        # é©—è­‰çµæœ
        assert optimized is not None, "å„ªåŒ–ç­”æ¡ˆç‚º None"
        assert len(optimized) > 0, "å„ªåŒ–ç­”æ¡ˆç‚ºç©º"
        # Note: tokens å¯èƒ½ç‚º 0 (ä½¿ç”¨å¿«é€Ÿè·¯å¾‘æˆ–æ¨¡æ¿æ ¼å¼åŒ–)
        # assert tokens > 0, "tokens æ•¸é‡ç•°å¸¸"

        print("\nâœ… LLMAnswerOptimizer æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"\nâŒ LLMAnswerOptimizer æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_provider_switching():
    """æ¸¬è©¦ Provider åˆ‡æ›åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 3: Provider åœ¨ä¸åŒæœå‹™é–“çš„ä¸€è‡´æ€§")
    print("=" * 60)

    try:
        # åˆå§‹åŒ–å…©å€‹æœå‹™
        classifier = IntentClassifier(use_database=False)
        optimizer = LLMAnswerOptimizer()

        print(f"IntentClassifier Provider: {classifier.llm_provider.provider_name}")
        print(f"LLMAnswerOptimizer Provider: {optimizer.llm_provider.provider_name}")

        # é©—è­‰ä½¿ç”¨åŒä¸€å€‹ Provider å¯¦ä¾‹(å–®ä¾‹æ¨¡å¼)
        assert classifier.llm_provider.provider_name == optimizer.llm_provider.provider_name, \
            "å…©å€‹æœå‹™ä½¿ç”¨ä¸åŒçš„ Provider"

        print("\nâœ… Provider ä¸€è‡´æ€§æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"\nâŒ Provider ä¸€è‡´æ€§æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_backward_compatibility():
    """æ¸¬è©¦å‘å¾Œå…¼å®¹æ€§ - ç¢ºä¿åœ¨ LLM_PROVIDER=openai æ™‚è¡Œç‚ºä¸è®Š"""
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ 4: å‘å¾Œå…¼å®¹æ€§ (LLM_PROVIDER=openai)")
    print("=" * 60)

    try:
        # ç¢ºèªç’°å¢ƒè®Šæ•¸
        provider_type = os.getenv("LLM_PROVIDER", "openai")
        print(f"ç•¶å‰ LLM_PROVIDER: {provider_type}")

        if provider_type != "openai":
            print("âš ï¸  è·³é(ç•¶å‰ä¸æ˜¯ä½¿ç”¨ openai)")
            return None

        # æ¸¬è©¦æœå‹™åˆå§‹åŒ–
        classifier = IntentClassifier(use_database=False)
        optimizer = LLMAnswerOptimizer()

        # é©—è­‰ä½¿ç”¨ OpenAI Provider
        assert classifier.llm_provider.provider_name == "OpenAI", \
            f"é æœŸ OpenAIï¼Œå¯¦éš›: {classifier.llm_provider.provider_name}"
        assert optimizer.llm_provider.provider_name == "OpenAI", \
            f"é æœŸ OpenAIï¼Œå¯¦éš›: {optimizer.llm_provider.provider_name}"

        print("âœ… ä½¿ç”¨ OpenAI Provider")
        print("âœ… å‘å¾Œå…¼å®¹æ€§æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"\nâŒ å‘å¾Œå…¼å®¹æ€§æ¸¬è©¦å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("=" * 60)
    print("LLM Provider é›†æˆæ¸¬è©¦")
    print("=" * 60)

    # é¡¯ç¤ºç•¶å‰ç’°å¢ƒ
    print("\nğŸ“‹ ç’°å¢ƒé…ç½®:")
    print(f"   LLM_PROVIDER: {os.getenv('LLM_PROVIDER', '(æœªè¨­å®š,é»˜èª openai)')}")
    print(f"   OPENAI_API_KEY: {'âœ… å·²è¨­å®š' if os.getenv('OPENAI_API_KEY') else 'âŒ æœªè¨­å®š'}")

    results = {}

    # åŸ·è¡Œæ¸¬è©¦
    results['intent_classifier'] = test_intent_classifier()
    results['answer_optimizer'] = test_llm_answer_optimizer()
    results['provider_consistency'] = test_provider_switching()
    results['backward_compatibility'] = test_backward_compatibility()

    # ç¸½çµ
    print("\n" + "=" * 60)
    print("æ¸¬è©¦ç¸½çµ")
    print("=" * 60)

    for test_name, result in results.items():
        if result is True:
            status = "âœ… é€šé"
        elif result is None:
            status = "âš ï¸  è·³é"
        else:
            status = "âŒ å¤±æ•—"

        print(f"   {test_name:30s}: {status}")

    # è¨ˆç®—é€šéç‡
    passed = sum(1 for r in results.values() if r is True)
    total = len([r for r in results.values() if r is not None])
    skipped = sum(1 for r in results.values() if r is None)

    print(f"\né€šé: {passed}/{total}")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ä¿®æ”¹å¾Œçš„æœå‹™é‹ä½œæ­£å¸¸")
        print("\næ¥ä¸‹ä¾†å¯ä»¥:")
        print("  1. Commit ç•¶å‰ä¿®æ”¹")
        print("  2. ç¹¼çºŒå®Œæˆå‰©é¤˜æœå‹™çš„ä¿®æ”¹")
        return 0
    else:
        print("\nâš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦ä¿®å¾©")
        return 1


if __name__ == "__main__":
    sys.exit(main())
