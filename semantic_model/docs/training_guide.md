# çœŸå¯¦è³‡æ–™è¨“ç·´å®Œå…¨æŒ‡å—

## ğŸ¯ ç†è§£ã€ŒçœŸå¯¦è³‡æ–™è¨“ç·´ã€

### ä»€éº¼æ˜¯çœŸå¯¦è³‡æ–™ï¼Ÿ

**ä¸æ˜¯**ï¼šéš¨ä¾¿å¯«ä¸€äº›æ¸¬è©¦å¥å­
**è€Œæ˜¯**ï¼šæ‚¨ç³»çµ±ä¸­å¯¦éš›çš„è³‡æ–™

åŒ…å«ä¸‰å€‹éƒ¨åˆ†ï¼š
1. **çŸ¥è­˜åº«å…§å®¹** - æ‚¨è³‡æ–™åº«ä¸­çš„æ‰€æœ‰çŸ¥è­˜é»
2. **ç”¨æˆ¶æŸ¥è©¢æ­·å²** - ç”¨æˆ¶å¯¦éš›å•éçš„å•é¡Œï¼ˆå¦‚æœæœ‰è¨˜éŒ„ï¼‰
3. **æ­£ç¢ºé…å°é—œä¿‚** - å“ªå€‹æŸ¥è©¢æ‡‰è©²åŒ¹é…å“ªå€‹çŸ¥è­˜é»

## ğŸ“Š Step 1: æå–æ‚¨çš„çŸ¥è­˜åº«

### 1.1 åŒ¯å‡ºçŸ¥è­˜åº«è³‡æ–™

```python
# semantic_model/scripts/extract_knowledge.py

import psycopg2
import json

def extract_knowledge_base():
    """å¾è³‡æ–™åº«æå–æ‰€æœ‰çŸ¥è­˜é»"""

    conn = psycopg2.connect(
        host="localhost",
        database="aichatbot",
        user="aichatbot_user",
        password="aichatbot_password"
    )

    cursor = conn.cursor()

    # æå–æ‰€æœ‰çŸ¥è­˜é»
    cursor.execute("""
        SELECT
            id,
            title,
            content,
            action_type,
            form_id,
            priority
        FROM knowledge_base
        WHERE vendor_id = 1
    """)

    knowledge_points = []
    for row in cursor.fetchall():
        knowledge_points.append({
            "id": row[0],
            "title": row[1],
            "content": row[2],
            "action_type": row[3],
            "form_id": row[4],
            "priority": row[5]
        })

    # ä¿å­˜ç‚º JSON
    with open("data/knowledge_base.json", "w", encoding="utf-8") as f:
        json.dump(knowledge_points, f, ensure_ascii=False, indent=2)

    print(f"âœ… æå–äº† {len(knowledge_points)} å€‹çŸ¥è­˜é»")
    return knowledge_points

if __name__ == "__main__":
    extract_knowledge_base()
```

åŸ·è¡Œï¼š
```bash
python semantic_model/scripts/extract_knowledge.py
```

### 1.2 åˆ†æçŸ¥è­˜é»é¡å‹

```python
# analyze_knowledge.py

def analyze_knowledge_types():
    """åˆ†æçŸ¥è­˜é»çš„é¡å‹åˆ†å¸ƒ"""

    with open("data/knowledge_base.json", "r", encoding="utf-8") as f:
        knowledge = json.load(f)

    # çµ±è¨ˆ
    stats = {
        "total": len(knowledge),
        "by_action_type": {},
        "has_form": 0,
        "patterns_found": {}
    }

    for kb in knowledge:
        # çµ±è¨ˆ action_type
        action = kb["action_type"]
        stats["by_action_type"][action] = stats["by_action_type"].get(action, 0) + 1

        # çµ±è¨ˆè¡¨å–®
        if kb["form_id"]:
            stats["has_form"] += 1

        # è­˜åˆ¥æ¨¡å¼
        title = kb["title"]
        if any(word in title for word in ["æ™‚é–“", "å¹¾è™Ÿ", "ä½•æ™‚", "æœŸé™"]):
            stats["patterns_found"]["time"] = stats["patterns_found"].get("time", 0) + 1
        if any(word in title for word in ["è²»ç”¨", "åƒ¹æ ¼", "å¤šå°‘éŒ¢"]):
            stats["patterns_found"]["cost"] = stats["patterns_found"].get("cost", 0) + 1

    print("çŸ¥è­˜åº«åˆ†æçµæœï¼š")
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    return stats
```

## ğŸ“ Step 2: å»ºç«‹è¨“ç·´æ•¸æ“šé›†

### 2.1 ç”ŸæˆæŸ¥è©¢-çŸ¥è­˜é…å°

```python
# semantic_model/scripts/generate_training_data.py

def generate_query_knowledge_pairs():
    """ç”ŸæˆæŸ¥è©¢-çŸ¥è­˜é»é…å°ä½œç‚ºè¨“ç·´æ•¸æ“š"""

    # è¼‰å…¥çŸ¥è­˜åº«
    with open("data/knowledge_base.json", "r", encoding="utf-8") as f:
        knowledge_base = json.load(f)

    training_pairs = []

    # ç‚ºæ¯å€‹çŸ¥è­˜é»ç”Ÿæˆå¯èƒ½çš„æŸ¥è©¢
    for kb in knowledge_base:
        # åŸºæ–¼æ¨™é¡Œç”ŸæˆæŸ¥è©¢è®ŠåŒ–
        title = kb["title"]

        if "é›»è²»" in title and "å¯„é€" in title:
            # é›»è²»å¯„é€ç›¸é—œ
            queries = [
                "é›»è²»å¹¾è™Ÿå¯„",
                "é›»è²»ä»€éº¼æ™‚å€™å¯„é€",
                "é›»è²»å¸³å–®å¯„é€æ™‚é–“",
                "æŸ¥è©¢é›»è²»å¯„é€å€é–“",
                "å–®æœˆé›»è²»ä½•æ™‚å¯„",
                "é›™æœˆé›»è²»å¯„é€æ™‚é–“"
            ]
            for q in queries:
                training_pairs.append({
                    "query": q,
                    "knowledge_id": kb["id"],
                    "knowledge_content": kb["content"],
                    "is_match": True,  # æ­£ä¾‹
                    "pattern": "time_query"
                })

        elif "ç§Ÿå±‹" in title and "é ˆçŸ¥" in title:
            # ç§Ÿå±‹è¦å®šç›¸é—œ
            queries = [
                "ç§Ÿå±‹è¦å®š",
                "ç§Ÿå±‹é ˆçŸ¥",
                "æ‰¿ç§Ÿæ³¨æ„äº‹é …",
                "æˆ¿å®¢è¦å®š"
            ]
            for q in queries:
                training_pairs.append({
                    "query": q,
                    "knowledge_id": kb["id"],
                    "knowledge_content": kb["content"],
                    "is_match": True,
                    "pattern": "regulation"
                })

        # ... ç‚ºå…¶ä»–é¡å‹ç”Ÿæˆ

    # ç”Ÿæˆè² ä¾‹ï¼ˆä¸æ‡‰è©²åŒ¹é…çš„ï¼‰
    negative_pairs = generate_negative_examples(training_pairs, knowledge_base)
    training_pairs.extend(negative_pairs)

    # ä¿å­˜è¨“ç·´æ•¸æ“š
    with open("data/training_data.json", "w", encoding="utf-8") as f:
        json.dump(training_pairs, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç”Ÿæˆäº† {len(training_pairs)} å€‹è¨“ç·´æ¨£æœ¬")
    return training_pairs

def generate_negative_examples(positive_pairs, knowledge_base):
    """ç”Ÿæˆè² ä¾‹ï¼šæŸ¥è©¢ä¸æ‡‰è©²åŒ¹é…çš„çŸ¥è­˜"""

    negative_pairs = []

    for pos in positive_pairs[:100]:  # å–å‰100å€‹æ­£ä¾‹
        # éš¨æ©Ÿé¸æ“‡ä¸€å€‹ä¸ç›¸é—œçš„çŸ¥è­˜é»
        wrong_kb = random.choice(knowledge_base)

        # ç¢ºä¿ä¸æ˜¯æ­£ç¢ºç­”æ¡ˆ
        if wrong_kb["id"] != pos["knowledge_id"]:
            negative_pairs.append({
                "query": pos["query"],
                "knowledge_id": wrong_kb["id"],
                "knowledge_content": wrong_kb["content"],
                "is_match": False,  # è² ä¾‹
                "pattern": pos["pattern"]
            })

    return negative_pairs
```

### 2.2 å¦‚æœæœ‰æ­·å²æŸ¥è©¢è¨˜éŒ„

```python
# extract_historical_queries.py

def extract_user_queries():
    """æå–çœŸå¯¦çš„ç”¨æˆ¶æŸ¥è©¢æ­·å²"""

    conn = psycopg2.connect(
        host="localhost",
        database="aichatbot",
        user="aichatbot_user",
        password="aichatbot_password"
    )

    cursor = conn.cursor()

    # å‡è¨­æ‚¨æœ‰æŸ¥è©¢æ—¥èªŒè¡¨
    cursor.execute("""
        SELECT
            user_query,
            matched_knowledge_id,
            user_feedback,
            created_at
        FROM query_logs
        WHERE created_at > NOW() - INTERVAL '30 days'
        ORDER BY created_at DESC
    """)

    real_queries = []
    for row in cursor.fetchall():
        real_queries.append({
            "query": row[0],
            "matched_id": row[1],
            "was_correct": row[2] == 'positive',  # å‡è¨­æœ‰ç”¨æˆ¶åé¥‹
            "timestamp": row[3].isoformat()
        })

    print(f"âœ… æå–äº† {len(real_queries)} å€‹çœŸå¯¦æŸ¥è©¢")

    # é€™äº›æ˜¯æœ€å¯¶è²´çš„è¨“ç·´æ•¸æ“šï¼
    return real_queries
```

## ğŸš€ Step 3: åŸ·è¡Œè¨“ç·´

### 3.1 å®Œæ•´è¨“ç·´è…³æœ¬

```python
# semantic_model/scripts/train.py

from sentence_transformers import CrossEncoder, InputExample
import json
from sklearn.model_selection import train_test_split

def train_semantic_model():
    """è¨“ç·´èªç¾©ç†è§£æ¨¡å‹"""

    print("="*60)
    print("é–‹å§‹è¨“ç·´èªç¾©æ¨¡å‹")
    print("="*60)

    # 1. è¼‰å…¥è¨“ç·´æ•¸æ“š
    with open("data/training_data.json", "r", encoding="utf-8") as f:
        training_data = json.load(f)

    print(f"è¼‰å…¥äº† {len(training_data)} å€‹è¨“ç·´æ¨£æœ¬")

    # 2. æº–å‚™è¨“ç·´æ ¼å¼
    train_examples = []
    for item in training_data:
        # CrossEncoder éœ€è¦çš„æ ¼å¼ï¼š(æŸ¥è©¢, æ–‡æª”) -> æ˜¯å¦ç›¸é—œ
        example = InputExample(
            texts=[item["query"], item["knowledge_content"]],
            label=float(item["is_match"])  # True=1.0, False=0.0
        )
        train_examples.append(example)

    # 3. åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†
    train_samples, val_samples = train_test_split(
        train_examples,
        test_size=0.2,
        random_state=42
    )

    print(f"è¨“ç·´é›†: {len(train_samples)} æ¨£æœ¬")
    print(f"é©—è­‰é›†: {len(val_samples)} æ¨£æœ¬")

    # 4. åˆå§‹åŒ–æ¨¡å‹
    model = CrossEncoder('BAAI/bge-reranker-base', num_labels=1)

    # 5. è¨“ç·´åƒæ•¸
    print("\nè¨“ç·´é…ç½®ï¼š")
    print("- åŸºç¤æ¨¡å‹: BAAI/bge-reranker-base")
    print("- Epochs: 3")
    print("- Batch Size: 16")
    print("- é è¨ˆæ™‚é–“: 30-60åˆ†é˜ (GPU) / 2-3å°æ™‚ (CPU)")

    # 6. é–‹å§‹è¨“ç·´
    model.fit(
        train_examples=train_samples,
        dev_examples=val_samples,
        epochs=3,
        batch_size=16,
        warmup_steps=100,
        evaluation_steps=500,
        output_path='models/semantic_v1',
        save_best_model=True
    )

    print("\nâœ… è¨“ç·´å®Œæˆï¼")
    print("æ¨¡å‹ä¿å­˜åœ¨: models/semantic_v1/")

    return model

if __name__ == "__main__":
    train_semantic_model()
```

### 3.2 è¨“ç·´åŸ·è¡Œå‘½ä»¤

```bash
# CPU è¨“ç·´ï¼ˆè¼ƒæ…¢ï¼‰
python semantic_model/scripts/train.py

# GPU è¨“ç·´ï¼ˆè¼ƒå¿«ï¼Œå¦‚æœæœ‰ CUDAï¼‰
CUDA_VISIBLE_DEVICES=0 python semantic_model/scripts/train.py
```

## ğŸ“Š Step 4: è©•ä¼°æ¨¡å‹æ•ˆæœ

### 4.1 æ¸¬è©¦è…³æœ¬

```python
# semantic_model/scripts/evaluate.py

def evaluate_model():
    """è©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹"""

    # è¼‰å…¥æ¨¡å‹
    model = CrossEncoder('models/semantic_v1')

    # æ¸¬è©¦é›†
    test_queries = [
        {"query": "é›»è²»å¹¾è™Ÿå¯„", "expected_id": 1296},
        {"query": "ç§Ÿå±‹è¦å®š", "expected_id": 1295},
        {"query": "ç®¡ç†è²»å¤šå°‘", "expected_id": 1297},
        # ... æ›´å¤šæ¸¬è©¦
    ]

    # è¼‰å…¥çŸ¥è­˜åº«
    with open("data/knowledge_base.json", "r") as f:
        knowledge_base = json.load(f)

    correct = 0
    total = len(test_queries)

    for test in test_queries:
        # å°æ‰€æœ‰çŸ¥è­˜é»è©•åˆ†
        scores = []
        for kb in knowledge_base:
            score = model.predict([(test["query"], kb["content"])])[0]
            scores.append((kb["id"], score))

        # é¸æ“‡æœ€é«˜åˆ†
        best_id = max(scores, key=lambda x: x[1])[0]

        if best_id == test["expected_id"]:
            correct += 1
            print(f"âœ… {test['query']} -> æ­£ç¢º")
        else:
            print(f"âŒ {test['query']} -> éŒ¯èª¤ (é æœŸ:{test['expected_id']}, å¯¦éš›:{best_id})")

    accuracy = correct / total * 100
    print(f"\næº–ç¢ºç‡: {accuracy:.1f}%")

    return accuracy
```

## ğŸ”„ Step 5: æŒçºŒå„ªåŒ–å¾ªç’°

```python
# semantic_model/scripts/continuous_improvement.py

def improvement_cycle():
    """æŒçºŒæ”¹é€²å¾ªç’°"""

    while True:
        # 1. æ”¶é›†ä¸€é€±çš„æ–°æŸ¥è©¢
        new_queries = collect_weekly_queries()

        # 2. äººå·¥æ¨™è¨»ï¼ˆæˆ–åŠè‡ªå‹•ï¼‰
        annotated = annotate_queries(new_queries)

        # 3. åŠ å…¥è¨“ç·´é›†
        add_to_training_set(annotated)

        # 4. æ¯æœˆé‡æ–°è¨“ç·´
        if is_month_end():
            retrain_model()

        # 5. A/B æ¸¬è©¦æ–°æ¨¡å‹
        if new_model_ready():
            run_ab_test()
```

## ğŸ’¡ å¯¦ç”¨å»ºè­°

### 1. å¾å°é–‹å§‹

ä¸éœ€è¦ä¸€æ¬¡æº–å‚™10000å€‹è¨“ç·´æ¨£æœ¬ã€‚é–‹å§‹æ™‚ï¼š
- 100å€‹æ­£ä¾‹ï¼ˆæ­£ç¢ºé…å°ï¼‰
- 100å€‹è² ä¾‹ï¼ˆéŒ¯èª¤é…å°ï¼‰
- å°±èƒ½çœ‹åˆ°æ•ˆæœ

### 2. é‡é»å„ªåŒ–é«˜é »æŸ¥è©¢

```python
# æ‰¾å‡ºæœ€å¸¸è¦‹çš„æŸ¥è©¢é¡å‹
SELECT query_pattern, COUNT(*) as freq
FROM query_logs
GROUP BY query_pattern
ORDER BY freq DESC
LIMIT 20;
```

å„ªå…ˆç‚ºé€™äº›é«˜é »æŸ¥è©¢æº–å‚™è¨“ç·´æ•¸æ“šã€‚

### 3. ä½¿ç”¨çœŸå¯¦åé¥‹

å¦‚æœæ‚¨çš„ç³»çµ±æœ‰ç”¨æˆ¶åé¥‹æ©Ÿåˆ¶ï¼š
```python
# æœ€æœ‰åƒ¹å€¼çš„è¨“ç·´æ•¸æ“š
SELECT
    query,
    knowledge_id,
    CASE
        WHEN user_feedback = 'helpful' THEN 1.0
        ELSE 0.0
    END as label
FROM query_logs
WHERE user_feedback IS NOT NULL;
```

## ğŸ“ˆ é æœŸæ•ˆæœ

| è¨“ç·´æ•¸æ“šé‡ | é æœŸæº–ç¢ºç‡ | è¨“ç·´æ™‚é–“(CPU) |
|-----------|----------|-------------|
| 200 æ¨£æœ¬   | 70-75%   | 30 åˆ†é˜      |
| 1000 æ¨£æœ¬  | 80-85%   | 2 å°æ™‚       |
| 5000 æ¨£æœ¬  | 85-90%   | 6 å°æ™‚       |
| 10000+ æ¨£æœ¬ | 90-95%  | 12 å°æ™‚      |

## âš¡ å¿«é€Ÿé–‹å§‹å‘½ä»¤

```bash
# 1. æå–çŸ¥è­˜åº«
python semantic_model/scripts/extract_knowledge.py

# 2. ç”Ÿæˆè¨“ç·´æ•¸æ“š
python semantic_model/scripts/generate_training_data.py

# 3. è¨“ç·´æ¨¡å‹
python semantic_model/scripts/train.py

# 4. è©•ä¼°æ•ˆæœ
python semantic_model/scripts/evaluate.py
```

å®Œæˆé€™4æ­¥ï¼Œæ‚¨å°±æœ‰äº†ä¸€å€‹è¨“ç·´å¥½çš„èªç¾©æ¨¡å‹ï¼