"""
LINE èŠå¤©è¨˜éŒ„çŸ¥è­˜æå–èˆ‡æ¸¬è©¦æƒ…å¢ƒç´¯ç©è…³æœ¬ï¼ˆå„ªåŒ–ç‰ˆï¼‰
å¸¶æœ‰è©³ç´°é€²åº¦é¡¯ç¤ºå’Œæ—¥èªŒè¨˜éŒ„
"""

import os
import re
import json
from datetime import datetime
from typing import List, Dict, Tuple
import pandas as pd
from openai import OpenAI
import time
import sys

class LineKnowledgeExtractor:
    """LINE èŠå¤©è¨˜éŒ„çŸ¥è­˜æå–å™¨"""

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = "gpt-4o-mini"

        # çµ±è¨ˆè³‡è¨Š
        self.stats = {
            "total_messages": 0,
            "total_qa_pairs": 0,
            "total_test_scenarios": 0,
            "files_processed": 0,
            "batches_processed": 0,
            "api_calls": 0
        }

    def log(self, message):
        """ç«‹å³è¼¸å‡ºæ—¥èªŒ"""
        print(message, flush=True)
        sys.stdout.flush()

    def parse_line_chat(self, file_path: str) -> List[Dict]:
        """è§£æ LINE èŠå¤©è¨˜éŒ„"""
        self.log(f"\nğŸ“– è®€å–æ–‡ä»¶: {os.path.basename(file_path)}")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        messages = []
        lines = content.split('\n')
        current_date = None

        for line in lines:
            if not line.strip():
                continue

            # è·³éæ¨™é¡Œå’Œå„²å­˜æ—¥æœŸ
            if line.startswith('[LINE]') or line.startswith('å„²å­˜æ—¥æœŸ'):
                continue

            # æª¢æŸ¥æ˜¯å¦ç‚ºæ—¥æœŸè¡Œ
            date_match = re.match(r'(\d{4}/\d{2}/\d{2})', line)
            if date_match:
                current_date = date_match.group(1)
                continue

            # æª¢æŸ¥æ˜¯å¦ç‚ºè¨Šæ¯è¡Œï¼ˆä½¿ç”¨ tab åˆ†éš”ï¼‰
            parts = line.split('\t')
            if len(parts) >= 3:
                time_str = parts[0].strip()
                user = parts[1].strip()
                message = '\t'.join(parts[2:]).strip()

                # éæ¿¾ç³»çµ±è¨Šæ¯å’Œç„¡æ„ç¾©è¨Šæ¯
                if not user or 'å·²æ–°å¢' in message or 'å·²é›¢é–‹' in message:
                    continue

                # éæ¿¾å¤šåª’é«”è¨Šæ¯
                if message in ['[ç…§ç‰‡]', '[å½±ç‰‡]', '[è²¼åœ–]', '[æª”æ¡ˆ]', '[èªéŸ³è¨Šæ¯]']:
                    continue

                # ç§»é™¤å¼•è™Ÿ
                message = message.strip('"')

                if message:  # åªä¿ç•™æœ‰å…§å®¹çš„è¨Šæ¯
                    timestamp = f"{current_date} {time_str}" if current_date else time_str
                    messages.append({
                        'timestamp': timestamp,
                        'user': user,
                        'message': message
                    })

        self.stats['total_messages'] += len(messages)
        self.log(f"   âœ… è§£æ {len(messages)} æ¢æœ‰æ•ˆè¨Šæ¯")
        return messages

    def extract_qa_pairs_and_tests(
        self,
        messages: List[Dict],
        source_file: str,
        batch_size: int = 30  # æ¸›å°æ‰¹æ¬¡å¤§å°ä»¥æé«˜é€Ÿåº¦
    ) -> Tuple[List[Dict], List[Dict]]:
        """ä½¿ç”¨ LLM æå–å•ç­”å°å’Œæ¸¬è©¦æƒ…å¢ƒ"""
        self.log(f"\nğŸ¤– é–‹å§‹ LLM åˆ†æ...")

        qa_pairs = []
        test_scenarios = []

        total_batches = (len(messages) - 1) // batch_size + 1

        # å°‡è¨Šæ¯åˆ†æ‰¹è™•ç†
        for i in range(0, len(messages), batch_size):
            batch_num = i // batch_size + 1
            batch = messages[i:i+batch_size]
            batch_text = self._format_messages_for_llm(batch)

            self.log(f"   æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} æ¢è¨Šæ¯)...")

            # å‘¼å« LLM æå–
            try:
                extracted = self._call_llm_extract(batch_text, source_file)
                qa_pairs.extend(extracted['qa_pairs'])
                test_scenarios.extend(extracted['test_scenarios'])

                self.stats['batches_processed'] += 1
                self.stats['api_calls'] += 1

                self.log(f"      âœ… æå– {len(extracted['qa_pairs'])} å€‹å•ç­”å°, {len(extracted['test_scenarios'])} å€‹æ¸¬è©¦")
            except Exception as e:
                self.log(f"      âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")

            # é¿å… API rate limit
            time.sleep(0.5)

        self.stats['total_qa_pairs'] += len(qa_pairs)
        self.stats['total_test_scenarios'] += len(test_scenarios)

        self.log(f"\n   âœ… ç´¯è¨ˆæå– {len(qa_pairs)} å€‹å•ç­”å°")
        self.log(f"   âœ… ç´¯ç© {len(test_scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")

        return qa_pairs, test_scenarios

    def _format_messages_for_llm(self, messages: List[Dict]) -> str:
        """æ ¼å¼åŒ–è¨Šæ¯ä¾› LLM åˆ†æ"""
        formatted = []
        for msg in messages:
            formatted.append(f"[{msg['timestamp']}] {msg['user']}: {msg['message']}")
        return '\n'.join(formatted)

    def _call_llm_extract(self, conversation: str, source_file: str) -> Dict:
        """å‘¼å« LLM æå–çŸ¥è­˜å’Œæ¸¬è©¦æƒ…å¢ƒ"""

        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å®¢æœçŸ¥è­˜åº«åˆ†æå¸«ã€‚åˆ†æ LINE å°è©±è¨˜éŒ„ï¼Œæå–ï¼š

1. **å•ç­”å°ï¼ˆQA Pairsï¼‰**ï¼šæå–å®¢æœå›ç­”çš„å•é¡Œå’Œç­”æ¡ˆ
   - å•é¡Œè¦é€šç”¨åŒ–ï¼ˆç§»é™¤å…·é«”ç§Ÿå®¢å§“åã€æˆ¿è™Ÿç­‰ï¼‰
   - ç­”æ¡ˆè¦å®Œæ•´ä¸”å¯¦ç”¨
   - è­˜åˆ¥å•é¡Œé¡å‹ï¼ˆå¸³å‹™/åˆç´„/æœå‹™/è¨­æ–½ï¼‰
   - åˆ¤æ–·å°è±¡ï¼ˆæˆ¿æ±/ç§Ÿå®¢/ç®¡ç†å¸«ï¼‰

2. **æ¸¬è©¦æƒ…å¢ƒï¼ˆTest Scenariosï¼‰**ï¼šæå–å¯ä»¥ä½œç‚ºæ¸¬è©¦çš„çœŸå¯¦å•é¡Œ
   - ä¿ç•™å•é¡Œçš„åŸå§‹è¡¨é”æ–¹å¼
   - è¨˜éŒ„é æœŸç­”æ¡ˆçš„é—œéµè¦é»

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
{
  "qa_pairs": [
    {
      "title": "å•é¡Œæ¨™é¡Œ",
      "category": "å¸³å‹™å•é¡Œ|åˆç´„å•é¡Œ|æœå‹™å•é¡Œ|è¨­æ–½å•é¡Œ",
      "question_summary": "å•é¡Œæ‘˜è¦",
      "answer": "å®Œæ•´ç­”æ¡ˆ",
      "audience": "æˆ¿æ±|ç§Ÿå®¢|ç®¡ç†å¸«",
      "keywords": ["é—œéµå­—1", "é—œéµå­—2"]
    }
  ],
  "test_scenarios": [
    {
      "test_question": "æ¸¬è©¦å•é¡Œï¼ˆåŸå§‹è¡¨é”ï¼‰",
      "expected_answer_points": ["ç­”æ¡ˆè¦é»1", "ç­”æ¡ˆè¦é»2"],
      "difficulty": "easy|medium|hard",
      "notes": "å‚™è¨»ï¼ˆç°¡è¦èªªæ˜å•é¡Œé¡å‹å’Œé‡é»ï¼‰"
    }
  ]
}

æ³¨æ„ï¼š
- åªæå–æ¸…æ™°ã€å®Œæ•´çš„å®¢æœå•ç­”
- é¿å…åŒ…å«ç§äººè³‡è¨Šï¼ˆå§“åã€é›»è©±ã€åœ°å€ç­‰ï¼‰
- æ¸¬è©¦å•é¡Œè¦å…·æœ‰ä»£è¡¨æ€§
- å¦‚æœæ²’æœ‰æ‰¾åˆ°ç›¸é—œå…§å®¹ï¼Œè¿”å›ç©ºé™£åˆ—å³å¯
"""

        user_prompt = f"""åˆ†æä»¥ä¸‹ LINE å°è©±è¨˜éŒ„ï¼Œæå–å•ç­”å°å’Œæ¸¬è©¦æƒ…å¢ƒï¼š

ä¾†æºæ–‡ä»¶ï¼š{source_file}

å°è©±å…§å®¹ï¼š
{conversation[:6000]}  # å¢åŠ é•·åº¦é™åˆ¶

è«‹æå–ï¼š
1. æ¸…æ™°çš„å®¢æœå•ç­”å°
2. å¯ä»¥ä½œç‚ºæ¸¬è©¦çš„å…¸å‹å•é¡Œ
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                temperature=0.3,
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )

            result = json.loads(response.choices[0].message.content)

            # æ·»åŠ ä¾†æºè³‡è¨Š
            for qa in result.get('qa_pairs', []):
                qa['source_file'] = source_file
                qa['source_date'] = datetime.now().strftime('%Y-%m-%d')

            for test in result.get('test_scenarios', []):
                test['source_file'] = source_file

            return result

        except Exception as e:
            self.log(f"   âš ï¸  LLM æå–å¤±æ•—: {e}")
            return {'qa_pairs': [], 'test_scenarios': []}

    def process_all_files(self, file_paths: List[str]) -> Tuple[List[Dict], List[Dict]]:
        """è™•ç†æ‰€æœ‰æ–‡ä»¶"""
        all_qa_pairs = []
        all_test_scenarios = []

        for idx, file_path in enumerate(file_paths, 1):
            self.log(f"\n{'='*70}")
            self.log(f"è™•ç†æ–‡ä»¶ {idx}/{len(file_paths)}: {os.path.basename(file_path)}")
            self.log(f"{'='*70}")

            # è§£æå°è©±
            messages = self.parse_line_chat(file_path)

            if not messages:
                self.log("   âš ï¸  æ²’æœ‰æœ‰æ•ˆè¨Šæ¯ï¼Œè·³éæ­¤æ–‡ä»¶")
                continue

            # æå–çŸ¥è­˜å’Œæ¸¬è©¦
            qa_pairs, test_scenarios = self.extract_qa_pairs_and_tests(
                messages,
                os.path.basename(file_path)
            )

            all_qa_pairs.extend(qa_pairs)
            all_test_scenarios.extend(test_scenarios)

            self.stats['files_processed'] += 1

        return all_qa_pairs, all_test_scenarios

    def save_to_excel(
        self,
        qa_pairs: List[Dict],
        test_scenarios: List[Dict],
        output_dir: str
    ):
        """å„²å­˜ç‚º Excel æ–‡ä»¶"""
        self.log(f"\nğŸ’¾ å„²å­˜çµæœ...")

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)

        # å„²å­˜çŸ¥è­˜åº«
        if qa_pairs:
            kb_df = pd.DataFrame(qa_pairs)
            kb_output = os.path.join(output_dir, 'knowledge_base_extracted.xlsx')
            kb_df.to_excel(kb_output, index=False, engine='openpyxl')
            self.log(f"   âœ… çŸ¥è­˜åº«å·²å„²å­˜: {kb_output}")
            self.log(f"      å…± {len(qa_pairs)} æ¢çŸ¥è­˜")

        # å„²å­˜æ¸¬è©¦æƒ…å¢ƒ
        if test_scenarios:
            test_df = pd.DataFrame(test_scenarios)
            test_output = os.path.join(output_dir, 'test_scenarios.xlsx')
            test_df.to_excel(test_output, index=False, engine='openpyxl')
            self.log(f"   âœ… æ¸¬è©¦æƒ…å¢ƒå·²å„²å­˜: {test_output}")
            self.log(f"      å…± {len(test_scenarios)} å€‹æ¸¬è©¦")

    def print_summary(self):
        """åˆ—å°è™•ç†æ‘˜è¦"""
        self.log(f"\n{'='*70}")
        self.log("ğŸ“Š è™•ç†æ‘˜è¦")
        self.log(f"{'='*70}")
        self.log(f"è™•ç†æ–‡ä»¶æ•¸ï¼š{self.stats['files_processed']}")
        self.log(f"ç¸½è¨Šæ¯æ•¸ï¼š{self.stats['total_messages']}")
        self.log(f"è™•ç†æ‰¹æ¬¡æ•¸ï¼š{self.stats['batches_processed']}")
        self.log(f"API å‘¼å«æ¬¡æ•¸ï¼š{self.stats['api_calls']}")
        self.log(f"æå–å•ç­”å°ï¼š{self.stats['total_qa_pairs']}")
        self.log(f"ç´¯ç©æ¸¬è©¦æƒ…å¢ƒï¼š{self.stats['total_test_scenarios']}")
        self.log(f"{'='*70}\n")


def main():
    """ä¸»ç¨‹å¼"""
    print("="*70, flush=True)
    print("LINE èŠå¤©è¨˜éŒ„çŸ¥è­˜æå–èˆ‡æ¸¬è©¦æƒ…å¢ƒç´¯ç©ï¼ˆå„ªåŒ–ç‰ˆï¼‰", flush=True)
    print("="*70, flush=True)

    # æª¢æŸ¥ OpenAI API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸", flush=True)
        return

    print(f"âœ… OpenAI API Key å·²è¨­å®š", flush=True)

    # æ–‡ä»¶è·¯å¾‘
    data_dir = "/Users/lenny/jgb/AIChatbot/data"
    output_dir = "/Users/lenny/jgb/AIChatbot/output"

    files = [
        os.path.join(data_dir, "[LINE] ä¸€æ–¹ç”Ÿæ´»x JGBçš„èŠå¤©.txt"),
        os.path.join(data_dir, "[LINE] å¯Œå–¬ X JGBæ’é™¤ç–‘é›£é›œç—‡å€çš„èŠå¤©.txt"),
        os.path.join(data_dir, "[LINE] èˆˆä¸­è³‡ç”¢ç®¡ç†&æ˜“èšçš„èŠå¤©.txt")
    ]

    # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for f in files:
        if not os.path.exists(f):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {f}", flush=True)
            return

    print(f"âœ… æ‰¾åˆ° {len(files)} å€‹æ–‡ä»¶", flush=True)

    # å»ºç«‹æå–å™¨
    extractor = LineKnowledgeExtractor()

    # è™•ç†æ‰€æœ‰æ–‡ä»¶
    start_time = time.time()
    qa_pairs, test_scenarios = extractor.process_all_files(files)
    elapsed_time = time.time() - start_time

    # å„²å­˜çµæœ
    extractor.save_to_excel(qa_pairs, test_scenarios, output_dir)

    # åˆ—å°æ‘˜è¦
    extractor.print_summary()

    print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“ï¼š{elapsed_time/60:.1f} åˆ†é˜", flush=True)
    print("âœ… è™•ç†å®Œæˆï¼", flush=True)
    print(f"\nä¸‹ä¸€æ­¥ï¼š", flush=True)
    print(f"1. æª¢æŸ¥ {output_dir}/knowledge_base_extracted.xlsx", flush=True)
    print(f"2. æª¢æŸ¥ {output_dir}/test_scenarios.xlsx", flush=True)
    print(f"3. åŸ·è¡ŒçŸ¥è­˜åº«å°å…¥è…³æœ¬", flush=True)
    print(f"4. åŸ·è¡Œå›æ¸¬è…³æœ¬", flush=True)


if __name__ == "__main__":
    main()
