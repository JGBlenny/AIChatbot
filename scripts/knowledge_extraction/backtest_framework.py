"""
çŸ¥è­˜åº«å›æ¸¬æ¡†æ¶
æ¸¬è©¦ RAG ç³»çµ±å°æ¸¬è©¦å•é¡Œçš„å›ç­”æº–ç¢ºåº¦
"""

import os
import sys
import time
from typing import List, Dict
from datetime import datetime
import pandas as pd
import requests
import json

class BacktestFramework:
    """å›æ¸¬æ¡†æ¶"""

    def __init__(self, base_url: str = "http://localhost:8100", vendor_id: int = 1):
        self.base_url = base_url
        self.vendor_id = vendor_id
        self.results = []

    def load_test_scenarios(self, excel_path: str) -> List[Dict]:
        """è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ"""
        print(f"ğŸ“– è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ: {excel_path}")

        df = pd.read_excel(excel_path, engine='openpyxl')
        scenarios = df.to_dict('records')

        print(f"   âœ… è¼‰å…¥ {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
        return scenarios

    def query_rag_system(self, question: str) -> Dict:
        """æŸ¥è©¢ RAG ç³»çµ±"""
        url = f"{self.base_url}/api/v1/message"

        payload = {
            "message": question,
            "vendor_id": self.vendor_id,
            "mode": "tenant",
            "include_sources": True
        }

        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            print(f"   âŒ API è«‹æ±‚å¤±æ•—: {e}")
            return None

    def evaluate_answer(
        self,
        test_scenario: Dict,
        system_response: Dict
    ) -> Dict:
        """è©•ä¼°ç­”æ¡ˆå“è³ª"""

        if not system_response:
            return {
                "passed": False,
                "score": 0.0,
                "checks": {},
                "reason": "ç³»çµ±ç„¡å›æ‡‰",
                "optimization_tips": "ç³»çµ±ç„¡æ³•å›æ‡‰ï¼Œè«‹æª¢æŸ¥ RAG API æ˜¯å¦æ­£å¸¸é‹ä½œ"
            }

        evaluation = {
            "passed": True,
            "score": 0.0,
            "checks": {},
            "optimization_tips": []
        }

        # 1. æª¢æŸ¥åˆ†é¡æ˜¯å¦æ­£ç¢ºï¼ˆæ”¯æ´å¤š Intentï¼‰
        expected_category = test_scenario.get('expected_category', '')
        actual_intent = system_response.get('intent_name', '')
        all_intents = system_response.get('all_intents')

        # ç¢ºä¿ all_intents æ˜¯åˆ—è¡¨
        if all_intents is None or not all_intents:
            all_intents = [actual_intent] if actual_intent else []

        if expected_category:
            # æª¢æŸ¥é æœŸåˆ†é¡æ˜¯å¦åœ¨ä¸»è¦æ„åœ–æˆ–æ‰€æœ‰ç›¸é—œæ„åœ–ä¸­
            # æ”¯æ´éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ã€Œå¸³å‹™å•é¡Œã€å¯ä»¥åŒ¹é…ã€Œå¸³å‹™æŸ¥è©¢ã€ï¼‰
            def fuzzy_match(expected: str, actual: str) -> bool:
                """æ¨¡ç³ŠåŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦æœ‰å…±åŒçš„é—œéµå­—"""
                # ç›´æ¥åŒ…å«é—œä¿‚
                if expected in actual or actual in expected:
                    return True
                # æå–å‰å…©å€‹å­—åšæ¨¡ç³ŠåŒ¹é…ï¼ˆä¾‹å¦‚ã€Œå¸³å‹™ã€ï¼‰
                if len(expected) >= 2 and len(actual) >= 2:
                    if expected[:2] in actual or actual[:2] in expected:
                        return True
                return False

            category_match = (
                fuzzy_match(expected_category, actual_intent) or
                any(fuzzy_match(expected_category, intent) for intent in all_intents)
            )

            evaluation['checks']['category_match'] = category_match
            evaluation['checks']['matched_intents'] = all_intents if category_match else []

            if category_match:
                evaluation['score'] += 0.3
                # å¦‚æœåŒ¹é…çš„æ˜¯æ¬¡è¦æ„åœ–ï¼Œçµ¦äºˆæç¤º
                if expected_category not in actual_intent and actual_intent not in expected_category:
                    evaluation['optimization_tips'].append(
                        f"âœ… å¤šæ„åœ–åŒ¹é…: é æœŸã€Œ{expected_category}ã€åœ¨æ¬¡è¦æ„åœ–ä¸­æ‰¾åˆ°\n"
                        f"   ä¸»è¦æ„åœ–: {actual_intent}ï¼Œæ‰€æœ‰æ„åœ–: {all_intents}"
                    )
            else:
                # åˆ†é¡ä¸åŒ¹é… - æä¾›å„ªåŒ–å»ºè­°
                evaluation['optimization_tips'].append(
                    f"æ„åœ–åˆ†é¡ä¸åŒ¹é…: é æœŸã€Œ{expected_category}ã€ä½†è­˜åˆ¥ç‚ºã€Œ{actual_intent}ã€\n"
                    f"   æ‰€æœ‰æ„åœ–: {all_intents}\n"
                    f"ğŸ’¡ å»ºè­°: åœ¨æ„åœ–ç®¡ç†ä¸­ç·¨è¼¯ã€Œ{actual_intent}ã€æ„åœ–ï¼Œæ·»åŠ æ›´å¤šç›¸é—œé—œéµå­—"
                )

        # 2. æª¢æŸ¥æ˜¯å¦åŒ…å«é æœŸé—œéµå­—
        expected_keywords = test_scenario.get('expected_keywords', [])
        if isinstance(expected_keywords, str):
            expected_keywords = [k.strip() for k in expected_keywords.split(',') if k.strip()]

        answer = system_response.get('answer', '')
        keyword_matches = sum(1 for kw in expected_keywords if kw in answer)
        keyword_ratio = keyword_matches / len(expected_keywords) if expected_keywords else 0

        evaluation['checks']['keyword_coverage'] = keyword_ratio
        evaluation['score'] += keyword_ratio * 0.4

        if keyword_ratio < 0.5 and expected_keywords:
            missing_keywords = [kw for kw in expected_keywords if kw not in answer]
            evaluation['optimization_tips'].append(
                f"ç­”æ¡ˆç¼ºå°‘é—œéµå­—: {', '.join(missing_keywords)}\n"
                f"ğŸ’¡ å»ºè­°: åœ¨çŸ¥è­˜åº«ä¸­è£œå……ç›¸é—œå…§å®¹ï¼Œæˆ–å„ªåŒ–çŸ¥è­˜çš„é—œéµå­—"
            )

        # 3. æª¢æŸ¥ä¿¡å¿ƒåº¦
        confidence = system_response.get('confidence', 0)
        evaluation['checks']['confidence'] = confidence
        if confidence >= 0.7:
            evaluation['score'] += 0.3
        elif confidence < 0.5:
            evaluation['optimization_tips'].append(
                f"ä¿¡å¿ƒåº¦éä½ ({confidence:.2f})\n"
                f"ğŸ’¡ å»ºè­°: ç³»çµ±å°ç­”æ¡ˆä¸ç¢ºå®šï¼Œå¯èƒ½éœ€è¦æ–°å¢æ›´ç›¸é—œçš„çŸ¥è­˜"
            )

        # 4. åˆ¤å®šæ˜¯å¦é€šé
        evaluation['passed'] = evaluation['score'] >= 0.6

        # 5. ç”Ÿæˆå„ªåŒ–å»ºè­°æ‘˜è¦
        if not evaluation['passed']:
            if not evaluation['optimization_tips']:
                evaluation['optimization_tips'].append(
                    f"æ•´é«”å¾—åˆ†éä½ ({evaluation['score']:.2f}/1.0)\n"
                    f"ğŸ’¡ å»ºè­°: æª¢æŸ¥çŸ¥è­˜åº«æ˜¯å¦æœ‰ç›¸é—œå…§å®¹ï¼Œæˆ–å„ªåŒ–ç¾æœ‰çŸ¥è­˜çš„æè¿°"
                )
        else:
            if evaluation['optimization_tips']:
                # å³ä½¿é€šéï¼Œå¦‚æœæœ‰å„ªåŒ–å»ºè­°ä¹Ÿä¿ç•™
                evaluation['optimization_tips'].insert(0, "âœ… æ¸¬è©¦é€šéï¼Œä½†ä»æœ‰å„ªåŒ–ç©ºé–“:")

        return evaluation

    def run_backtest(
        self,
        test_scenarios: List[Dict],
        sample_size: int = None,
        delay: float = 1.0
    ) -> List[Dict]:
        """åŸ·è¡Œå›æ¸¬"""

        print(f"\nğŸ§ª é–‹å§‹å›æ¸¬...")
        print(f"   æ¸¬è©¦æƒ…å¢ƒæ•¸ï¼š{len(test_scenarios)}")
        if sample_size:
            print(f"   æŠ½æ¨£æ¸¬è©¦ï¼š{sample_size} å€‹")
            test_scenarios = test_scenarios[:sample_size]

        results = []

        for i, scenario in enumerate(test_scenarios, 1):
            question = scenario.get('test_question', '')
            if not question:
                continue

            print(f"\n[{i}/{len(test_scenarios)}] æ¸¬è©¦å•é¡Œ: {question[:50]}...")

            # æŸ¥è©¢ç³»çµ±
            system_response = self.query_rag_system(question)

            # è©•ä¼°ç­”æ¡ˆ
            evaluation = self.evaluate_answer(scenario, system_response)

            # è¨˜éŒ„çµæœ
            result = {
                'test_id': i,
                'test_question': question,
                'expected_category': scenario.get('expected_category', ''),
                'actual_intent': system_response.get('intent_name', '') if system_response else '',
                'system_answer': system_response.get('answer', '')[:200] if system_response else '',
                'confidence': system_response.get('confidence', 0) if system_response else 0,
                'score': evaluation['score'],
                'passed': evaluation['passed'],
                'evaluation': json.dumps(evaluation['checks'], ensure_ascii=False),
                'optimization_tips': '\n'.join(evaluation.get('optimization_tips', [])) if isinstance(evaluation.get('optimization_tips'), list) else evaluation.get('optimization_tips', ''),
                'difficulty': scenario.get('difficulty', 'medium'),
                'notes': scenario.get('notes', ''),
                'timestamp': datetime.now().isoformat()
            }

            results.append(result)

            # é¡¯ç¤ºçµæœ
            status = "âœ… PASS" if evaluation['passed'] else "âŒ FAIL"
            print(f"   {status} (åˆ†æ•¸: {evaluation['score']:.2f})")

            # é¡¯ç¤ºå„ªåŒ–å»ºè­°
            if evaluation.get('optimization_tips'):
                tips = evaluation['optimization_tips']
                if isinstance(tips, list):
                    for tip in tips:
                        print(f"   {tip}")
                else:
                    print(f"   {tips}")

            # é¿å… API rate limit
            time.sleep(delay)

        return results

    def generate_report(self, results: List[Dict], output_path: str):
        """ç”Ÿæˆå›æ¸¬å ±å‘Š"""

        print(f"\nğŸ“Š ç”Ÿæˆå›æ¸¬å ±å‘Š...")

        # è¨ˆç®—çµ±è¨ˆ
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r['passed'])
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        avg_score = sum(r['score'] for r in results) / total_tests if total_tests > 0 else 0
        avg_confidence = sum(r['confidence'] for r in results) / total_tests if total_tests > 0 else 0

        # æŒ‰é›£åº¦åˆ†çµ„
        by_difficulty = {}
        for r in results:
            diff = r.get('difficulty', 'medium')
            if diff not in by_difficulty:
                by_difficulty[diff] = {'total': 0, 'passed': 0}
            by_difficulty[diff]['total'] += 1
            if r['passed']:
                by_difficulty[diff]['passed'] += 1

        # å»ºç«‹ DataFrame
        df = pd.DataFrame(results)

        # å„²å­˜è©³ç´°çµæœ
        df.to_excel(output_path, index=False, engine='openpyxl')
        print(f"   âœ… è©³ç´°çµæœå·²å„²å­˜: {output_path}")

        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        summary_path = output_path.replace('.xlsx', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("çŸ¥è­˜åº«å›æ¸¬å ±å‘Š\n")
            f.write("="*60 + "\n\n")

            f.write(f"æ¸¬è©¦æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"RAG ç³»çµ±ï¼š{self.base_url}\n")
            f.write(f"æ¥­è€… IDï¼š{self.vendor_id}\n\n")

            f.write("="*60 + "\n")
            f.write("æ•´é«”çµ±è¨ˆ\n")
            f.write("="*60 + "\n")
            f.write(f"ç¸½æ¸¬è©¦æ•¸ï¼š{total_tests}\n")
            f.write(f"é€šéæ•¸ï¼š{passed_tests}\n")
            f.write(f"å¤±æ•—æ•¸ï¼š{total_tests - passed_tests}\n")
            f.write(f"é€šéç‡ï¼š{pass_rate:.2f}%\n")
            f.write(f"å¹³å‡åˆ†æ•¸ï¼š{avg_score:.2f}\n")
            f.write(f"å¹³å‡ä¿¡å¿ƒåº¦ï¼š{avg_confidence:.2f}\n\n")

            f.write("="*60 + "\n")
            f.write("æŒ‰é›£åº¦çµ±è¨ˆ\n")
            f.write("="*60 + "\n")
            for diff, stats in by_difficulty.items():
                rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
                f.write(f"{diff.upper():10s}: {stats['passed']}/{stats['total']} ({rate:.1f}%)\n")

            f.write("\n" + "="*60 + "\n")
            f.write("å¤±æ•—æ¡ˆä¾‹\n")
            f.write("="*60 + "\n")

            failed = [r for r in results if not r['passed']]
            if failed:
                for r in failed[:10]:  # åªé¡¯ç¤ºå‰ 10 å€‹
                    f.write(f"\nå•é¡Œï¼š{r['test_question']}\n")
                    f.write(f"é æœŸåˆ†é¡ï¼š{r['expected_category']}\n")
                    f.write(f"å¯¦éš›æ„åœ–ï¼š{r['actual_intent']}\n")
                    f.write(f"åˆ†æ•¸ï¼š{r['score']:.2f}\n")
                    f.write("-" * 60 + "\n")
            else:
                f.write("\nç„¡å¤±æ•—æ¡ˆä¾‹ ğŸ‰\n")

        print(f"   âœ… æ‘˜è¦å ±å‘Šå·²å„²å­˜: {summary_path}")

        # åˆ—å°æ‘˜è¦åˆ°æ§åˆ¶å°
        print(f"\n{'='*60}")
        print("å›æ¸¬æ‘˜è¦")
        print(f"{'='*60}")
        print(f"é€šéç‡ï¼š{pass_rate:.2f}% ({passed_tests}/{total_tests})")
        print(f"å¹³å‡åˆ†æ•¸ï¼š{avg_score:.2f}")
        print(f"å¹³å‡ä¿¡å¿ƒåº¦ï¼š{avg_confidence:.2f}")
        print(f"{'='*60}\n")

        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'pass_rate': pass_rate,
            'avg_score': avg_score,
            'avg_confidence': avg_confidence
        }


def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("çŸ¥è­˜åº«å›æ¸¬æ¡†æ¶")
    print("="*60)

    # é…ç½®
    base_url = os.getenv("RAG_API_URL", "http://localhost:8100")
    vendor_id = int(os.getenv("VENDOR_ID", "1"))

    # å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
    project_root = os.getenv("PROJECT_ROOT", os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

    # æ”¯æ´é¸æ“‡ä¸åŒçš„æ¸¬è©¦æª”æ¡ˆï¼ˆsmoke tests æˆ– full testsï¼‰
    test_type = os.getenv("BACKTEST_TYPE", "smoke")  # smoke, full, or custom
    if test_type == "smoke":
        test_scenarios_path = os.path.join(project_root, "test_scenarios_smoke.xlsx")
    elif test_type == "full":
        test_scenarios_path = os.path.join(project_root, "test_scenarios_full.xlsx")
    else:
        # custom: ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æŒ‡å®šçš„è·¯å¾‘
        test_scenarios_path = os.getenv("BACKTEST_SCENARIOS_PATH", os.path.join(project_root, "test_scenarios.xlsx"))

    output_path = os.path.join(project_root, "output/backtest/backtest_results.xlsx")

    # æª¢æŸ¥æ–‡ä»¶
    if not os.path.exists(test_scenarios_path):
        print(f"âŒ æ¸¬è©¦æƒ…å¢ƒæ–‡ä»¶ä¸å­˜åœ¨: {test_scenarios_path}")
        print("è«‹å…ˆåŸ·è¡Œ extract_knowledge_and_tests.py æå–æ¸¬è©¦æƒ…å¢ƒ")
        return

    # å»ºç«‹å›æ¸¬æ¡†æ¶
    backtest = BacktestFramework(base_url, vendor_id)

    # è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ
    scenarios = backtest.load_test_scenarios(test_scenarios_path)

    # åŸ·è¡Œå›æ¸¬
    # æ”¯æ´éäº¤äº’æ¨¡å¼ï¼ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨£æœ¬æ•¸é‡ï¼‰
    non_interactive = os.getenv("BACKTEST_NON_INTERACTIVE", "false").lower() == "true"

    if non_interactive:
        # éäº¤äº’æ¨¡å¼ï¼šç›´æ¥åŸ·è¡Œå…¨éƒ¨æ¸¬è©¦
        sample_size_str = os.getenv("BACKTEST_SAMPLE_SIZE", "")
        if sample_size_str:
            sample_size = int(sample_size_str)
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œ {sample_size} å€‹æ¸¬è©¦")
        else:
            sample_size = None
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œå…¨éƒ¨ {len(scenarios)} å€‹æ¸¬è©¦")
    else:
        # äº¤äº’æ¨¡å¼ï¼šè©¢å•ç”¨æˆ¶
        print(f"\næ˜¯å¦è¦åŸ·è¡Œå®Œæ•´å›æ¸¬ï¼Ÿ")
        print(f"ç¸½å…± {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
        sample_size = input("è¼¸å…¥è¦æ¸¬è©¦çš„æ•¸é‡ï¼ˆç›´æ¥æŒ‰ Enter æ¸¬è©¦å…¨éƒ¨ï¼‰: ").strip()

        if sample_size:
            sample_size = int(sample_size)
        else:
            sample_size = None

    results = backtest.run_backtest(scenarios, sample_size=sample_size)

    # ç”Ÿæˆå ±å‘Š
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    backtest.generate_report(results, output_path)

    print("âœ… å›æ¸¬å®Œæˆï¼")


if __name__ == "__main__":
    main()
