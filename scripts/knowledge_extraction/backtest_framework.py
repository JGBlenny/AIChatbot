"""
Áü•Ë≠òÂ∫´ÂõûÊ∏¨Ê°ÜÊû∂
Ê∏¨Ë©¶ RAG Á≥ªÁµ±Â∞çÊ∏¨Ë©¶ÂïèÈ°åÁöÑÂõûÁ≠îÊ∫ñÁ¢∫Â∫¶

ÊîØÊè¥‰∏âÁ®ÆÂìÅË≥™Ë©ï‰º∞Ê®°ÂºèÔºö
- basic: Âø´ÈÄüË©ï‰º∞ÔºàÈóúÈçµÂ≠ó„ÄÅÂàÜÈ°û„ÄÅ‰ø°ÂøÉÂ∫¶Ôºâ
- detailed: LLM Ê∑±Â∫¶ÂìÅË≥™Ë©ï‰º∞
- hybrid: Ê∑∑ÂêàÊ®°ÂºèÔºàÊé®Ëñ¶Ôºâ
"""

import os
import sys
import time
import math
import asyncio
from typing import List, Dict, Optional
from datetime import datetime
import pandas as pd
import requests
import json
from openai import OpenAI
import psycopg2
from psycopg2.extras import RealDictCursor

class BacktestFramework:
    """ÂõûÊ∏¨Ê°ÜÊû∂"""

    def __init__(
        self,
        base_url: str = "http://localhost:8100",
        vendor_id: int = 1,
        quality_mode: str = "basic",  # basic, detailed, hybrid
        use_database: bool = True  # ÊòØÂê¶‰ΩøÁî®Ë≥áÊñôÂ∫´ÔºàÈ†êË®≠ TrueÔºâ
    ):
        self.base_url = base_url
        self.vendor_id = vendor_id
        self.quality_mode = quality_mode
        self.use_database = use_database
        self.results = []
        self.run_started_at = datetime.now()  # Ë®òÈåÑÈñãÂßãÊôÇÈñì

        # Ë≥áÊñôÂ∫´ÈÄ£Á∑öÈÖçÁΩÆ
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'user': os.getenv('DB_USER', 'aichatbot'),
            'password': os.getenv('DB_PASSWORD', 'aichatbot_password'),
            'database': os.getenv('DB_NAME', 'aichatbot_admin')
        }

        # Â¶ÇÊûú‰ΩøÁî® detailed Êàñ hybrid Ê®°ÂºèÔºåÂàùÂßãÂåñ OpenAI ÂÆ¢Êà∂Á´Ø
        if quality_mode in ['detailed', 'hybrid']:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("‚ö†Ô∏è  Ë≠¶ÂëäÔºöÊú™Ë®≠ÂÆö OPENAI_API_KEYÔºåÂ∞áÈôçÁ¥öÁÇ∫ basic Ê®°Âºè")
                self.quality_mode = 'basic'
            else:
                self.openai_client = OpenAI(api_key=api_key)
                print(f"‚úÖ ÂìÅË≥™Ë©ï‰º∞Ê®°Âºè: {quality_mode}")
        else:
            print(f"‚úÖ ÂìÅË≥™Ë©ï‰º∞Ê®°Âºè: basicÔºàÂø´ÈÄüÊ®°ÂºèÔºâ")

        # È°ØÁ§∫Êï∏ÊìöÊ∫ê
        if self.use_database:
            print(f"‚úÖ Ê∏¨Ë©¶È°åÂ∫´‰æÜÊ∫ê: Ë≥áÊñôÂ∫´ ({self.db_config['database']})")
        else:
            print(f"‚úÖ Ê∏¨Ë©¶È°åÂ∫´‰æÜÊ∫ê: Excel Ê™îÊ°à")

    def get_db_connection(self):
        """Âª∫Á´ãË≥áÊñôÂ∫´ÈÄ£Á∑ö"""
        return psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)

    def load_test_scenarios_from_db(
        self,
        difficulty: str = None,
        limit: int = None,
        min_avg_score: float = None,
        prioritize_failed: bool = True
    ) -> List[Dict]:
        """ÂæûË≥áÊñôÂ∫´ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢É

        Args:
            difficulty: Èõ£Â∫¶ÁØ©ÈÅ∏ (easy, medium, hard)
            limit: ÈôêÂà∂Êï∏Èáè
            min_avg_score: ÊúÄ‰ΩéÂπ≥ÂùáÂàÜÊï∏ÁØ©ÈÅ∏ÔºàÂÑ™ÂÖàÈÅ∏Êìá‰ΩéÂàÜÊ∏¨Ë©¶Ôºâ
            prioritize_failed: ÂÑ™ÂÖàÈÅ∏ÊìáÂ§±ÊïóÁéáÈ´òÁöÑÊ∏¨Ë©¶

        Returns:
            Ê∏¨Ë©¶ÊÉÖÂ¢ÉÂàóË°®
        """
        print(f"üìñ ÂæûË≥áÊñôÂ∫´ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢É...")
        if difficulty:
            print(f"   Èõ£Â∫¶: {difficulty}")
        if limit:
            print(f"   ÈôêÂà∂: {limit} ÂÄã")
        if prioritize_failed:
            print(f"   Á≠ñÁï•: ÂÑ™ÂÖàÊ∏¨Ë©¶‰ΩéÂàÜ/Â§±ÊïóÊ°à‰æã")

        conn = self.get_db_connection()
        cur = conn.cursor()

        try:
            # Âª∫Á´ãÊü•Ë©¢
            query = """
                SELECT
                    ts.id,
                    ts.test_question,
                    ts.expected_category,
                    ts.expected_keywords,
                    ts.difficulty,
                    ts.notes,
                    ts.priority,
                    ts.total_runs,
                    ts.pass_count,
                    ts.avg_score,
                    CASE
                        WHEN ts.total_runs > 0
                        THEN 1.0 - (ts.pass_count::float / ts.total_runs)
                        ELSE 0.5
                    END as fail_rate
                FROM test_scenarios ts
                WHERE ts.is_active = TRUE
                  AND ts.status = 'approved'
            """
            params = []

            # ÁØ©ÈÅ∏Èõ£Â∫¶
            if difficulty:
                query += " AND ts.difficulty = %s"
                params.append(difficulty)

            # ÁØ©ÈÅ∏ÊúÄ‰ΩéÂàÜÊï∏
            if min_avg_score is not None:
                query += " AND (ts.avg_score IS NULL OR ts.avg_score <= %s)"
                params.append(min_avg_score)

            # ÊéíÂ∫èÁ≠ñÁï•
            if prioritize_failed:
                # ÂÑ™ÂÖàÈÅ∏ÊìáÔºöÂ§±ÊïóÁéáÈ´ò„ÄÅÂπ≥ÂùáÂàÜ‰Ωé„ÄÅÂÑ™ÂÖàÁ¥öÈ´òÁöÑÊ∏¨Ë©¶
                query += " ORDER BY fail_rate DESC, COALESCE(ts.avg_score, 0) ASC, ts.priority DESC, ts.id"
            else:
                # È†êË®≠ÊéíÂ∫èÔºöÂÑ™ÂÖàÁ¥öÈ´òÁöÑÂÖàÊ∏¨Ë©¶
                query += " ORDER BY ts.priority DESC, ts.id"

            # ÈôêÂà∂Êï∏Èáè
            if limit:
                query += " LIMIT %s"
                params.append(limit)

            cur.execute(query, params)
            rows = cur.fetchall()

            # ËΩâÊèõÁÇ∫Â≠óÂÖ∏ÂàóË°®
            scenarios = []
            for row in rows:
                scenario = dict(row)
                # ËΩâÊèõÈóúÈçµÂ≠óÈô£ÂàóÁÇ∫ÈÄóËôüÂàÜÈöîÂ≠ó‰∏≤ÔºàËàá Excel Ê†ºÂºè‰∏ÄËá¥Ôºâ
                if scenario.get('expected_keywords') and isinstance(scenario['expected_keywords'], list):
                    scenario['expected_keywords'] = ', '.join(scenario['expected_keywords'])
                scenarios.append(scenario)

            print(f"   ‚úÖ ËºâÂÖ• {len(scenarios)} ÂÄãÊ∏¨Ë©¶ÊÉÖÂ¢É")
            return scenarios

        finally:
            cur.close()
            conn.close()

    def load_test_scenarios(
        self,
        excel_path: str = None,
        difficulty: str = None,
        limit: int = None,
        prioritize_failed: bool = True
    ) -> List[Dict]:
        """ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢ÉÔºàÊîØÊè¥Ë≥áÊñôÂ∫´Ëàá Excel ÂÖ©Á®ÆÊ®°ÂºèÔºâ

        Args:
            excel_path: Excel Ê™îÊ°àË∑ØÂæëÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
            difficulty: Èõ£Â∫¶ÁØ©ÈÅ∏
            limit: ÈôêÂà∂Êï∏Èáè
            prioritize_failed: ÂÑ™ÂÖàÈÅ∏ÊìáÂ§±ÊïóÁéáÈ´òÁöÑÊ∏¨Ë©¶ÔºàÂÉÖË≥áÊñôÂ∫´Ê®°ÂºèÔºâ

        Returns:
            Ê∏¨Ë©¶ÊÉÖÂ¢ÉÂàóË°®
        """
        if self.use_database:
            # ‰ΩøÁî®Ë≥áÊñôÂ∫´Ê®°Âºè
            return self.load_test_scenarios_from_db(
                difficulty=difficulty,
                limit=limit,
                prioritize_failed=prioritize_failed
            )
        elif excel_path:
            # ‰ΩøÁî® Excel Ê®°ÂºèÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
            print(f"üìñ ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢É: {excel_path}")
            df = pd.read_excel(excel_path, engine='openpyxl')
            scenarios = df.to_dict('records')
            print(f"   ‚úÖ ËºâÂÖ• {len(scenarios)} ÂÄãÊ∏¨Ë©¶ÊÉÖÂ¢É")
            return scenarios
        else:
            raise ValueError("ÂøÖÈ†àÊèê‰æõ excel_path ÊàñÂïüÁî®Ë≥áÊñôÂ∫´Ê®°Âºè")

    def query_rag_system(self, question: str) -> Dict:
        """Êü•Ë©¢ RAG Á≥ªÁµ±"""
        url = f"{self.base_url}/api/v1/message"

        payload = {
            "message": question,
            "vendor_id": self.vendor_id,
            "mode": "tenant",
            "include_sources": True
        }

        # ‚≠ê ÂõûÊ∏¨Â∞àÁî®ÔºöÊ™¢Êü•ÊòØÂê¶Á¶ÅÁî®Á≠îÊ°àÂêàÊàê
        disable_synthesis = os.getenv("BACKTEST_DISABLE_ANSWER_SYNTHESIS", "false").lower() == "true"
        if disable_synthesis:
            payload["disable_answer_synthesis"] = True
            # Âè™Âú®Á¨¨‰∏ÄÊ¨°Ë´ãÊ±ÇÊôÇÈ°ØÁ§∫ÊèêÁ§∫
            if not hasattr(self, '_synthesis_disabled_logged'):
                print("   ‚öôÔ∏è  ÂõûÊ∏¨Ê®°ÂºèÔºöÁ≠îÊ°àÂêàÊàêÂ∑≤Á¶ÅÁî®ÔºàBACKTEST_DISABLE_ANSWER_SYNTHESIS=trueÔºâ")
                self._synthesis_disabled_logged = True

        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            print(f"   ‚ùå API Ë´ãÊ±ÇÂ§±Êïó: {e}")
            return None

    def evaluate_answer(
        self,
        test_scenario: Dict,
        system_response: Dict
    ) -> Dict:
        """Ë©ï‰º∞Á≠îÊ°àÂìÅË≥™"""

        if not system_response:
            return {
                "passed": False,
                "score": 0.0,
                "checks": {},
                "reason": "Á≥ªÁµ±ÁÑ°ÂõûÊáâ",
                "optimization_tips": "Á≥ªÁµ±ÁÑ°Ê≥ïÂõûÊáâÔºåË´ãÊ™¢Êü• RAG API ÊòØÂê¶Ê≠£Â∏∏ÈÅã‰Ωú"
            }

        evaluation = {
            "passed": True,
            "score": 0.0,
            "checks": {},
            "optimization_tips": []
        }

        # 1. Ê™¢Êü•ÂàÜÈ°ûÊòØÂê¶Ê≠£Á¢∫ÔºàÊîØÊè¥Â§ö IntentÔºâ
        expected_category = test_scenario.get('expected_category', '')
        actual_intent = system_response.get('intent_name', '')
        all_intents = system_response.get('all_intents')

        # Á¢∫‰øù all_intents ÊòØÂàóË°®
        if all_intents is None or not all_intents:
            all_intents = [actual_intent] if actual_intent else []

        if expected_category:
            # Ê™¢Êü•È†êÊúüÂàÜÈ°ûÊòØÂê¶Âú®‰∏ªË¶ÅÊÑèÂúñÊàñÊâÄÊúâÁõ∏ÈóúÊÑèÂúñ‰∏≠
            # ÊîØÊè¥ÈÉ®ÂàÜÂåπÈÖçÔºà‰æãÂ¶Ç„ÄåÂ∏≥ÂãôÂïèÈ°å„ÄçÂèØ‰ª•ÂåπÈÖç„ÄåÂ∏≥ÂãôÊü•Ë©¢„ÄçÔºâ
            def fuzzy_match(expected: str, actual: str) -> bool:
                """Ê®°Á≥äÂåπÈÖçÔºöÊ™¢Êü•ÊòØÂê¶ÊúâÂÖ±ÂêåÁöÑÈóúÈçµÂ≠ó"""
                # Áõ¥Êé•ÂåÖÂê´Èóú‰øÇ
                if expected in actual or actual in expected:
                    return True
                # ÊèêÂèñÂâçÂÖ©ÂÄãÂ≠óÂÅöÊ®°Á≥äÂåπÈÖçÔºà‰æãÂ¶Ç„ÄåÂ∏≥Âãô„ÄçÔºâ
                if len(expected) >= 2 and len(actual) >= 2:
                    if expected[:2] in actual or actual[:2] in expected:
                        return True
                return False

            category_match = (
                fuzzy_match(expected_category, actual_intent) or
                any(fuzzy_match(expected_category, intent) for intent in all_intents)
            )

            evaluation['checks']['category_match'] = category_match
            evaluation['checks']['matched_intents'] = all_intents if category_match else []

            if category_match:
                evaluation['score'] += 0.3
                # Â¶ÇÊûúÂåπÈÖçÁöÑÊòØÊ¨°Ë¶ÅÊÑèÂúñÔºåÁµ¶‰∫àÊèêÁ§∫
                if expected_category not in actual_intent and actual_intent not in expected_category:
                    evaluation['optimization_tips'].append(
                        f"‚úÖ Â§öÊÑèÂúñÂåπÈÖç: È†êÊúü„Äå{expected_category}„ÄçÂú®Ê¨°Ë¶ÅÊÑèÂúñ‰∏≠ÊâæÂà∞\n"
                        f"   ‰∏ªË¶ÅÊÑèÂúñ: {actual_intent}ÔºåÊâÄÊúâÊÑèÂúñ: {all_intents}"
                    )
            else:
                # ÂàÜÈ°û‰∏çÂåπÈÖç - Êèê‰æõÂÑ™ÂåñÂª∫Ë≠∞
                evaluation['optimization_tips'].append(
                    f"ÊÑèÂúñÂàÜÈ°û‰∏çÂåπÈÖç: È†êÊúü„Äå{expected_category}„Äç‰ΩÜË≠òÂà•ÁÇ∫„Äå{actual_intent}„Äç\n"
                    f"   ÊâÄÊúâÊÑèÂúñ: {all_intents}\n"
                    f"üí° Âª∫Ë≠∞: Âú®ÊÑèÂúñÁÆ°ÁêÜ‰∏≠Á∑®ËºØ„Äå{actual_intent}„ÄçÊÑèÂúñÔºåÊ∑ªÂä†Êõ¥Â§öÁõ∏ÈóúÈóúÈçµÂ≠ó"
                )

        # 2. Ê™¢Êü•ÊòØÂê¶ÂåÖÂê´È†êÊúüÈóúÈçµÂ≠ó
        expected_keywords = test_scenario.get('expected_keywords', [])
        if isinstance(expected_keywords, str):
            expected_keywords = [k.strip() for k in expected_keywords.split(',') if k.strip()]
        elif expected_keywords is None:
            expected_keywords = []

        answer = system_response.get('answer', '')
        keyword_matches = sum(1 for kw in expected_keywords if kw in answer)
        keyword_ratio = keyword_matches / len(expected_keywords) if expected_keywords else 0

        evaluation['checks']['keyword_coverage'] = keyword_ratio
        evaluation['score'] += keyword_ratio * 0.4

        if keyword_ratio < 0.5 and expected_keywords:
            missing_keywords = [kw for kw in expected_keywords if kw not in answer]
            evaluation['optimization_tips'].append(
                f"Á≠îÊ°àÁº∫Â∞ëÈóúÈçµÂ≠ó: {', '.join(missing_keywords)}\n"
                f"üí° Âª∫Ë≠∞: Âú®Áü•Ë≠òÂ∫´‰∏≠Ë£úÂÖÖÁõ∏ÈóúÂÖßÂÆπÔºåÊàñÂÑ™ÂåñÁü•Ë≠òÁöÑÈóúÈçµÂ≠ó"
            )

        # 3. Ê™¢Êü•‰ø°ÂøÉÂ∫¶
        confidence = system_response.get('confidence', 0)
        evaluation['checks']['confidence'] = confidence
        if confidence >= 0.7:
            evaluation['score'] += 0.3
        elif confidence < 0.5:
            evaluation['optimization_tips'].append(
                f"‰ø°ÂøÉÂ∫¶ÈÅé‰Ωé ({confidence:.2f})\n"
                f"üí° Âª∫Ë≠∞: Á≥ªÁµ±Â∞çÁ≠îÊ°à‰∏çÁ¢∫ÂÆöÔºåÂèØËÉΩÈúÄË¶ÅÊñ∞Â¢ûÊõ¥Áõ∏ÈóúÁöÑÁü•Ë≠ò"
            )

        # 4. Âà§ÂÆöÊòØÂê¶ÈÄöÈÅé
        evaluation['passed'] = evaluation['score'] >= 0.6

        # 5. ÁîüÊàêÂÑ™ÂåñÂª∫Ë≠∞ÊëòË¶Å
        if not evaluation['passed']:
            if not evaluation['optimization_tips']:
                evaluation['optimization_tips'].append(
                    f"Êï¥È´îÂæóÂàÜÈÅé‰Ωé ({evaluation['score']:.2f}/1.0)\n"
                    f"üí° Âª∫Ë≠∞: Ê™¢Êü•Áü•Ë≠òÂ∫´ÊòØÂê¶ÊúâÁõ∏ÈóúÂÖßÂÆπÔºåÊàñÂÑ™ÂåñÁèæÊúâÁü•Ë≠òÁöÑÊèèËø∞"
                )
        else:
            if evaluation['optimization_tips']:
                # Âç≥‰ΩøÈÄöÈÅéÔºåÂ¶ÇÊûúÊúâÂÑ™ÂåñÂª∫Ë≠∞‰πü‰øùÁïô
                evaluation['optimization_tips'].insert(0, "‚úÖ Ê∏¨Ë©¶ÈÄöÈÅéÔºå‰ΩÜ‰ªçÊúâÂÑ™ÂåñÁ©∫Èñì:")

        return evaluation

    def llm_evaluate_answer(
        self,
        question: str,
        answer: str,
        expected_intent: str
    ) -> Dict:
        """‰ΩøÁî® LLM Ë©ï‰º∞Á≠îÊ°àÂìÅË≥™

        Returns:
            {
                'relevance': 1-5,
                'completeness': 1-5,
                'accuracy': 1-5,
                'intent_match': 1-5,
                'overall': 1-5,
                'reasoning': str
            }
        """
        prompt = f"""Ë´ãË©ï‰º∞‰ª•‰∏ãÂïèÁ≠îÁöÑÂìÅË≥™Ôºà1-5ÂàÜÔºå5ÂàÜÊúÄ‰Ω≥ÔºâÔºö

ÂïèÈ°åÔºö{question}
È†êÊúüÊÑèÂúñÔºö{expected_intent}
Á≠îÊ°àÔºö{answer}

Ë´ãÂæû‰ª•‰∏ãÁ∂≠Â∫¶Ë©ïÂàÜÔºö
1. Áõ∏ÈóúÊÄß (Relevance): Á≠îÊ°àÊòØÂê¶Áõ¥Êé•ÂõûÁ≠îÂïèÈ°åÔºü
2. ÂÆåÊï¥ÊÄß (Completeness): Á≠îÊ°àÊòØÂê¶ÂÆåÊï¥Ê∂µËìãÂïèÈ°åÊâÄÂïèÔºü
3. Ê∫ñÁ¢∫ÊÄß (Accuracy): Á≠îÊ°àÂÖßÂÆπÊòØÂê¶Ê∫ñÁ¢∫ÂèØÈù†Ôºü
4. ÊÑèÂúñÂåπÈÖç (Intent Match): Á≠îÊ°àÊòØÂê¶Á¨¶ÂêàÈ†êÊúüÊÑèÂúñÔºü

Ë´ã‰ª• JSON Ê†ºÂºèÂõûË¶ÜÔºö
{{
    "relevance": <1-5>,
    "completeness": <1-5>,
    "accuracy": <1-5>,
    "intent_match": <1-5>,
    "overall": <1-5>,
    "reasoning": "Á∞°Áü≠Ë™™ÊòéË©ïÂàÜÁêÜÁî±"
}}"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            print(f"‚ö†Ô∏è  LLM Ë©ï‰º∞Â§±Êïó: {e}")
            return {
                'relevance': 0,
                'completeness': 0,
                'accuracy': 0,
                'intent_match': 0,
                'overall': 0,
                'reasoning': f"Ë©ï‰º∞Â§±Êïó: {str(e)}"
            }

    def evaluate_answer_with_quality(
        self,
        test_scenario: Dict,
        system_response: Dict
    ) -> Dict:
        """Êï¥ÂêàÂü∫Á§éË©ï‰º∞Âíå LLM ÂìÅË≥™Ë©ï‰º∞

        Returns:
            {
                'basic_eval': Dict,  # Âü∫Á§éË©ï‰º∞ÁµêÊûú
                'quality_eval': Dict,  # LLM ÂìÅË≥™Ë©ï‰º∞ÔºàÂ¶ÇÊûúÂïüÁî®Ôºâ
                'overall_score': float,  # Ê∑∑ÂêàË©ïÂàÜ
                'passed': bool
            }
        """
        # 1. Âü∑Ë°åÂü∫Á§éË©ï‰º∞Ôºà‰øùÊåÅÂêëÂæåÁõ∏ÂÆπÔºâ
        basic_eval = self.evaluate_answer(test_scenario, system_response)

        # 2. Â¶ÇÊûúÊòØ basic Ê®°ÂºèÔºåÁõ¥Êé•ËøîÂõû
        if self.quality_mode == 'basic':
            return {
                'basic_eval': basic_eval,
                'overall_score': basic_eval['score'],
                'passed': basic_eval['passed']
            }

        # 3. Âü∑Ë°å LLM Ë©ï‰º∞
        question = test_scenario.get('test_question', '')
        answer = system_response.get('answer', '') if system_response else ''
        expected_intent = test_scenario.get('expected_category', '')

        if not answer:
            # Ê≤íÊúâÁ≠îÊ°àÔºåÂè™‰ΩøÁî®Âü∫Á§éË©ï‰º∞
            return {
                'basic_eval': basic_eval,
                'overall_score': basic_eval['score'],
                'passed': basic_eval['passed']
            }

        quality_eval = self.llm_evaluate_answer(question, answer, expected_intent)

        # 4. Ë®àÁÆóÊ∑∑ÂêàË©ïÂàÜ
        overall_score = self._calculate_hybrid_score(basic_eval, quality_eval)

        # 5. Âà§ÂÆöÊòØÂê¶ÈÄöÈÅé
        passed = self._determine_pass_status(basic_eval, quality_eval, overall_score)

        return {
            'basic_eval': basic_eval,
            'quality_eval': quality_eval,
            'overall_score': overall_score,
            'passed': passed
        }

    def _calculate_hybrid_score(self, basic_eval: Dict, quality_eval: Dict) -> float:
        """Ë®àÁÆóÊ∑∑ÂêàË©ïÂàÜ

        Ê¨äÈáçÂàÜÈÖçÔºö
        - basic Ê®°ÂºèÔºö100% Âü∫Á§éË©ïÂàÜ
        - hybrid Ê®°ÂºèÔºö40% Âü∫Á§é + 60% LLM
        - detailed Ê®°ÂºèÔºö100% LLM
        """
        basic_score = basic_eval['score']
        quality_score = quality_eval['overall'] / 5.0  # Ê®ôÊ∫ñÂåñÂà∞ 0-1

        if self.quality_mode == 'hybrid':
            return 0.4 * basic_score + 0.6 * quality_score
        elif self.quality_mode == 'detailed':
            return quality_score
        else:
            return basic_score

    def _determine_pass_status(
        self,
        basic_eval: Dict,
        quality_eval: Dict,
        hybrid_score: float
    ) -> bool:
        """Âà§ÂÆöÊòØÂê¶ÈÄöÈÅé"""

        if self.quality_mode == 'hybrid':
            # Ê∑∑ÂêàÊ®°ÂºèÔºöÁ∂úÂêàÂàÜÊï∏ >= 0.5 ‰∏îÂÆåÊï¥ÊÄß >= 2.5
            return (
                hybrid_score >= 0.5 and
                quality_eval.get('completeness', 0) >= 2.5
            )
        elif self.quality_mode == 'detailed':
            # Ë©≥Á¥∞Ê®°ÂºèÔºöÁ∂úÂêà >= 3 ‰∏îÂÆåÊï¥ÊÄß >= 3
            return (
                quality_eval.get('overall', 0) >= 3 and
                quality_eval.get('completeness', 0) >= 3
            )
        else:
            # Âü∫Á§éÊ®°ÂºèÔºöÁèæÊúâÈÇèËºØ
            return basic_eval['passed']

    def calculate_ndcg(self, results: List[Dict], k: int = 3) -> Dict:
        """Ë®àÁÆóÊâÄÊúâÊ∏¨Ë©¶ÁöÑÂπ≥Âùá NDCG@K

        NDCG (Normalized Discounted Cumulative Gain) Ë°°ÈáèÊéíÂ∫èÂìÅË≥™

        Returns:
            {
                'avg_ndcg': float,  # Âπ≥Âùá NDCG
                'count': int  # Ë®àÁÆóÊï∏Èáè
            }
        """
        def calculate_single_ndcg(relevance_scores: List[float]) -> float:
            """Ë®àÁÆóÂñÆÂÄãÊ∏¨Ë©¶ÁöÑ NDCG@K"""
            if not relevance_scores or len(relevance_scores) == 0:
                return 0.0

            # DCG (Discounted Cumulative Gain)
            dcg = 0.0
            for i, score in enumerate(relevance_scores[:k], 1):
                dcg += (2 ** score - 1) / math.log2(i + 1)

            # IDCG (Ideal DCG) - ÊúÄ‰Ω≥ÊéíÂ∫è
            ideal_scores = sorted(relevance_scores, reverse=True)[:k]
            idcg = 0.0
            for i, score in enumerate(ideal_scores, 1):
                idcg += (2 ** score - 1) / math.log2(i + 1)

            return dcg / idcg if idcg > 0 else 0.0

        # Ë®àÁÆóÊØèÂÄãÊ∏¨Ë©¶ÁöÑ NDCG
        ndcg_scores = []
        for result in results:
            if 'quality_eval' in result and result['quality_eval']:
                # ‰ΩøÁî® LLM Ë©ï‰º∞ÁöÑÁõ∏ÈóúÊÄßÂàÜÊï∏
                relevance = result['quality_eval'].get('relevance', 0)
                if relevance > 0:
                    # ÈÄôË£°Á∞°ÂåñËôïÁêÜÔºåÂÅáË®≠ÊØèÂÄãÊ∏¨Ë©¶Âè™Êúâ‰∏ÄÂÄãÁ≠îÊ°à
                    # ÂØ¶ÈöõÊáâÁî®‰∏≠ÂèØ‰ª•Âü∫ÊñºÂ§öÂÄãÁü•Ë≠ò‰æÜÊ∫êË®àÁÆó NDCG
                    ndcg_scores.append(relevance / 5.0)  # Ê®ôÊ∫ñÂåñÂà∞ 0-1

        if ndcg_scores:
            avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)
            return {
                'avg_ndcg': avg_ndcg,
                'count': len(ndcg_scores)
            }

        return {'avg_ndcg': 0.0, 'count': 0}

    def run_backtest(
        self,
        test_scenarios: List[Dict],
        sample_size: int = None,
        delay: float = 1.0
    ) -> List[Dict]:
        """Âü∑Ë°åÂõûÊ∏¨"""

        print(f"\nüß™ ÈñãÂßãÂõûÊ∏¨...")
        print(f"   Ê∏¨Ë©¶ÊÉÖÂ¢ÉÊï∏Ôºö{len(test_scenarios)}")
        if sample_size:
            print(f"   ÊäΩÊ®£Ê∏¨Ë©¶Ôºö{sample_size} ÂÄã")
            test_scenarios = test_scenarios[:sample_size]

        results = []

        for i, scenario in enumerate(test_scenarios, 1):
            question = scenario.get('test_question', '')
            if not question:
                continue

            print(f"\n[{i}/{len(test_scenarios)}] Ê∏¨Ë©¶ÂïèÈ°å: {question[:50]}...")

            # Êü•Ë©¢Á≥ªÁµ±
            system_response = self.query_rag_system(question)

            # Ë©ï‰º∞Á≠îÊ°àÔºà‰ΩøÁî®Â¢ûÂº∑Ë©ï‰º∞Ôºâ
            evaluation_result = self.evaluate_answer_with_quality(scenario, system_response)

            # ÊèêÂèñË©ï‰º∞Ë≥áË®äÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
            if 'basic_eval' in evaluation_result:
                evaluation = evaluation_result['basic_eval']
                quality_eval = evaluation_result.get('quality_eval')
                overall_score = evaluation_result.get('overall_score', evaluation['score'])
                passed = evaluation_result.get('passed', evaluation['passed'])
            else:
                # ÂêëÂæåÁõ∏ÂÆπÔºàÂ¶ÇÊûúÂè™ËøîÂõûÂü∫Á§éË©ï‰º∞Ôºâ
                evaluation = evaluation_result
                quality_eval = None
                overall_score = evaluation['score']
                passed = evaluation['passed']

            # ÊèêÂèñÁü•Ë≠ò‰æÜÊ∫êË≥áË®ä
            sources = system_response.get('sources', []) if system_response else []
            # Á¢∫‰øù sources ÊòØÂàóË°®
            if sources is None:
                sources = []
            source_ids = [s.get('id') for s in sources if s.get('id')]
            source_summary = '; '.join([
                f"[{s.get('id', 'N/A')}] {s.get('question_summary', 'N/A')[:40]}"
                for s in sources[:3]  # Âè™È°ØÁ§∫Ââç 3 ÂÄã
            ]) if sources else 'ÁÑ°‰æÜÊ∫ê'

            # ÁîüÊàêÁü•Ë≠òÂ∫´ÁÆ°ÁêÜÁïåÈù¢ÁöÑÁõ¥Êé•ÈèàÊé•
            knowledge_urls = []
            if source_ids:
                # ÊñπÊ°à1ÔºöÂñÆÂÄãÁü•Ë≠òÁöÑÁõ¥Êé•ÈèàÊé•
                for kb_id in source_ids[:3]:  # Âè™È°ØÁ§∫Ââç3ÂÄã
                    knowledge_urls.append(f"http://localhost:8080/#/knowledge?search={kb_id}")
                # ÊñπÊ°à2ÔºöÊâπÈáèÊü•Ë©¢ÈèàÊé•ÔºàÁî® IDs ‰ΩúÁÇ∫ÊêúÂ∞ãÊ¢ù‰ª∂Ôºâ
                ids_param = ','.join(map(str, source_ids))
                batch_url = f"http://localhost:8080/#/knowledge?ids={ids_param}"
            else:
                batch_url = "http://localhost:8080/#/knowledge"

            knowledge_links = '\n'.join(knowledge_urls) if knowledge_urls else 'ÁÑ°'

            # Ë®òÈåÑÁµêÊûú
            result = {
                'test_id': i,
                'scenario_id': scenario.get('id'),  # Êñ∞Â¢ûÔºöÊ∏¨Ë©¶ÊÉÖÂ¢É IDÔºàÁî®ÊñºË≥áÊñôÂ∫´Ôºâ
                'test_question': question,
                'expected_category': scenario.get('expected_category', ''),
                'actual_intent': system_response.get('intent_name', '') if system_response else '',
                'all_intents': system_response.get('all_intents', []) if system_response else [],
                'system_answer': system_response.get('answer', '')[:200] if system_response else '',
                'confidence': system_response.get('confidence', 0) if system_response else 0,
                'score': evaluation['score'],
                'overall_score': overall_score,  # Êñ∞Â¢ûÔºöÊ∑∑ÂêàË©ïÂàÜ
                'passed': passed,  # ‰ΩøÁî®Ê∑∑ÂêàÂà§ÂÆö
                'category_match': evaluation['checks'].get('category_match', False),
                'keyword_coverage': evaluation['checks'].get('keyword_coverage', 0.0),
                'evaluation': json.dumps(evaluation['checks'], ensure_ascii=False),
                'optimization_tips': '\n'.join(evaluation.get('optimization_tips', [])) if isinstance(evaluation.get('optimization_tips'), list) else evaluation.get('optimization_tips', ''),
                'knowledge_sources': source_summary,
                'source_ids': ','.join(map(str, source_ids)),
                'source_count': len(sources),
                'knowledge_links': knowledge_links,
                'batch_url': batch_url,
                'difficulty': scenario.get('difficulty', 'medium'),
                'notes': scenario.get('notes', ''),
                'timestamp': datetime.now().isoformat()
            }

            # Â¶ÇÊûúÊúâ LLM ÂìÅË≥™Ë©ï‰º∞ÔºåÊ∑ªÂä†Âà∞ÁµêÊûú‰∏≠
            if quality_eval:
                result['quality_eval'] = json.dumps(quality_eval, ensure_ascii=False)
                result['relevance'] = quality_eval.get('relevance', 0)
                result['completeness'] = quality_eval.get('completeness', 0)
                result['accuracy'] = quality_eval.get('accuracy', 0)
                result['intent_match'] = quality_eval.get('intent_match', 0)
                result['quality_overall'] = quality_eval.get('overall', 0)
                result['quality_reasoning'] = quality_eval.get('reasoning', '')

            results.append(result)

            # È°ØÁ§∫ÁµêÊûú
            status = "‚úÖ PASS" if evaluation['passed'] else "‚ùå FAIL"
            print(f"   {status} (ÂàÜÊï∏: {evaluation['score']:.2f})")

            # È°ØÁ§∫Áü•Ë≠ò‰æÜÊ∫ê
            if sources:
                print(f"   üìö Áü•Ë≠ò‰æÜÊ∫ê ({len(sources)} ÂÄã):")
                for idx, src in enumerate(sources[:3], 1):  # Âè™È°ØÁ§∫Ââç3ÂÄã
                    kb_id = src.get('id', 'N/A')
                    title = src.get('question_summary', 'N/A')[:50]
                    print(f"      {idx}. [ID {kb_id}] {title}")

                # È°ØÁ§∫Áü•Ë≠òÂ∫´Áõ¥Êé•ÈèàÊé•
                if knowledge_urls:
                    print(f"   üîó Áõ¥Êé•ÈèàÊé•:")
                    for idx, url in enumerate(knowledge_urls[:3], 1):
                        print(f"      {idx}. {url}")
                    print(f"   üì¶ ÊâπÈáèÊü•Ë©¢: {batch_url}")

            # È°ØÁ§∫ÂÑ™ÂåñÂª∫Ë≠∞
            if evaluation.get('optimization_tips'):
                tips = evaluation['optimization_tips']
                if isinstance(tips, list):
                    for tip in tips:
                        print(f"   {tip}")
                else:
                    print(f"   {tips}")

            # ÈÅøÂÖç API rate limit
            time.sleep(delay)

        return results

    def generate_report(self, results: List[Dict], output_path: str):
        """ÁîüÊàêÂõûÊ∏¨Â†±Âëä"""

        print(f"\nüìä ÁîüÊàêÂõûÊ∏¨Â†±Âëä...")

        # Ë®àÁÆóÁµ±Ë®à
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r['passed'])
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        avg_score = sum(r['score'] for r in results) / total_tests if total_tests > 0 else 0
        avg_confidence = sum(r['confidence'] for r in results) / total_tests if total_tests > 0 else 0

        # Ê™¢Êü•ÊòØÂê¶ÊúâÂìÅË≥™Ë©ï‰º∞Ë≥áÊñô
        has_quality_eval = any('relevance' in r for r in results)

        # Ë®àÁÆóÂìÅË≥™ÊåáÊ®ôÔºàÂ¶ÇÊûúÊúâÁöÑË©±Ôºâ
        quality_stats = {}
        if has_quality_eval:
            quality_results = [r for r in results if 'relevance' in r]
            if quality_results:
                quality_stats = {
                    'avg_relevance': sum(r['relevance'] for r in quality_results) / len(quality_results),
                    'avg_completeness': sum(r['completeness'] for r in quality_results) / len(quality_results),
                    'avg_accuracy': sum(r['accuracy'] for r in quality_results) / len(quality_results),
                    'avg_intent_match': sum(r['intent_match'] for r in quality_results) / len(quality_results),
                    'avg_quality_overall': sum(r['quality_overall'] for r in quality_results) / len(quality_results),
                    'quality_count': len(quality_results)
                }

                # Ë®àÁÆó NDCG
                ndcg_data = self.calculate_ndcg(quality_results)
                quality_stats['ndcg'] = ndcg_data['avg_ndcg']
                quality_stats['ndcg_count'] = ndcg_data['count']

        # ÊåâÈõ£Â∫¶ÂàÜÁµÑ
        by_difficulty = {}
        for r in results:
            diff = r.get('difficulty', 'medium')
            if diff not in by_difficulty:
                by_difficulty[diff] = {'total': 0, 'passed': 0}
            by_difficulty[diff]['total'] += 1
            if r['passed']:
                by_difficulty[diff]['passed'] += 1

        # Âª∫Á´ã DataFrame
        df = pd.DataFrame(results)

        # ÂÑ≤Â≠òË©≥Á¥∞ÁµêÊûú
        df.to_excel(output_path, index=False, engine='openpyxl')
        print(f"   ‚úÖ Ë©≥Á¥∞ÁµêÊûúÂ∑≤ÂÑ≤Â≠ò: {output_path}")

        # ÁîüÊàêÊëòË¶ÅÂ†±Âëä
        summary_path = output_path.replace('.xlsx', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("Áü•Ë≠òÂ∫´ÂõûÊ∏¨Â†±Âëä\n")
            f.write("="*60 + "\n\n")

            f.write(f"Ê∏¨Ë©¶ÊôÇÈñìÔºö{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"RAG Á≥ªÁµ±Ôºö{self.base_url}\n")
            f.write(f"Ê•≠ËÄÖ IDÔºö{self.vendor_id}\n")
            f.write(f"ÂìÅË≥™Ë©ï‰º∞Ê®°ÂºèÔºö{self.quality_mode}\n\n")

            f.write("="*60 + "\n")
            f.write("Êï¥È´îÁµ±Ë®à\n")
            f.write("="*60 + "\n")
            f.write(f"Á∏ΩÊ∏¨Ë©¶Êï∏Ôºö{total_tests}\n")
            f.write(f"ÈÄöÈÅéÊï∏Ôºö{passed_tests}\n")
            f.write(f"Â§±ÊïóÊï∏Ôºö{total_tests - passed_tests}\n")
            f.write(f"ÈÄöÈÅéÁéáÔºö{pass_rate:.2f}%\n")
            f.write(f"Âπ≥ÂùáÂàÜÊï∏ÔºàÂü∫Á§éÔºâÔºö{avg_score:.2f}\n")
            f.write(f"Âπ≥Âùá‰ø°ÂøÉÂ∫¶Ôºö{avg_confidence:.2f}\n\n")

            # Êñ∞Â¢ûÔºöÂìÅË≥™Ë©ï‰º∞Áµ±Ë®à
            if quality_stats:
                f.write("="*60 + "\n")
                f.write("LLM ÂìÅË≥™Ë©ï‰º∞Áµ±Ë®à\n")
                f.write("="*60 + "\n")
                f.write(f"Ë©ï‰º∞Ê∏¨Ë©¶Êï∏Ôºö{quality_stats['quality_count']}\n")
                f.write(f"Âπ≥ÂùáÁõ∏ÈóúÊÄß (Relevance)Ôºö{quality_stats['avg_relevance']:.2f}/5.0\n")
                f.write(f"Âπ≥ÂùáÂÆåÊï¥ÊÄß (Completeness)Ôºö{quality_stats['avg_completeness']:.2f}/5.0\n")
                f.write(f"Âπ≥ÂùáÊ∫ñÁ¢∫ÊÄß (Accuracy)Ôºö{quality_stats['avg_accuracy']:.2f}/5.0\n")
                f.write(f"Âπ≥ÂùáÊÑèÂúñÂåπÈÖç (Intent Match)Ôºö{quality_stats['avg_intent_match']:.2f}/5.0\n")
                f.write(f"Âπ≥ÂùáÁ∂úÂêàË©ïÂàÜ (Overall)Ôºö{quality_stats['avg_quality_overall']:.2f}/5.0\n")
                f.write(f"NDCG@3 (ÊéíÂ∫èÂìÅË≥™)Ôºö{quality_stats['ndcg']:.4f}\n")
                f.write("\nÂìÅË≥™Ë©ïÁ¥ö:\n")

                # Ë©ïÁ¥öÈÇèËºØ
                def get_rating(score):
                    if score >= 4.0:
                        return "üéâ ÂÑ™ÁßÄ"
                    elif score >= 3.5:
                        return "‚úÖ ËâØÂ•Ω"
                    elif score >= 3.0:
                        return "‚ö†Ô∏è  ‰∏≠Á≠â"
                    else:
                        return "‚ùå ÈúÄÊîπÂñÑ"

                f.write(f"  Áõ∏ÈóúÊÄßÔºö{get_rating(quality_stats['avg_relevance'])}\n")
                f.write(f"  ÂÆåÊï¥ÊÄßÔºö{get_rating(quality_stats['avg_completeness'])}\n")
                f.write(f"  Ê∫ñÁ¢∫ÊÄßÔºö{get_rating(quality_stats['avg_accuracy'])}\n")
                f.write(f"  ÊÑèÂúñÂåπÈÖçÔºö{get_rating(quality_stats['avg_intent_match'])}\n")
                f.write(f"  Á∂úÂêàË©ïÂàÜÔºö{get_rating(quality_stats['avg_quality_overall'])}\n")

                # NDCG Ë©ïÁ¥ö
                ndcg_rating = "üéâ ÂÑ™ÁßÄ" if quality_stats['ndcg'] >= 0.9 else \
                              "‚úÖ ËâØÂ•Ω" if quality_stats['ndcg'] >= 0.7 else \
                              "‚ö†Ô∏è  ‰∏≠Á≠â" if quality_stats['ndcg'] >= 0.5 else "‚ùå ÈúÄÊîπÂñÑ"
                f.write(f"  ÊéíÂ∫èÂìÅË≥™Ôºö{ndcg_rating}\n\n")

            f.write("="*60 + "\n")
            f.write("ÊåâÈõ£Â∫¶Áµ±Ë®à\n")
            f.write("="*60 + "\n")
            for diff, stats in by_difficulty.items():
                rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
                f.write(f"{diff.upper():10s}: {stats['passed']}/{stats['total']} ({rate:.1f}%)\n")

            f.write("\n" + "="*60 + "\n")
            f.write("Â§±ÊïóÊ°à‰æã\n")
            f.write("="*60 + "\n")

            failed = [r for r in results if not r['passed']]
            if failed:
                for r in failed[:10]:  # Âè™È°ØÁ§∫Ââç 10 ÂÄã
                    f.write(f"\nÂïèÈ°åÔºö{r['test_question']}\n")
                    f.write(f"È†êÊúüÂàÜÈ°ûÔºö{r['expected_category']}\n")
                    f.write(f"ÂØ¶ÈöõÊÑèÂúñÔºö{r['actual_intent']}\n")
                    f.write(f"ÂàÜÊï∏Ôºö{r['score']:.2f}\n")
                    f.write(f"Áü•Ë≠ò‰æÜÊ∫êÔºö{r.get('knowledge_sources', 'ÁÑ°')}\n")
                    f.write(f"‰æÜÊ∫êIDsÔºö{r.get('source_ids', 'ÁÑ°')}\n")
                    # Êñ∞Â¢ûÔºöÁü•Ë≠òÂ∫´Áõ¥Êé•ÈèàÊé•
                    knowledge_links = r.get('knowledge_links', 'ÁÑ°')
                    if knowledge_links and knowledge_links != 'ÁÑ°':
                        f.write(f"Áü•Ë≠òÂ∫´ÈèàÊé•Ôºö\n{knowledge_links}\n")
                    batch_url = r.get('batch_url', '')
                    if batch_url:
                        f.write(f"ÊâπÈáèÊü•Ë©¢Ôºö{batch_url}\n")
                    f.write(f"ÂÑ™ÂåñÂª∫Ë≠∞Ôºö\n{r.get('optimization_tips', 'ÁÑ°')}\n")
                    f.write("-" * 60 + "\n")
            else:
                f.write("\nÁÑ°Â§±ÊïóÊ°à‰æã üéâ\n")

        print(f"   ‚úÖ ÊëòË¶ÅÂ†±ÂëäÂ∑≤ÂÑ≤Â≠ò: {summary_path}")

        # ÂàóÂç∞ÊëòË¶ÅÂà∞ÊéßÂà∂Âè∞
        print(f"\n{'='*60}")
        print("ÂõûÊ∏¨ÊëòË¶Å")
        print(f"{'='*60}")
        print(f"ÈÄöÈÅéÁéáÔºö{pass_rate:.2f}% ({passed_tests}/{total_tests})")
        print(f"Âπ≥ÂùáÂàÜÊï∏ÔºàÂü∫Á§éÔºâÔºö{avg_score:.2f}")
        print(f"Âπ≥Âùá‰ø°ÂøÉÂ∫¶Ôºö{avg_confidence:.2f}")

        # È°ØÁ§∫ÂìÅË≥™Ë©ï‰º∞ÁµêÊûú
        if quality_stats:
            print(f"\nüéØ LLM ÂìÅË≥™Ë©ï‰º∞Áµ±Ë®à ({quality_stats['quality_count']} ÂÄãÊ∏¨Ë©¶):")
            print(f"   Áõ∏ÈóúÊÄßÔºö{quality_stats['avg_relevance']:.2f}/5.0")
            print(f"   ÂÆåÊï¥ÊÄßÔºö{quality_stats['avg_completeness']:.2f}/5.0")
            print(f"   Ê∫ñÁ¢∫ÊÄßÔºö{quality_stats['avg_accuracy']:.2f}/5.0")
            print(f"   ÊÑèÂúñÂåπÈÖçÔºö{quality_stats['avg_intent_match']:.2f}/5.0")
            print(f"   Á∂úÂêàË©ïÂàÜÔºö{quality_stats['avg_quality_overall']:.2f}/5.0")
            print(f"   NDCG@3Ôºö{quality_stats['ndcg']:.4f}")

        print(f"{'='*60}\n")

        # ÁµÑË£ùËøîÂõûË≥áÊñô
        summary_data = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'pass_rate': pass_rate,
            'avg_score': avg_score,
            'avg_confidence': avg_confidence,
            'quality_mode': self.quality_mode
        }

        # Ê∑ªÂä†ÂìÅË≥™Áµ±Ë®à
        if quality_stats:
            summary_data['quality_stats'] = quality_stats

        return summary_data

    def save_results_to_database(self, results: List[Dict], summary_data: Dict, output_path: str):
        """ÂÑ≤Â≠òÂõûÊ∏¨ÁµêÊûúÂà∞Ë≥áÊñôÂ∫´"""
        print(f"\nüíæ ÂÑ≤Â≠òÂõûÊ∏¨ÁµêÊûúÂà∞Ë≥áÊñôÂ∫´...")

        if not self.use_database:
            print("   ‚ö†Ô∏è  Ë≥áÊñôÂ∫´Ê®°ÂºèÊú™ÂïüÁî®ÔºåË∑≥ÈÅéÂÑ≤Â≠ò")
            return None

        conn = self.get_db_connection()
        cur = conn.cursor()

        try:
            # 1. Âª∫Á´ã backtest_run Ë®òÈåÑ
            completed_at = datetime.now()
            duration_seconds = int((completed_at - self.run_started_at).total_seconds())

            cur.execute("""
                INSERT INTO backtest_runs (
                    quality_mode, test_type, total_scenarios, executed_scenarios,
                    status, rag_api_url, vendor_id,
                    passed_count, failed_count, pass_rate,
                    avg_score, avg_confidence,
                    avg_relevance, avg_completeness, avg_accuracy,
                    avg_intent_match, avg_quality_overall, ndcg_score,
                    started_at, completed_at, duration_seconds,
                    output_file_path, summary_file_path, executed_by
                ) VALUES (
                    %s, %s, %s, %s, 'completed', %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s
                ) RETURNING id
            """, (
                self.quality_mode,
                os.getenv('BACKTEST_TYPE', 'full'),
                summary_data['total_tests'],
                summary_data['total_tests'],
                self.base_url,
                self.vendor_id,
                summary_data['passed_tests'],
                summary_data['total_tests'] - summary_data['passed_tests'],
                summary_data['pass_rate'],
                summary_data['avg_score'],
                summary_data['avg_confidence'],
                summary_data.get('quality_stats', {}).get('avg_relevance'),
                summary_data.get('quality_stats', {}).get('avg_completeness'),
                summary_data.get('quality_stats', {}).get('avg_accuracy'),
                summary_data.get('quality_stats', {}).get('avg_intent_match'),
                summary_data.get('quality_stats', {}).get('avg_quality_overall'),
                summary_data.get('quality_stats', {}).get('ndcg'),
                self.run_started_at,
                completed_at,
                duration_seconds,
                output_path,
                output_path.replace('.xlsx', '_summary.txt'),
                'backtest_framework'
            ))

            run_id = cur.fetchone()['id']
            print(f"   ‚úÖ Âª∫Á´ãÂõûÊ∏¨Âü∑Ë°åË®òÈåÑ (Run ID: {run_id})")

            # 2. ÊèíÂÖ•ÊØèÂÄãÊ∏¨Ë©¶ÁµêÊûú
            inserted_count = 0
            for result in results:
                # Ê∫ñÂÇô all_intents Èô£Âàó
                all_intents = result.get('all_intents', [])
                if isinstance(all_intents, str):
                    all_intents = [all_intents] if all_intents else []

                cur.execute("""
                    INSERT INTO backtest_results (
                        run_id, scenario_id, test_question, expected_category,
                        actual_intent, all_intents, system_answer, confidence,
                        score, overall_score, passed,
                        category_match, keyword_coverage,
                        relevance, completeness, accuracy, intent_match,
                        quality_overall, quality_reasoning,
                        source_ids, source_count, knowledge_sources, optimization_tips,
                        evaluation
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """, (
                    run_id,
                    result.get('scenario_id'),
                    result['test_question'],
                    result.get('expected_category'),
                    result.get('actual_intent'),
                    all_intents,
                    result.get('system_answer'),
                    result.get('confidence', 0),
                    result.get('score', 0),
                    result.get('overall_score', result.get('score', 0)),
                    result.get('passed', False),
                    result.get('category_match', False),
                    result.get('keyword_coverage', 0.0),
                    result.get('relevance'),
                    result.get('completeness'),
                    result.get('accuracy'),
                    result.get('intent_match'),
                    result.get('quality_overall'),
                    result.get('quality_reasoning'),
                    result.get('source_ids'),
                    result.get('source_count', 0),
                    result.get('knowledge_sources'),
                    result.get('optimization_tips'),
                    result.get('evaluation')
                ))
                inserted_count += 1

            conn.commit()
            print(f"   ‚úÖ ÂÑ≤Â≠ò {inserted_count} ÂÄãÊ∏¨Ë©¶ÁµêÊûúÂà∞Ë≥áÊñôÂ∫´")
            print(f"   üìä ÂõûÊ∏¨Âü∑Ë°å ID: {run_id}")
            print(f"   ‚è±Ô∏è  Âü∑Ë°åÊôÇÈñì: {duration_seconds} Áßí")

            return run_id

        except Exception as e:
            conn.rollback()
            print(f"   ‚ùå ÂÑ≤Â≠òÂà∞Ë≥áÊñôÂ∫´Â§±Êïó: {e}")
            import traceback
            traceback.print_exc()
            return None

        finally:
            cur.close()
            conn.close()


def main():
    """‰∏ªÁ®ãÂºè"""
    print("="*60)
    print("Áü•Ë≠òÂ∫´ÂõûÊ∏¨Ê°ÜÊû∂")
    print("="*60)

    # ÈÖçÁΩÆ
    base_url = os.getenv("RAG_API_URL", "http://localhost:8100")
    vendor_id = int(os.getenv("VENDOR_ID", "1"))
    quality_mode = os.getenv("BACKTEST_QUALITY_MODE", "basic")  # basic, detailed, hybrid

    # Ë≥áÊñôÂ∫´Ê®°ÂºèÊéßÂà∂ÔºàÈ†êË®≠ÂïüÁî®Ôºâ
    use_database = os.getenv("BACKTEST_USE_DATABASE", "true").lower() == "true"

    # ÂèñÂæóÂ∞àÊ°àÊ†πÁõÆÈåÑ
    project_root = os.getenv("PROJECT_ROOT", os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

    # ÊîØÊè¥ÈÅ∏Êìá‰∏çÂêåÁöÑÊ∏¨Ë©¶Ê™îÊ°àÔºàsmoke tests Êàñ full testsÔºâ
    test_type = os.getenv("BACKTEST_TYPE", "smoke")  # smoke, full, or custom

    output_path = os.path.join(project_root, "output/backtest/backtest_results.xlsx")

    # Âª∫Á´ãÂõûÊ∏¨Ê°ÜÊû∂ÔºàÂ∏∂ÂìÅË≥™Ë©ï‰º∞Ê®°ÂºèËàáË≥áÊñôÂ∫´ÊîØÊè¥Ôºâ
    backtest = BacktestFramework(base_url, vendor_id, quality_mode, use_database=use_database)

    # ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢É
    if use_database:
        # Ë≥áÊñôÂ∫´Ê®°ÂºèÔºöÂü∫ÊñºË©ïÂàÜÂíåÈõ£Â∫¶ÁØ©ÈÅ∏
        difficulty = os.getenv("BACKTEST_DIFFICULTY")  # easy, medium, hard, or None for all
        prioritize_failed = os.getenv("BACKTEST_PRIORITIZE_FAILED", "true").lower() == "true"

        try:
            scenarios = backtest.load_test_scenarios(
                difficulty=difficulty,
                prioritize_failed=prioritize_failed
            )
        except Exception as e:
            print(f"‚ùå ÂæûË≥áÊñôÂ∫´ËºâÂÖ•Ê∏¨Ë©¶ÊÉÖÂ¢ÉÂ§±Êïó: {e}")
            print("üí° ÊèêÁ§∫ÔºöË´ãÁ¢∫Ë™çË≥áÊñôÂ∫´ÈÄ£Á∑öÊ≠£Â∏∏Ôºå‰∏îÂ∑≤Âü∑Ë°åÊ∏¨Ë©¶È°åÂ∫´ÈÅ∑Áßª")
            print("   ÊàñË®≠ÂÆö BACKTEST_USE_DATABASE=false ‰ΩøÁî® Excel Ê®°Âºè")
            return
    else:
        # Excel Ê®°ÂºèÔºö‰ΩøÁî®Ê™îÊ°àË∑ØÂæëÔºàÂêëÂæåÁõ∏ÂÆπÔºâ
        if test_type == "smoke":
            test_scenarios_path = os.path.join(project_root, "test_scenarios_smoke.xlsx")
        elif test_type == "full":
            test_scenarios_path = os.path.join(project_root, "test_scenarios_full.xlsx")
        else:
            # custom: ‰ΩøÁî®Áí∞Â¢ÉËÆäÊï∏ÊåáÂÆöÁöÑË∑ØÂæë
            test_scenarios_path = os.getenv("BACKTEST_SCENARIOS_PATH", os.path.join(project_root, "test_scenarios.xlsx"))

        # Ê™¢Êü•Êñá‰ª∂
        if not os.path.exists(test_scenarios_path):
            print(f"‚ùå Ê∏¨Ë©¶ÊÉÖÂ¢ÉÊñá‰ª∂‰∏çÂ≠òÂú®: {test_scenarios_path}")
            print("Ë´ãÂÖàÂü∑Ë°å extract_knowledge_and_tests.py ÊèêÂèñÊ∏¨Ë©¶ÊÉÖÂ¢É")
            print("ÊàñË®≠ÂÆö BACKTEST_USE_DATABASE=true ‰ΩøÁî®Ë≥áÊñôÂ∫´Ê®°Âºè")
            return

        scenarios = backtest.load_test_scenarios(excel_path=test_scenarios_path)

    # Âü∑Ë°åÂõûÊ∏¨
    # ÊîØÊè¥Èùû‰∫§‰∫íÊ®°ÂºèÔºàÂæûÁí∞Â¢ÉËÆäÊï∏ËÆÄÂèñÊ®£Êú¨Êï∏ÈáèÔºâ
    non_interactive = os.getenv("BACKTEST_NON_INTERACTIVE", "false").lower() == "true"

    if non_interactive:
        # Èùû‰∫§‰∫íÊ®°ÂºèÔºöÁõ¥Êé•Âü∑Ë°åÂÖ®ÈÉ®Ê∏¨Ë©¶
        sample_size_str = os.getenv("BACKTEST_SAMPLE_SIZE", "")
        if sample_size_str:
            sample_size = int(sample_size_str)
            print(f"\nüß™ Èùû‰∫§‰∫íÊ®°ÂºèÔºöÂü∑Ë°å {sample_size} ÂÄãÊ∏¨Ë©¶")
        else:
            sample_size = None
            print(f"\nüß™ Èùû‰∫§‰∫íÊ®°ÂºèÔºöÂü∑Ë°åÂÖ®ÈÉ® {len(scenarios)} ÂÄãÊ∏¨Ë©¶")
    else:
        # ‰∫§‰∫íÊ®°ÂºèÔºöË©¢ÂïèÁî®Êà∂
        print(f"\nÊòØÂê¶Ë¶ÅÂü∑Ë°åÂÆåÊï¥ÂõûÊ∏¨Ôºü")
        print(f"Á∏ΩÂÖ± {len(scenarios)} ÂÄãÊ∏¨Ë©¶ÊÉÖÂ¢É")
        sample_size = input("Ëº∏ÂÖ•Ë¶ÅÊ∏¨Ë©¶ÁöÑÊï∏ÈáèÔºàÁõ¥Êé•Êåâ Enter Ê∏¨Ë©¶ÂÖ®ÈÉ®Ôºâ: ").strip()

        if sample_size:
            sample_size = int(sample_size)
        else:
            sample_size = None

    results = backtest.run_backtest(scenarios, sample_size=sample_size)

    # ÁîüÊàêÂ†±Âëä
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    summary_data = backtest.generate_report(results, output_path)

    # ÂÑ≤Â≠òÂà∞Ë≥áÊñôÂ∫´
    backtest.save_results_to_database(results, summary_data, output_path)

    print("‚úÖ ÂõûÊ∏¨ÂÆåÊàêÔºÅ")


if __name__ == "__main__":
    main()
