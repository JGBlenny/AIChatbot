"""
çŸ¥è­˜åº«å›æ¸¬æ¡†æ¶
æ¸¬è©¦ RAG ç³»çµ±å°æ¸¬è©¦å•é¡Œçš„å›ç­”æº–ç¢ºåº¦

æ”¯æ´ä¸‰ç¨®å“è³ªè©•ä¼°æ¨¡å¼ï¼š
- basic: å¿«é€Ÿè©•ä¼°ï¼ˆé—œéµå­—ã€åˆ†é¡ã€ä¿¡å¿ƒåº¦ï¼‰
- detailed: LLM æ·±åº¦å“è³ªè©•ä¼°
- hybrid: æ··åˆæ¨¡å¼ï¼ˆæ¨è–¦ï¼‰
"""

import os
import sys
import time
import math
import asyncio
from typing import List, Dict, Optional
from datetime import datetime
import pandas as pd
import requests
import json
from openai import OpenAI
import psycopg2
from psycopg2.extras import RealDictCursor

class BacktestFramework:
    """å›æ¸¬æ¡†æ¶"""

    def __init__(
        self,
        base_url: str = "http://localhost:8100",
        vendor_id: int = 1,
        quality_mode: str = "basic",  # basic, detailed, hybrid
        use_database: bool = True  # æ˜¯å¦ä½¿ç”¨è³‡æ–™åº«ï¼ˆé è¨­ Trueï¼‰
    ):
        self.base_url = base_url
        self.vendor_id = vendor_id
        self.quality_mode = quality_mode
        self.use_database = use_database
        self.results = []
        self.run_started_at = datetime.now()  # è¨˜éŒ„é–‹å§‹æ™‚é–“

        # è³‡æ–™åº«é€£ç·šé…ç½®
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 5432)),
            'user': os.getenv('DB_USER', 'aichatbot'),
            'password': os.getenv('DB_PASSWORD', 'aichatbot_password'),
            'database': os.getenv('DB_NAME', 'aichatbot_admin')
        }

        # å¦‚æœä½¿ç”¨ detailed æˆ– hybrid æ¨¡å¼ï¼Œåˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯
        if quality_mode in ['detailed', 'hybrid']:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("âš ï¸  è­¦å‘Šï¼šæœªè¨­å®š OPENAI_API_KEYï¼Œå°‡é™ç´šç‚º basic æ¨¡å¼")
                self.quality_mode = 'basic'
            else:
                self.openai_client = OpenAI(api_key=api_key)
                print(f"âœ… å“è³ªè©•ä¼°æ¨¡å¼: {quality_mode}")
        else:
            print(f"âœ… å“è³ªè©•ä¼°æ¨¡å¼: basicï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰")

        # é¡¯ç¤ºæ•¸æ“šæº
        if self.use_database:
            print(f"âœ… æ¸¬è©¦é¡Œåº«ä¾†æº: è³‡æ–™åº« ({self.db_config['database']})")
        else:
            print(f"âœ… æ¸¬è©¦é¡Œåº«ä¾†æº: Excel æª”æ¡ˆ")

    def get_db_connection(self):
        """å»ºç«‹è³‡æ–™åº«é€£ç·š"""
        return psycopg2.connect(**self.db_config, cursor_factory=RealDictCursor)

    def load_test_scenarios_from_db(
        self,
        difficulty: str = None,
        limit: int = None,
        min_avg_score: float = None,
        prioritize_failed: bool = True
    ) -> List[Dict]:
        """å¾è³‡æ–™åº«è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ

        Args:
            difficulty: é›£åº¦ç¯©é¸ (easy, medium, hard)
            limit: é™åˆ¶æ•¸é‡
            min_avg_score: æœ€ä½å¹³å‡åˆ†æ•¸ç¯©é¸ï¼ˆå„ªå…ˆé¸æ“‡ä½åˆ†æ¸¬è©¦ï¼‰
            prioritize_failed: å„ªå…ˆé¸æ“‡å¤±æ•—ç‡é«˜çš„æ¸¬è©¦

        Returns:
            æ¸¬è©¦æƒ…å¢ƒåˆ—è¡¨
        """
        print(f"ğŸ“– å¾è³‡æ–™åº«è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ...")
        if difficulty:
            print(f"   é›£åº¦: {difficulty}")
        if limit:
            print(f"   é™åˆ¶: {limit} å€‹")
        if prioritize_failed:
            print(f"   ç­–ç•¥: å„ªå…ˆæ¸¬è©¦ä½åˆ†/å¤±æ•—æ¡ˆä¾‹")

        conn = self.get_db_connection()
        cur = conn.cursor()

        try:
            # å»ºç«‹æŸ¥è©¢
            query = """
                SELECT
                    ts.id,
                    ts.test_question,
                    ts.expected_category,
                    ts.expected_keywords,
                    ts.difficulty,
                    ts.notes,
                    ts.priority,
                    ts.total_runs,
                    ts.pass_count,
                    ts.avg_score,
                    CASE
                        WHEN ts.total_runs > 0
                        THEN 1.0 - (ts.pass_count::float / ts.total_runs)
                        ELSE 0.5
                    END as fail_rate
                FROM test_scenarios ts
                WHERE ts.is_active = TRUE
                  AND ts.status = 'approved'
            """
            params = []

            # ç¯©é¸é›£åº¦
            if difficulty:
                query += " AND ts.difficulty = %s"
                params.append(difficulty)

            # ç¯©é¸æœ€ä½åˆ†æ•¸
            if min_avg_score is not None:
                query += " AND (ts.avg_score IS NULL OR ts.avg_score <= %s)"
                params.append(min_avg_score)

            # æ’åºç­–ç•¥
            if prioritize_failed:
                # å„ªå…ˆé¸æ“‡ï¼šå¤±æ•—ç‡é«˜ã€å¹³å‡åˆ†ä½ã€å„ªå…ˆç´šé«˜çš„æ¸¬è©¦
                query += " ORDER BY fail_rate DESC, COALESCE(ts.avg_score, 0) ASC, ts.priority DESC, ts.id"
            else:
                # é è¨­æ’åºï¼šå„ªå…ˆç´šé«˜çš„å…ˆæ¸¬è©¦
                query += " ORDER BY ts.priority DESC, ts.id"

            # é™åˆ¶æ•¸é‡
            if limit:
                query += " LIMIT %s"
                params.append(limit)

            cur.execute(query, params)
            rows = cur.fetchall()

            # è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            scenarios = []
            for row in rows:
                scenario = dict(row)
                # è½‰æ›é—œéµå­—é™£åˆ—ç‚ºé€—è™Ÿåˆ†éš”å­—ä¸²ï¼ˆèˆ‡ Excel æ ¼å¼ä¸€è‡´ï¼‰
                if scenario.get('expected_keywords') and isinstance(scenario['expected_keywords'], list):
                    scenario['expected_keywords'] = ', '.join(scenario['expected_keywords'])
                scenarios.append(scenario)

            print(f"   âœ… è¼‰å…¥ {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
            return scenarios

        finally:
            cur.close()
            conn.close()

    def load_test_scenarios(
        self,
        excel_path: str = None,
        difficulty: str = None,
        limit: int = None,
        prioritize_failed: bool = True
    ) -> List[Dict]:
        """è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒï¼ˆæ”¯æ´è³‡æ–™åº«èˆ‡ Excel å…©ç¨®æ¨¡å¼ï¼‰

        Args:
            excel_path: Excel æª”æ¡ˆè·¯å¾‘ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            difficulty: é›£åº¦ç¯©é¸
            limit: é™åˆ¶æ•¸é‡
            prioritize_failed: å„ªå…ˆé¸æ“‡å¤±æ•—ç‡é«˜çš„æ¸¬è©¦ï¼ˆåƒ…è³‡æ–™åº«æ¨¡å¼ï¼‰

        Returns:
            æ¸¬è©¦æƒ…å¢ƒåˆ—è¡¨
        """
        if self.use_database:
            # ä½¿ç”¨è³‡æ–™åº«æ¨¡å¼
            return self.load_test_scenarios_from_db(
                difficulty=difficulty,
                limit=limit,
                prioritize_failed=prioritize_failed
            )
        elif excel_path:
            # ä½¿ç”¨ Excel æ¨¡å¼ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            print(f"ğŸ“– è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ: {excel_path}")
            df = pd.read_excel(excel_path, engine='openpyxl')
            scenarios = df.to_dict('records')
            print(f"   âœ… è¼‰å…¥ {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
            return scenarios
        else:
            raise ValueError("å¿…é ˆæä¾› excel_path æˆ–å•Ÿç”¨è³‡æ–™åº«æ¨¡å¼")

    def query_rag_system(self, question: str) -> Dict:
        """æŸ¥è©¢ RAG ç³»çµ±"""
        url = f"{self.base_url}/api/v1/message"

        payload = {
            "message": question,
            "vendor_id": self.vendor_id,
            "mode": "tenant",
            "include_sources": True
        }

        # â­ å›æ¸¬å°ˆç”¨ï¼šæª¢æŸ¥æ˜¯å¦ç¦ç”¨ç­”æ¡ˆåˆæˆ
        disable_synthesis = os.getenv("BACKTEST_DISABLE_ANSWER_SYNTHESIS", "false").lower() == "true"
        if disable_synthesis:
            payload["disable_answer_synthesis"] = True
            # åªåœ¨ç¬¬ä¸€æ¬¡è«‹æ±‚æ™‚é¡¯ç¤ºæç¤º
            if not hasattr(self, '_synthesis_disabled_logged'):
                print("   âš™ï¸  å›æ¸¬æ¨¡å¼ï¼šç­”æ¡ˆåˆæˆå·²ç¦ç”¨ï¼ˆBACKTEST_DISABLE_ANSWER_SYNTHESIS=trueï¼‰")
                self._synthesis_disabled_logged = True

        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            return response.json()

        except requests.exceptions.RequestException as e:
            print(f"   âŒ API è«‹æ±‚å¤±æ•—: {e}")
            return None

    def evaluate_answer(
        self,
        test_scenario: Dict,
        system_response: Dict
    ) -> Dict:
        """è©•ä¼°ç­”æ¡ˆå“è³ª"""

        if not system_response:
            return {
                "passed": False,
                "score": 0.0,
                "checks": {},
                "reason": "ç³»çµ±ç„¡å›æ‡‰",
                "optimization_tips": "ç³»çµ±ç„¡æ³•å›æ‡‰ï¼Œè«‹æª¢æŸ¥ RAG API æ˜¯å¦æ­£å¸¸é‹ä½œ"
            }

        evaluation = {
            "passed": True,
            "score": 0.0,
            "checks": {},
            "optimization_tips": []
        }

        # 1. æª¢æŸ¥åˆ†é¡æ˜¯å¦æ­£ç¢ºï¼ˆæ”¯æ´å¤š Intentï¼‰
        expected_category = test_scenario.get('expected_category', '')
        actual_intent = system_response.get('intent_name', '')
        all_intents = system_response.get('all_intents')

        # ç¢ºä¿ all_intents æ˜¯åˆ—è¡¨
        if all_intents is None or not all_intents:
            all_intents = [actual_intent] if actual_intent else []

        if expected_category:
            # æª¢æŸ¥é æœŸåˆ†é¡æ˜¯å¦åœ¨ä¸»è¦æ„åœ–æˆ–æ‰€æœ‰ç›¸é—œæ„åœ–ä¸­
            # æ”¯æ´éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ã€Œå¸³å‹™å•é¡Œã€å¯ä»¥åŒ¹é…ã€Œå¸³å‹™æŸ¥è©¢ã€ï¼‰
            def fuzzy_match(expected: str, actual: str) -> bool:
                """æ¨¡ç³ŠåŒ¹é…ï¼šæª¢æŸ¥æ˜¯å¦æœ‰å…±åŒçš„é—œéµå­—"""
                # ç›´æ¥åŒ…å«é—œä¿‚
                if expected in actual or actual in expected:
                    return True
                # æå–å‰å…©å€‹å­—åšæ¨¡ç³ŠåŒ¹é…ï¼ˆä¾‹å¦‚ã€Œå¸³å‹™ã€ï¼‰
                if len(expected) >= 2 and len(actual) >= 2:
                    if expected[:2] in actual or actual[:2] in expected:
                        return True
                return False

            category_match = (
                fuzzy_match(expected_category, actual_intent) or
                any(fuzzy_match(expected_category, intent) for intent in all_intents)
            )

            evaluation['checks']['category_match'] = category_match
            evaluation['checks']['matched_intents'] = all_intents if category_match else []

            if category_match:
                evaluation['score'] += 0.3
                # å¦‚æœåŒ¹é…çš„æ˜¯æ¬¡è¦æ„åœ–ï¼Œçµ¦äºˆæç¤º
                if expected_category not in actual_intent and actual_intent not in expected_category:
                    evaluation['optimization_tips'].append(
                        f"âœ… å¤šæ„åœ–åŒ¹é…: é æœŸã€Œ{expected_category}ã€åœ¨æ¬¡è¦æ„åœ–ä¸­æ‰¾åˆ°\n"
                        f"   ä¸»è¦æ„åœ–: {actual_intent}ï¼Œæ‰€æœ‰æ„åœ–: {all_intents}"
                    )
            else:
                # åˆ†é¡ä¸åŒ¹é… - æä¾›å„ªåŒ–å»ºè­°
                evaluation['optimization_tips'].append(
                    f"æ„åœ–åˆ†é¡ä¸åŒ¹é…: é æœŸã€Œ{expected_category}ã€ä½†è­˜åˆ¥ç‚ºã€Œ{actual_intent}ã€\n"
                    f"   æ‰€æœ‰æ„åœ–: {all_intents}\n"
                    f"ğŸ’¡ å»ºè­°: åœ¨æ„åœ–ç®¡ç†ä¸­ç·¨è¼¯ã€Œ{actual_intent}ã€æ„åœ–ï¼Œæ·»åŠ æ›´å¤šç›¸é—œé—œéµå­—"
                )

        # 2. æª¢æŸ¥æ˜¯å¦åŒ…å«é æœŸé—œéµå­—
        expected_keywords = test_scenario.get('expected_keywords', [])
        if isinstance(expected_keywords, str):
            expected_keywords = [k.strip() for k in expected_keywords.split(',') if k.strip()]
        elif expected_keywords is None:
            expected_keywords = []

        answer = system_response.get('answer', '')
        keyword_matches = sum(1 for kw in expected_keywords if kw in answer)
        keyword_ratio = keyword_matches / len(expected_keywords) if expected_keywords else 0

        evaluation['checks']['keyword_coverage'] = keyword_ratio
        evaluation['score'] += keyword_ratio * 0.4

        if keyword_ratio < 0.5 and expected_keywords:
            missing_keywords = [kw for kw in expected_keywords if kw not in answer]
            evaluation['optimization_tips'].append(
                f"ç­”æ¡ˆç¼ºå°‘é—œéµå­—: {', '.join(missing_keywords)}\n"
                f"ğŸ’¡ å»ºè­°: åœ¨çŸ¥è­˜åº«ä¸­è£œå……ç›¸é—œå…§å®¹ï¼Œæˆ–å„ªåŒ–çŸ¥è­˜çš„é—œéµå­—"
            )

        # 3. æª¢æŸ¥ä¿¡å¿ƒåº¦
        confidence = system_response.get('confidence', 0)
        evaluation['checks']['confidence'] = confidence
        if confidence >= 0.7:
            evaluation['score'] += 0.3
        elif confidence < 0.5:
            evaluation['optimization_tips'].append(
                f"ä¿¡å¿ƒåº¦éä½ ({confidence:.2f})\n"
                f"ğŸ’¡ å»ºè­°: ç³»çµ±å°ç­”æ¡ˆä¸ç¢ºå®šï¼Œå¯èƒ½éœ€è¦æ–°å¢æ›´ç›¸é—œçš„çŸ¥è­˜"
            )

        # 4. åˆ¤å®šæ˜¯å¦é€šé
        evaluation['passed'] = evaluation['score'] >= 0.6

        # 5. ç”Ÿæˆå„ªåŒ–å»ºè­°æ‘˜è¦
        if not evaluation['passed']:
            if not evaluation['optimization_tips']:
                evaluation['optimization_tips'].append(
                    f"æ•´é«”å¾—åˆ†éä½ ({evaluation['score']:.2f}/1.0)\n"
                    f"ğŸ’¡ å»ºè­°: æª¢æŸ¥çŸ¥è­˜åº«æ˜¯å¦æœ‰ç›¸é—œå…§å®¹ï¼Œæˆ–å„ªåŒ–ç¾æœ‰çŸ¥è­˜çš„æè¿°"
                )
        else:
            if evaluation['optimization_tips']:
                # å³ä½¿é€šéï¼Œå¦‚æœæœ‰å„ªåŒ–å»ºè­°ä¹Ÿä¿ç•™
                evaluation['optimization_tips'].insert(0, "âœ… æ¸¬è©¦é€šéï¼Œä½†ä»æœ‰å„ªåŒ–ç©ºé–“:")

        return evaluation

    def llm_evaluate_answer(
        self,
        question: str,
        answer: str,
        expected_intent: str
    ) -> Dict:
        """ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆå“è³ª

        Returns:
            {
                'relevance': 1-5,
                'completeness': 1-5,
                'accuracy': 1-5,
                'intent_match': 1-5,
                'overall': 1-5,
                'reasoning': str
            }
        """
        prompt = f"""è«‹è©•ä¼°ä»¥ä¸‹å•ç­”çš„å“è³ªï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€ä½³ï¼‰ï¼š

å•é¡Œï¼š{question}
é æœŸæ„åœ–ï¼š{expected_intent}
ç­”æ¡ˆï¼š{answer}

è«‹å¾ä»¥ä¸‹ç¶­åº¦è©•åˆ†ï¼š
1. ç›¸é—œæ€§ (Relevance): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”å•é¡Œï¼Ÿ
2. å®Œæ•´æ€§ (Completeness): ç­”æ¡ˆæ˜¯å¦å®Œæ•´æ¶µè“‹å•é¡Œæ‰€å•ï¼Ÿ
3. æº–ç¢ºæ€§ (Accuracy): ç­”æ¡ˆå…§å®¹æ˜¯å¦æº–ç¢ºå¯é ï¼Ÿ
4. æ„åœ–åŒ¹é… (Intent Match): ç­”æ¡ˆæ˜¯å¦ç¬¦åˆé æœŸæ„åœ–ï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼š
{{
    "relevance": <1-5>,
    "completeness": <1-5>,
    "accuracy": <1-5>,
    "intent_match": <1-5>,
    "overall": <1-5>,
    "reasoning": "ç°¡çŸ­èªªæ˜è©•åˆ†ç†ç”±"
}}"""

        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            print(f"âš ï¸  LLM è©•ä¼°å¤±æ•—: {e}")
            return {
                'relevance': 0,
                'completeness': 0,
                'accuracy': 0,
                'intent_match': 0,
                'overall': 0,
                'reasoning': f"è©•ä¼°å¤±æ•—: {str(e)}"
            }

    def evaluate_answer_with_quality(
        self,
        test_scenario: Dict,
        system_response: Dict
    ) -> Dict:
        """æ•´åˆåŸºç¤è©•ä¼°å’Œ LLM å“è³ªè©•ä¼°

        Returns:
            {
                'basic_eval': Dict,  # åŸºç¤è©•ä¼°çµæœ
                'quality_eval': Dict,  # LLM å“è³ªè©•ä¼°ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
                'overall_score': float,  # æ··åˆè©•åˆ†
                'passed': bool
            }
        """
        # 1. åŸ·è¡ŒåŸºç¤è©•ä¼°ï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
        basic_eval = self.evaluate_answer(test_scenario, system_response)

        # 2. å¦‚æœæ˜¯ basic æ¨¡å¼ï¼Œç›´æ¥è¿”å›
        if self.quality_mode == 'basic':
            return {
                'basic_eval': basic_eval,
                'overall_score': basic_eval['score'],
                'passed': basic_eval['passed']
            }

        # 3. åŸ·è¡Œ LLM è©•ä¼°
        question = test_scenario.get('test_question', '')
        answer = system_response.get('answer', '') if system_response else ''
        expected_intent = test_scenario.get('expected_category', '')

        if not answer:
            # æ²’æœ‰ç­”æ¡ˆï¼Œåªä½¿ç”¨åŸºç¤è©•ä¼°
            return {
                'basic_eval': basic_eval,
                'overall_score': basic_eval['score'],
                'passed': basic_eval['passed']
            }

        quality_eval = self.llm_evaluate_answer(question, answer, expected_intent)

        # 4. è¨ˆç®—æ··åˆè©•åˆ†
        overall_score = self._calculate_hybrid_score(basic_eval, quality_eval)

        # 5. åˆ¤å®šæ˜¯å¦é€šé
        passed = self._determine_pass_status(basic_eval, quality_eval, overall_score)

        return {
            'basic_eval': basic_eval,
            'quality_eval': quality_eval,
            'overall_score': overall_score,
            'passed': passed
        }

    def _calculate_hybrid_score(self, basic_eval: Dict, quality_eval: Dict) -> float:
        """è¨ˆç®—æ··åˆè©•åˆ†

        æ¬Šé‡åˆ†é…ï¼š
        - basic æ¨¡å¼ï¼š100% åŸºç¤è©•åˆ†
        - hybrid æ¨¡å¼ï¼š40% åŸºç¤ + 60% LLM
        - detailed æ¨¡å¼ï¼š100% LLM
        """
        basic_score = basic_eval['score']
        quality_score = quality_eval['overall'] / 5.0  # æ¨™æº–åŒ–åˆ° 0-1

        if self.quality_mode == 'hybrid':
            return 0.4 * basic_score + 0.6 * quality_score
        elif self.quality_mode == 'detailed':
            return quality_score
        else:
            return basic_score

    def _determine_pass_status(
        self,
        basic_eval: Dict,
        quality_eval: Dict,
        hybrid_score: float
    ) -> bool:
        """åˆ¤å®šæ˜¯å¦é€šé"""

        if self.quality_mode == 'hybrid':
            # æ··åˆæ¨¡å¼ï¼šç¶œåˆåˆ†æ•¸ >= 0.5 ä¸”å®Œæ•´æ€§ >= 2.5
            return (
                hybrid_score >= 0.5 and
                quality_eval.get('completeness', 0) >= 2.5
            )
        elif self.quality_mode == 'detailed':
            # è©³ç´°æ¨¡å¼ï¼šç¶œåˆ >= 3 ä¸”å®Œæ•´æ€§ >= 3
            return (
                quality_eval.get('overall', 0) >= 3 and
                quality_eval.get('completeness', 0) >= 3
            )
        else:
            # åŸºç¤æ¨¡å¼ï¼šç¾æœ‰é‚è¼¯
            return basic_eval['passed']

    def calculate_ndcg(self, results: List[Dict], k: int = 3) -> Dict:
        """è¨ˆç®—æ‰€æœ‰æ¸¬è©¦çš„å¹³å‡ NDCG@K

        NDCG (Normalized Discounted Cumulative Gain) è¡¡é‡æ’åºå“è³ª

        Returns:
            {
                'avg_ndcg': float,  # å¹³å‡ NDCG
                'count': int  # è¨ˆç®—æ•¸é‡
            }
        """
        def calculate_single_ndcg(relevance_scores: List[float]) -> float:
            """è¨ˆç®—å–®å€‹æ¸¬è©¦çš„ NDCG@K"""
            if not relevance_scores or len(relevance_scores) == 0:
                return 0.0

            # DCG (Discounted Cumulative Gain)
            dcg = 0.0
            for i, score in enumerate(relevance_scores[:k], 1):
                dcg += (2 ** score - 1) / math.log2(i + 1)

            # IDCG (Ideal DCG) - æœ€ä½³æ’åº
            ideal_scores = sorted(relevance_scores, reverse=True)[:k]
            idcg = 0.0
            for i, score in enumerate(ideal_scores, 1):
                idcg += (2 ** score - 1) / math.log2(i + 1)

            return dcg / idcg if idcg > 0 else 0.0

        # è¨ˆç®—æ¯å€‹æ¸¬è©¦çš„ NDCG
        ndcg_scores = []
        for result in results:
            if 'quality_eval' in result and result['quality_eval']:
                # ä½¿ç”¨ LLM è©•ä¼°çš„ç›¸é—œæ€§åˆ†æ•¸
                relevance = result['quality_eval'].get('relevance', 0)
                if relevance > 0:
                    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå‡è¨­æ¯å€‹æ¸¬è©¦åªæœ‰ä¸€å€‹ç­”æ¡ˆ
                    # å¯¦éš›æ‡‰ç”¨ä¸­å¯ä»¥åŸºæ–¼å¤šå€‹çŸ¥è­˜ä¾†æºè¨ˆç®— NDCG
                    ndcg_scores.append(relevance / 5.0)  # æ¨™æº–åŒ–åˆ° 0-1

        if ndcg_scores:
            avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)
            return {
                'avg_ndcg': avg_ndcg,
                'count': len(ndcg_scores)
            }

        return {'avg_ndcg': 0.0, 'count': 0}

    def run_backtest(
        self,
        test_scenarios: List[Dict],
        sample_size: int = None,
        delay: float = 1.0
    ) -> List[Dict]:
        """åŸ·è¡Œå›æ¸¬"""

        print(f"\nğŸ§ª é–‹å§‹å›æ¸¬...")
        print(f"   æ¸¬è©¦æƒ…å¢ƒæ•¸ï¼š{len(test_scenarios)}")
        if sample_size:
            print(f"   æŠ½æ¨£æ¸¬è©¦ï¼š{sample_size} å€‹")
            test_scenarios = test_scenarios[:sample_size]

        results = []

        for i, scenario in enumerate(test_scenarios, 1):
            question = scenario.get('test_question', '')
            if not question:
                continue

            print(f"\n[{i}/{len(test_scenarios)}] æ¸¬è©¦å•é¡Œ: {question[:50]}...")

            # æŸ¥è©¢ç³»çµ±
            system_response = self.query_rag_system(question)

            # è©•ä¼°ç­”æ¡ˆï¼ˆä½¿ç”¨å¢å¼·è©•ä¼°ï¼‰
            evaluation_result = self.evaluate_answer_with_quality(scenario, system_response)

            # æå–è©•ä¼°è³‡è¨Šï¼ˆå‘å¾Œç›¸å®¹ï¼‰
            if 'basic_eval' in evaluation_result:
                evaluation = evaluation_result['basic_eval']
                quality_eval = evaluation_result.get('quality_eval')
                overall_score = evaluation_result.get('overall_score', evaluation['score'])
                passed = evaluation_result.get('passed', evaluation['passed'])
            else:
                # å‘å¾Œç›¸å®¹ï¼ˆå¦‚æœåªè¿”å›åŸºç¤è©•ä¼°ï¼‰
                evaluation = evaluation_result
                quality_eval = None
                overall_score = evaluation['score']
                passed = evaluation['passed']

            # æå–çŸ¥è­˜ä¾†æºè³‡è¨Š
            sources = system_response.get('sources', []) if system_response else []
            # ç¢ºä¿ sources æ˜¯åˆ—è¡¨
            if sources is None:
                sources = []
            source_ids = [s.get('id') for s in sources if s.get('id')]
            source_summary = '; '.join([
                f"[{s.get('id', 'N/A')}] {s.get('question_summary', 'N/A')[:40]}"
                for s in sources[:3]  # åªé¡¯ç¤ºå‰ 3 å€‹
            ]) if sources else 'ç„¡ä¾†æº'

            # ç”ŸæˆçŸ¥è­˜åº«ç®¡ç†ç•Œé¢çš„ç›´æ¥éˆæ¥
            knowledge_urls = []
            if source_ids:
                # æ–¹æ¡ˆ1ï¼šå–®å€‹çŸ¥è­˜çš„ç›´æ¥éˆæ¥
                for kb_id in source_ids[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                    knowledge_urls.append(f"http://localhost:8080/#/knowledge?search={kb_id}")
                # æ–¹æ¡ˆ2ï¼šæ‰¹é‡æŸ¥è©¢éˆæ¥ï¼ˆç”¨ IDs ä½œç‚ºæœå°‹æ¢ä»¶ï¼‰
                ids_param = ','.join(map(str, source_ids))
                batch_url = f"http://localhost:8080/#/knowledge?ids={ids_param}"
            else:
                batch_url = "http://localhost:8080/#/knowledge"

            knowledge_links = '\n'.join(knowledge_urls) if knowledge_urls else 'ç„¡'

            # è¨˜éŒ„çµæœ
            result = {
                'test_id': i,
                'scenario_id': scenario.get('id'),  # æ–°å¢ï¼šæ¸¬è©¦æƒ…å¢ƒ IDï¼ˆç”¨æ–¼è³‡æ–™åº«ï¼‰
                'test_question': question,
                'expected_category': scenario.get('expected_category', ''),
                'actual_intent': system_response.get('intent_name', '') if system_response else '',
                'all_intents': system_response.get('all_intents', []) if system_response else [],
                'system_answer': system_response.get('answer', '')[:200] if system_response else '',
                'confidence': system_response.get('confidence', 0) if system_response else 0,
                'score': evaluation['score'],
                'overall_score': overall_score,  # æ–°å¢ï¼šæ··åˆè©•åˆ†
                'passed': passed,  # ä½¿ç”¨æ··åˆåˆ¤å®š
                'category_match': evaluation['checks'].get('category_match', False),
                'keyword_coverage': evaluation['checks'].get('keyword_coverage', 0.0),
                'evaluation': json.dumps(evaluation['checks'], ensure_ascii=False),
                'optimization_tips': '\n'.join(evaluation.get('optimization_tips', [])) if isinstance(evaluation.get('optimization_tips'), list) else evaluation.get('optimization_tips', ''),
                'knowledge_sources': source_summary,
                'source_ids': ','.join(map(str, source_ids)),
                'source_count': len(sources),
                'knowledge_links': knowledge_links,
                'batch_url': batch_url,
                'difficulty': scenario.get('difficulty', 'medium'),
                'notes': scenario.get('notes', ''),
                'timestamp': datetime.now().isoformat()
            }

            # å¦‚æœæœ‰ LLM å“è³ªè©•ä¼°ï¼Œæ·»åŠ åˆ°çµæœä¸­
            if quality_eval:
                result['quality_eval'] = json.dumps(quality_eval, ensure_ascii=False)
                result['relevance'] = quality_eval.get('relevance', 0)
                result['completeness'] = quality_eval.get('completeness', 0)
                result['accuracy'] = quality_eval.get('accuracy', 0)
                result['intent_match'] = quality_eval.get('intent_match', 0)
                result['quality_overall'] = quality_eval.get('overall', 0)
                result['quality_reasoning'] = quality_eval.get('reasoning', '')

            results.append(result)

            # é¡¯ç¤ºçµæœ
            status = "âœ… PASS" if evaluation['passed'] else "âŒ FAIL"
            print(f"   {status} (åˆ†æ•¸: {evaluation['score']:.2f})")

            # é¡¯ç¤ºçŸ¥è­˜ä¾†æº
            if sources:
                print(f"   ğŸ“š çŸ¥è­˜ä¾†æº ({len(sources)} å€‹):")
                for idx, src in enumerate(sources[:3], 1):  # åªé¡¯ç¤ºå‰3å€‹
                    kb_id = src.get('id', 'N/A')
                    title = src.get('question_summary', 'N/A')[:50]
                    print(f"      {idx}. [ID {kb_id}] {title}")

                # é¡¯ç¤ºçŸ¥è­˜åº«ç›´æ¥éˆæ¥
                if knowledge_urls:
                    print(f"   ğŸ”— ç›´æ¥éˆæ¥:")
                    for idx, url in enumerate(knowledge_urls[:3], 1):
                        print(f"      {idx}. {url}")
                    print(f"   ğŸ“¦ æ‰¹é‡æŸ¥è©¢: {batch_url}")

            # é¡¯ç¤ºå„ªåŒ–å»ºè­°
            if evaluation.get('optimization_tips'):
                tips = evaluation['optimization_tips']
                if isinstance(tips, list):
                    for tip in tips:
                        print(f"   {tip}")
                else:
                    print(f"   {tips}")

            # é¿å… API rate limit
            time.sleep(delay)

        return results

    def generate_report(self, results: List[Dict], output_path: str):
        """ç”Ÿæˆå›æ¸¬å ±å‘Š"""

        print(f"\nğŸ“Š ç”Ÿæˆå›æ¸¬å ±å‘Š...")

        # è¨ˆç®—çµ±è¨ˆ
        total_tests = len(results)
        passed_tests = sum(1 for r in results if r['passed'])
        pass_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        avg_score = sum(r['score'] for r in results) / total_tests if total_tests > 0 else 0
        avg_confidence = sum(r['confidence'] for r in results) / total_tests if total_tests > 0 else 0

        # æª¢æŸ¥æ˜¯å¦æœ‰å“è³ªè©•ä¼°è³‡æ–™
        has_quality_eval = any('relevance' in r for r in results)

        # è¨ˆç®—å“è³ªæŒ‡æ¨™ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        quality_stats = {}
        if has_quality_eval:
            quality_results = [r for r in results if 'relevance' in r]
            if quality_results:
                quality_stats = {
                    'avg_relevance': sum(r['relevance'] for r in quality_results) / len(quality_results),
                    'avg_completeness': sum(r['completeness'] for r in quality_results) / len(quality_results),
                    'avg_accuracy': sum(r['accuracy'] for r in quality_results) / len(quality_results),
                    'avg_intent_match': sum(r['intent_match'] for r in quality_results) / len(quality_results),
                    'avg_quality_overall': sum(r['quality_overall'] for r in quality_results) / len(quality_results),
                    'quality_count': len(quality_results)
                }

                # è¨ˆç®— NDCG
                ndcg_data = self.calculate_ndcg(quality_results)
                quality_stats['ndcg'] = ndcg_data['avg_ndcg']
                quality_stats['ndcg_count'] = ndcg_data['count']

        # æŒ‰é›£åº¦åˆ†çµ„
        by_difficulty = {}
        for r in results:
            diff = r.get('difficulty', 'medium')
            if diff not in by_difficulty:
                by_difficulty[diff] = {'total': 0, 'passed': 0}
            by_difficulty[diff]['total'] += 1
            if r['passed']:
                by_difficulty[diff]['passed'] += 1

        # å»ºç«‹ DataFrame
        df = pd.DataFrame(results)

        # å„²å­˜è©³ç´°çµæœ
        df.to_excel(output_path, index=False, engine='openpyxl')
        print(f"   âœ… è©³ç´°çµæœå·²å„²å­˜: {output_path}")

        # ç”Ÿæˆæ‘˜è¦å ±å‘Š
        summary_path = output_path.replace('.xlsx', '_summary.txt')
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("="*60 + "\n")
            f.write("çŸ¥è­˜åº«å›æ¸¬å ±å‘Š\n")
            f.write("="*60 + "\n\n")

            f.write(f"æ¸¬è©¦æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"RAG ç³»çµ±ï¼š{self.base_url}\n")
            f.write(f"æ¥­è€… IDï¼š{self.vendor_id}\n")
            f.write(f"å“è³ªè©•ä¼°æ¨¡å¼ï¼š{self.quality_mode}\n\n")

            f.write("="*60 + "\n")
            f.write("æ•´é«”çµ±è¨ˆ\n")
            f.write("="*60 + "\n")
            f.write(f"ç¸½æ¸¬è©¦æ•¸ï¼š{total_tests}\n")
            f.write(f"é€šéæ•¸ï¼š{passed_tests}\n")
            f.write(f"å¤±æ•—æ•¸ï¼š{total_tests - passed_tests}\n")
            f.write(f"é€šéç‡ï¼š{pass_rate:.2f}%\n")
            f.write(f"å¹³å‡åˆ†æ•¸ï¼ˆåŸºç¤ï¼‰ï¼š{avg_score:.2f}\n")
            f.write(f"å¹³å‡ä¿¡å¿ƒåº¦ï¼š{avg_confidence:.2f}\n\n")

            # æ–°å¢ï¼šå“è³ªè©•ä¼°çµ±è¨ˆ
            if quality_stats:
                f.write("="*60 + "\n")
                f.write("LLM å“è³ªè©•ä¼°çµ±è¨ˆ\n")
                f.write("="*60 + "\n")
                f.write(f"è©•ä¼°æ¸¬è©¦æ•¸ï¼š{quality_stats['quality_count']}\n")
                f.write(f"å¹³å‡ç›¸é—œæ€§ (Relevance)ï¼š{quality_stats['avg_relevance']:.2f}/5.0\n")
                f.write(f"å¹³å‡å®Œæ•´æ€§ (Completeness)ï¼š{quality_stats['avg_completeness']:.2f}/5.0\n")
                f.write(f"å¹³å‡æº–ç¢ºæ€§ (Accuracy)ï¼š{quality_stats['avg_accuracy']:.2f}/5.0\n")
                f.write(f"å¹³å‡æ„åœ–åŒ¹é… (Intent Match)ï¼š{quality_stats['avg_intent_match']:.2f}/5.0\n")
                f.write(f"å¹³å‡ç¶œåˆè©•åˆ† (Overall)ï¼š{quality_stats['avg_quality_overall']:.2f}/5.0\n")
                f.write(f"NDCG@3 (æ’åºå“è³ª)ï¼š{quality_stats['ndcg']:.4f}\n")
                f.write("\nå“è³ªè©•ç´š:\n")

                # è©•ç´šé‚è¼¯
                def get_rating(score):
                    if score >= 4.0:
                        return "ğŸ‰ å„ªç§€"
                    elif score >= 3.5:
                        return "âœ… è‰¯å¥½"
                    elif score >= 3.0:
                        return "âš ï¸  ä¸­ç­‰"
                    else:
                        return "âŒ éœ€æ”¹å–„"

                f.write(f"  ç›¸é—œæ€§ï¼š{get_rating(quality_stats['avg_relevance'])}\n")
                f.write(f"  å®Œæ•´æ€§ï¼š{get_rating(quality_stats['avg_completeness'])}\n")
                f.write(f"  æº–ç¢ºæ€§ï¼š{get_rating(quality_stats['avg_accuracy'])}\n")
                f.write(f"  æ„åœ–åŒ¹é…ï¼š{get_rating(quality_stats['avg_intent_match'])}\n")
                f.write(f"  ç¶œåˆè©•åˆ†ï¼š{get_rating(quality_stats['avg_quality_overall'])}\n")

                # NDCG è©•ç´š
                ndcg_rating = "ğŸ‰ å„ªç§€" if quality_stats['ndcg'] >= 0.9 else \
                              "âœ… è‰¯å¥½" if quality_stats['ndcg'] >= 0.7 else \
                              "âš ï¸  ä¸­ç­‰" if quality_stats['ndcg'] >= 0.5 else "âŒ éœ€æ”¹å–„"
                f.write(f"  æ’åºå“è³ªï¼š{ndcg_rating}\n\n")

            f.write("="*60 + "\n")
            f.write("æŒ‰é›£åº¦çµ±è¨ˆ\n")
            f.write("="*60 + "\n")
            for diff, stats in by_difficulty.items():
                rate = (stats['passed'] / stats['total'] * 100) if stats['total'] > 0 else 0
                f.write(f"{diff.upper():10s}: {stats['passed']}/{stats['total']} ({rate:.1f}%)\n")

            f.write("\n" + "="*60 + "\n")
            f.write("å¤±æ•—æ¡ˆä¾‹\n")
            f.write("="*60 + "\n")

            failed = [r for r in results if not r['passed']]
            if failed:
                for r in failed[:10]:  # åªé¡¯ç¤ºå‰ 10 å€‹
                    f.write(f"\nå•é¡Œï¼š{r['test_question']}\n")
                    f.write(f"é æœŸåˆ†é¡ï¼š{r['expected_category']}\n")
                    f.write(f"å¯¦éš›æ„åœ–ï¼š{r['actual_intent']}\n")
                    f.write(f"åˆ†æ•¸ï¼š{r['score']:.2f}\n")
                    f.write(f"çŸ¥è­˜ä¾†æºï¼š{r.get('knowledge_sources', 'ç„¡')}\n")
                    f.write(f"ä¾†æºIDsï¼š{r.get('source_ids', 'ç„¡')}\n")
                    # æ–°å¢ï¼šçŸ¥è­˜åº«ç›´æ¥éˆæ¥
                    knowledge_links = r.get('knowledge_links', 'ç„¡')
                    if knowledge_links and knowledge_links != 'ç„¡':
                        f.write(f"çŸ¥è­˜åº«éˆæ¥ï¼š\n{knowledge_links}\n")
                    batch_url = r.get('batch_url', '')
                    if batch_url:
                        f.write(f"æ‰¹é‡æŸ¥è©¢ï¼š{batch_url}\n")
                    f.write(f"å„ªåŒ–å»ºè­°ï¼š\n{r.get('optimization_tips', 'ç„¡')}\n")
                    f.write("-" * 60 + "\n")
            else:
                f.write("\nç„¡å¤±æ•—æ¡ˆä¾‹ ğŸ‰\n")

        print(f"   âœ… æ‘˜è¦å ±å‘Šå·²å„²å­˜: {summary_path}")

        # åˆ—å°æ‘˜è¦åˆ°æ§åˆ¶å°
        print(f"\n{'='*60}")
        print("å›æ¸¬æ‘˜è¦")
        print(f"{'='*60}")
        print(f"é€šéç‡ï¼š{pass_rate:.2f}% ({passed_tests}/{total_tests})")
        print(f"å¹³å‡åˆ†æ•¸ï¼ˆåŸºç¤ï¼‰ï¼š{avg_score:.2f}")
        print(f"å¹³å‡ä¿¡å¿ƒåº¦ï¼š{avg_confidence:.2f}")

        # é¡¯ç¤ºå“è³ªè©•ä¼°çµæœ
        if quality_stats:
            print(f"\nğŸ¯ LLM å“è³ªè©•ä¼°çµ±è¨ˆ ({quality_stats['quality_count']} å€‹æ¸¬è©¦):")
            print(f"   ç›¸é—œæ€§ï¼š{quality_stats['avg_relevance']:.2f}/5.0")
            print(f"   å®Œæ•´æ€§ï¼š{quality_stats['avg_completeness']:.2f}/5.0")
            print(f"   æº–ç¢ºæ€§ï¼š{quality_stats['avg_accuracy']:.2f}/5.0")
            print(f"   æ„åœ–åŒ¹é…ï¼š{quality_stats['avg_intent_match']:.2f}/5.0")
            print(f"   ç¶œåˆè©•åˆ†ï¼š{quality_stats['avg_quality_overall']:.2f}/5.0")
            print(f"   NDCG@3ï¼š{quality_stats['ndcg']:.4f}")

        print(f"{'='*60}\n")

        # çµ„è£è¿”å›è³‡æ–™
        summary_data = {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'pass_rate': pass_rate,
            'avg_score': avg_score,
            'avg_confidence': avg_confidence,
            'quality_mode': self.quality_mode
        }

        # æ·»åŠ å“è³ªçµ±è¨ˆ
        if quality_stats:
            summary_data['quality_stats'] = quality_stats

        return summary_data

    def save_results_to_database(self, results: List[Dict], summary_data: Dict, output_path: str):
        """å„²å­˜å›æ¸¬çµæœåˆ°è³‡æ–™åº«"""
        print(f"\nğŸ’¾ å„²å­˜å›æ¸¬çµæœåˆ°è³‡æ–™åº«...")

        if not self.use_database:
            print("   âš ï¸  è³‡æ–™åº«æ¨¡å¼æœªå•Ÿç”¨ï¼Œè·³éå„²å­˜")
            return None

        conn = self.get_db_connection()
        cur = conn.cursor()

        try:
            # 1. å»ºç«‹ backtest_run è¨˜éŒ„
            completed_at = datetime.now()
            duration_seconds = int((completed_at - self.run_started_at).total_seconds())

            cur.execute("""
                INSERT INTO backtest_runs (
                    quality_mode, test_type, total_scenarios, executed_scenarios,
                    status, rag_api_url, vendor_id,
                    passed_count, failed_count, pass_rate,
                    avg_score, avg_confidence,
                    avg_relevance, avg_completeness, avg_accuracy,
                    avg_intent_match, avg_quality_overall, ndcg_score,
                    started_at, completed_at, duration_seconds,
                    output_file_path, summary_file_path, executed_by
                ) VALUES (
                    %s, %s, %s, %s, 'completed', %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s
                ) RETURNING id
            """, (
                self.quality_mode,
                os.getenv('BACKTEST_TYPE', 'full'),
                summary_data['total_tests'],
                summary_data['total_tests'],
                self.base_url,
                self.vendor_id,
                summary_data['passed_tests'],
                summary_data['total_tests'] - summary_data['passed_tests'],
                summary_data['pass_rate'],
                summary_data['avg_score'],
                summary_data['avg_confidence'],
                summary_data.get('quality_stats', {}).get('avg_relevance'),
                summary_data.get('quality_stats', {}).get('avg_completeness'),
                summary_data.get('quality_stats', {}).get('avg_accuracy'),
                summary_data.get('quality_stats', {}).get('avg_intent_match'),
                summary_data.get('quality_stats', {}).get('avg_quality_overall'),
                summary_data.get('quality_stats', {}).get('ndcg'),
                self.run_started_at,
                completed_at,
                duration_seconds,
                output_path,
                output_path.replace('.xlsx', '_summary.txt'),
                'backtest_framework'
            ))

            run_id = cur.fetchone()['id']
            print(f"   âœ… å»ºç«‹å›æ¸¬åŸ·è¡Œè¨˜éŒ„ (Run ID: {run_id})")

            # 2. æ’å…¥æ¯å€‹æ¸¬è©¦çµæœ
            inserted_count = 0
            for result in results:
                # æº–å‚™ all_intents é™£åˆ—
                all_intents = result.get('all_intents', [])
                if isinstance(all_intents, str):
                    all_intents = [all_intents] if all_intents else []

                cur.execute("""
                    INSERT INTO backtest_results (
                        run_id, scenario_id, test_question, expected_category,
                        actual_intent, all_intents, system_answer, confidence,
                        score, overall_score, passed,
                        category_match, keyword_coverage,
                        relevance, completeness, accuracy, intent_match,
                        quality_overall, quality_reasoning,
                        source_ids, source_count, knowledge_sources, optimization_tips,
                        evaluation
                    ) VALUES (
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                        %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
                    )
                """, (
                    run_id,
                    result.get('scenario_id'),
                    result['test_question'],
                    result.get('expected_category'),
                    result.get('actual_intent'),
                    all_intents,
                    result.get('system_answer'),
                    result.get('confidence', 0),
                    result.get('score', 0),
                    result.get('overall_score', result.get('score', 0)),
                    result.get('passed', False),
                    result.get('category_match', False),
                    result.get('keyword_coverage', 0.0),
                    result.get('relevance'),
                    result.get('completeness'),
                    result.get('accuracy'),
                    result.get('intent_match'),
                    result.get('quality_overall'),
                    result.get('quality_reasoning'),
                    result.get('source_ids'),
                    result.get('source_count', 0),
                    result.get('knowledge_sources'),
                    result.get('optimization_tips'),
                    result.get('evaluation')
                ))
                inserted_count += 1

            conn.commit()
            print(f"   âœ… å„²å­˜ {inserted_count} å€‹æ¸¬è©¦çµæœåˆ°è³‡æ–™åº«")
            print(f"   ğŸ“Š å›æ¸¬åŸ·è¡Œ ID: {run_id}")
            print(f"   â±ï¸  åŸ·è¡Œæ™‚é–“: {duration_seconds} ç§’")

            return run_id

        except Exception as e:
            conn.rollback()
            print(f"   âŒ å„²å­˜åˆ°è³‡æ–™åº«å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return None

        finally:
            cur.close()
            conn.close()


def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("çŸ¥è­˜åº«å›æ¸¬æ¡†æ¶")
    print("="*60)

    # é…ç½®
    base_url = os.getenv("RAG_API_URL", "http://localhost:8100")
    vendor_id = int(os.getenv("VENDOR_ID", "1"))
    quality_mode = os.getenv("BACKTEST_QUALITY_MODE", "basic")  # basic, detailed, hybrid

    # è³‡æ–™åº«æ¨¡å¼æ§åˆ¶ï¼ˆé è¨­å•Ÿç”¨ï¼‰
    use_database = os.getenv("BACKTEST_USE_DATABASE", "true").lower() == "true"

    # å–å¾—å°ˆæ¡ˆæ ¹ç›®éŒ„
    project_root = os.getenv("PROJECT_ROOT", os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

    # æ”¯æ´é¸æ“‡ä¸åŒçš„æ¸¬è©¦æª”æ¡ˆï¼ˆsmoke tests æˆ– full testsï¼‰
    test_type = os.getenv("BACKTEST_TYPE", "smoke")  # smoke, full, or custom

    output_path = os.path.join(project_root, "output/backtest/backtest_results.xlsx")

    # å»ºç«‹å›æ¸¬æ¡†æ¶ï¼ˆå¸¶å“è³ªè©•ä¼°æ¨¡å¼èˆ‡è³‡æ–™åº«æ”¯æ´ï¼‰
    backtest = BacktestFramework(base_url, vendor_id, quality_mode, use_database=use_database)

    # è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ
    if use_database:
        # è³‡æ–™åº«æ¨¡å¼ï¼šåŸºæ–¼è©•åˆ†å’Œé›£åº¦ç¯©é¸
        difficulty = os.getenv("BACKTEST_DIFFICULTY")  # easy, medium, hard, or None for all
        prioritize_failed = os.getenv("BACKTEST_PRIORITIZE_FAILED", "true").lower() == "true"

        try:
            scenarios = backtest.load_test_scenarios(
                difficulty=difficulty,
                prioritize_failed=prioritize_failed
            )
        except Exception as e:
            print(f"âŒ å¾è³‡æ–™åº«è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒå¤±æ•—: {e}")
            print("ğŸ’¡ æç¤ºï¼šè«‹ç¢ºèªè³‡æ–™åº«é€£ç·šæ­£å¸¸ï¼Œä¸”å·²åŸ·è¡Œæ¸¬è©¦é¡Œåº«é·ç§»")
            print("   æˆ–è¨­å®š BACKTEST_USE_DATABASE=false ä½¿ç”¨ Excel æ¨¡å¼")
            return
    else:
        # Excel æ¨¡å¼ï¼šä½¿ç”¨æª”æ¡ˆè·¯å¾‘ï¼ˆå‘å¾Œç›¸å®¹ï¼‰
        if test_type == "smoke":
            test_scenarios_path = os.path.join(project_root, "test_scenarios_smoke.xlsx")
        elif test_type == "full":
            test_scenarios_path = os.path.join(project_root, "test_scenarios_full.xlsx")
        else:
            # custom: ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æŒ‡å®šçš„è·¯å¾‘
            test_scenarios_path = os.getenv("BACKTEST_SCENARIOS_PATH", os.path.join(project_root, "test_scenarios.xlsx"))

        # æª¢æŸ¥æ–‡ä»¶
        if not os.path.exists(test_scenarios_path):
            print(f"âŒ æ¸¬è©¦æƒ…å¢ƒæ–‡ä»¶ä¸å­˜åœ¨: {test_scenarios_path}")
            print("è«‹å…ˆåŸ·è¡Œ extract_knowledge_and_tests.py æå–æ¸¬è©¦æƒ…å¢ƒ")
            print("æˆ–è¨­å®š BACKTEST_USE_DATABASE=true ä½¿ç”¨è³‡æ–™åº«æ¨¡å¼")
            return

        scenarios = backtest.load_test_scenarios(excel_path=test_scenarios_path)

    # åŸ·è¡Œå›æ¸¬
    # æ”¯æ´éäº¤äº’æ¨¡å¼ï¼ˆå¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨£æœ¬æ•¸é‡ï¼‰
    non_interactive = os.getenv("BACKTEST_NON_INTERACTIVE", "false").lower() == "true"

    if non_interactive:
        # éäº¤äº’æ¨¡å¼ï¼šç›´æ¥åŸ·è¡Œå…¨éƒ¨æ¸¬è©¦
        sample_size_str = os.getenv("BACKTEST_SAMPLE_SIZE", "")
        if sample_size_str:
            sample_size = int(sample_size_str)
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œ {sample_size} å€‹æ¸¬è©¦")
        else:
            sample_size = None
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œå…¨éƒ¨ {len(scenarios)} å€‹æ¸¬è©¦")
    else:
        # äº¤äº’æ¨¡å¼ï¼šè©¢å•ç”¨æˆ¶
        print(f"\næ˜¯å¦è¦åŸ·è¡Œå®Œæ•´å›æ¸¬ï¼Ÿ")
        print(f"ç¸½å…± {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
        sample_size = input("è¼¸å…¥è¦æ¸¬è©¦çš„æ•¸é‡ï¼ˆç›´æ¥æŒ‰ Enter æ¸¬è©¦å…¨éƒ¨ï¼‰: ").strip()

        if sample_size:
            sample_size = int(sample_size)
        else:
            sample_size = None

    results = backtest.run_backtest(scenarios, sample_size=sample_size)

    # ç”Ÿæˆå ±å‘Š
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    summary_data = backtest.generate_report(results, output_path)

    # å„²å­˜åˆ°è³‡æ–™åº«
    backtest.save_results_to_database(results, summary_data, output_path)

    print("âœ… å›æ¸¬å®Œæˆï¼")


if __name__ == "__main__":
    main()
