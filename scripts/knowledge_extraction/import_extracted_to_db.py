"""
å¾æå–çš„ Excel çŸ¥è­˜åº«åŒ¯å…¥è³‡æ–™åº«
è™•ç† extract_knowledge_and_tests.py ç”Ÿæˆçš„æ ¼å¼
"""

import os
import sys
import pandas as pd
import asyncpg
from datetime import datetime
from typing import List, Dict
from openai import OpenAI

# è³‡æ–™åº«é…ç½®
DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "port": int(os.getenv("DB_PORT", "5432")),
    "database": os.getenv("DB_NAME", "aichatbot_admin"),
    "user": os.getenv("DB_USER", "aichatbot"),
    "password": os.getenv("DB_PASSWORD", "aichatbot_password")
}


class ExtractedKnowledgeImporter:
    """æå–çš„çŸ¥è­˜åº«åŒ¯å…¥å™¨"""

    def __init__(self):
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.conn = None
        self.stats = {
            "total_rows": 0,
            "imported": 0,
            "skipped_duplicate": 0,
            "skipped_invalid": 0,
            "errors": 0
        }

    async def connect_db(self):
        """é€£æ¥è³‡æ–™åº«"""
        try:
            self.conn = await asyncpg.connect(**DB_CONFIG)
            print("âœ… è³‡æ–™åº«å·²é€£æ¥")
        except Exception as e:
            print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
            raise

    async def close_db(self):
        """é—œé–‰è³‡æ–™åº«é€£æ¥"""
        if self.conn:
            await self.conn.close()
            print("ğŸ‘‹ è³‡æ–™åº«é€£æ¥å·²é—œé–‰")

    def load_excel(self, file_path: str) -> pd.DataFrame:
        """è¼‰å…¥æå–çš„ Excel æ–‡ä»¶"""
        print(f"\nğŸ“– è®€å– Excel æ–‡ä»¶: {file_path}")

        df = pd.read_excel(file_path, engine='openpyxl')

        print(f"   âœ… è®€å– {len(df)} è¡Œè³‡æ–™")
        print(f"   æ¬„ä½: {list(df.columns)}")

        return df

    def validate_row(self, row: pd.Series) -> bool:
        """é©—è­‰å–®è¡Œè³‡æ–™æ˜¯å¦æœ‰æ•ˆ"""
        # å¿…é ˆæœ‰ question_summary å’Œ answer
        if pd.isna(row.get('question_summary')) or not str(row.get('question_summary')).strip():
            return False

        if pd.isna(row.get('answer')) or not str(row.get('answer')).strip():
            return False

        # ç­”æ¡ˆé•·åº¦å¿…é ˆ >= 10
        answer = str(row.get('answer')).strip()
        if len(answer) < 10:
            return False

        return True

    async def check_duplicate(self, question_summary: str, answer: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºé‡è¤‡çŸ¥è­˜"""
        count = await self.conn.fetchval("""
            SELECT COUNT(*) FROM knowledge_base
            WHERE question_summary = $1 OR answer = $2
        """, question_summary, answer)

        return count > 0

    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆå‘é‡åµŒå…¥"""
        try:
            response = self.client.embeddings.create(
                model="text-embedding-3-small",
                input=text[:2000]  # é™åˆ¶é•·åº¦
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"   âš ï¸  å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
            return None

    async def import_to_database(
        self,
        df: pd.DataFrame,
        vendor_id: int = None,
        enable_deduplication: bool = True
    ):
        """
        åŒ¯å…¥çŸ¥è­˜åˆ°è³‡æ–™åº«

        Args:
            df: çŸ¥è­˜åº« DataFrame
            vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
            enable_deduplication: æ˜¯å¦å•Ÿç”¨å»é‡
        """
        print(f"\nğŸ’¾ é–‹å§‹åŒ¯å…¥åˆ°è³‡æ–™åº«...")
        print(f"   å»é‡: {'å•Ÿç”¨' if enable_deduplication else 'åœç”¨'}")
        print(f"   æ¥­è€… ID: {vendor_id if vendor_id else 'é€šç”¨çŸ¥è­˜'}")

        # æŸ¥è©¢é è¨­æ„åœ– IDï¼ˆå„ªå…ˆä½¿ç”¨ã€Œæœå‹™èªªæ˜ã€æˆ–ä»»ä½•æ„åœ–ï¼‰
        default_intent_id = await self.conn.fetchval(
            "SELECT id FROM intents WHERE name = 'æœå‹™èªªæ˜' OR name = 'ä¸€èˆ¬çŸ¥è­˜' OR name = 'å…¶ä»–' LIMIT 1"
        )

        if not default_intent_id:
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå°±ç”¨ç¬¬ä¸€å€‹æ„åœ–
            default_intent_id = await self.conn.fetchval(
                "SELECT id FROM intents ORDER BY id LIMIT 1"
            )

        if not default_intent_id:
            print("âš ï¸  æ‰¾ä¸åˆ°ä»»ä½•æ„åœ–ï¼Œè«‹å…ˆå»ºç«‹æ„åœ–")
            return

        print(f"   ä½¿ç”¨æ„åœ– ID: {default_intent_id}")

        self.stats['total_rows'] = len(df)

        for idx, row in df.iterrows():
            try:
                # é©—è­‰è³‡æ–™
                if not self.validate_row(row):
                    print(f"   [{idx+1}/{len(df)}] â© è·³éç„¡æ•ˆè³‡æ–™")
                    self.stats['skipped_invalid'] += 1
                    continue

                # æå–æ¬„ä½
                title = str(row.get('title', '')).strip()
                category = str(row.get('category', 'ä¸€èˆ¬å•é¡Œ')).strip()
                question_summary = str(row.get('question_summary')).strip()
                answer = str(row.get('answer')).strip()
                audience = str(row.get('audience', 'ç§Ÿå®¢')).strip()

                # è™•ç†é—œéµå­—ï¼ˆå¯èƒ½æ˜¯å­—ä¸²æˆ–åˆ—è¡¨ï¼‰
                keywords_val = row.get('keywords', [])
                if isinstance(keywords_val, str):
                    # å˜—è©¦è§£æå­—ä¸²æ ¼å¼çš„åˆ—è¡¨
                    import ast
                    try:
                        keywords = ast.literal_eval(keywords_val)
                    except:
                        keywords = [k.strip() for k in keywords_val.split(',') if k.strip()]
                elif isinstance(keywords_val, list):
                    keywords = keywords_val
                else:
                    keywords = []

                source_file = str(row.get('source_file', 'LINEæå–')).strip()

                # è™•ç†æ—¥æœŸæ ¼å¼
                source_date_str = str(row.get('source_date', datetime.now().strftime('%Y-%m-%d'))).strip()
                try:
                    source_date = datetime.strptime(source_date_str, '%Y-%m-%d').date()
                except:
                    source_date = datetime.now().date()

                # å»é‡æª¢æŸ¥
                if enable_deduplication:
                    is_duplicate = await self.check_duplicate(question_summary, answer)
                    if is_duplicate:
                        print(f"   [{idx+1}/{len(df)}] â© å·²å­˜åœ¨: {question_summary[:30]}...")
                        self.stats['skipped_duplicate'] += 1
                        continue

                # ç”Ÿæˆå‘é‡åµŒå…¥
                embedding_text = f"{title} {question_summary} {answer[:200]}"
                embedding = self.generate_embedding(embedding_text)

                if not embedding:
                    print(f"   [{idx+1}/{len(df)}] âŒ å‘é‡ç”Ÿæˆå¤±æ•—: {question_summary[:30]}...")
                    self.stats['errors'] += 1
                    continue

                # å°‡ embedding è½‰æ›ç‚º pgvector æ ¼å¼å­—ä¸²
                embedding_str = '[' + ','.join(map(str, embedding)) + ']'

                # æ’å…¥è³‡æ–™åº«
                await self.conn.execute("""
                    INSERT INTO knowledge_base (
                        intent_id,
                        vendor_id,
                        title,
                        category,
                        question_summary,
                        answer,
                        audience,
                        keywords,
                        source_file,
                        source_date,
                        embedding,
                        scope,
                        priority,
                        created_at,
                        updated_at
                    ) VALUES (
                        $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11::vector, $12, $13,
                        CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                    )
                """,
                    default_intent_id,
                    vendor_id,
                    title or question_summary,  # å¦‚æœæ²’æœ‰ title å°±ç”¨ question_summary
                    category,
                    question_summary,
                    answer,
                    audience,
                    keywords,
                    source_file,
                    source_date,
                    embedding_str,  # ä½¿ç”¨å­—ä¸²æ ¼å¼
                    'global' if not vendor_id else 'vendor',
                    0
                )

                print(f"   [{idx+1}/{len(df)}] âœ… å·²åŒ¯å…¥: {question_summary[:40]}...")
                self.stats['imported'] += 1

            except Exception as e:
                print(f"   [{idx+1}/{len(df)}] âŒ åŒ¯å…¥å¤±æ•—: {e}")
                self.stats['errors'] += 1

        print(f"\nâœ… åŒ¯å…¥å®Œæˆï¼")
        self.print_stats()

    def print_stats(self):
        """åˆ—å°çµ±è¨ˆè³‡è¨Š"""
        print(f"\n{'='*60}")
        print("åŒ¯å…¥çµ±è¨ˆ")
        print(f"{'='*60}")
        print(f"ç¸½è¡Œæ•¸ï¼š{self.stats['total_rows']}")
        print(f"æˆåŠŸåŒ¯å…¥ï¼š{self.stats['imported']}")
        print(f"è·³éï¼ˆé‡è¤‡ï¼‰ï¼š{self.stats['skipped_duplicate']}")
        print(f"è·³éï¼ˆç„¡æ•ˆï¼‰ï¼š{self.stats['skipped_invalid']}")
        print(f"éŒ¯èª¤ï¼š{self.stats['errors']}")
        print(f"{'='*60}\n")


async def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("LINE æå–çŸ¥è­˜åº«åŒ¯å…¥å·¥å…·")
    print("="*60)

    # æª¢æŸ¥ OpenAI API Key
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ éŒ¯èª¤ï¼šæœªè¨­å®š OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸")
        return

    # æ–‡ä»¶è·¯å¾‘
    excel_file = "/Users/lenny/jgb/AIChatbot/output/knowledge_base_extracted.xlsx"

    if not os.path.exists(excel_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {excel_file}")
        print(f"\nè«‹å…ˆåŸ·è¡Œæå–è…³æœ¬ï¼š")
        print(f"python3 scripts/knowledge_extraction/extract_knowledge_and_tests.py")
        return

    # å»ºç«‹åŒ¯å…¥å™¨
    importer = ExtractedKnowledgeImporter()

    try:
        # é€£æ¥è³‡æ–™åº«
        await importer.connect_db()

        # è¼‰å…¥ Excel
        df = importer.load_excel(excel_file)

        if df.empty:
            print("âš ï¸  Excel æ–‡ä»¶ç‚ºç©º")
            return

        # è©¢å•æ˜¯å¦ç¹¼çºŒ
        print(f"\nå³å°‡åŒ¯å…¥ {len(df)} å€‹çŸ¥è­˜é …ç›®")
        print("é€™å°‡æœƒå‘¼å« OpenAI API ç”Ÿæˆå‘é‡åµŒå…¥")
        print(f"é ä¼°æˆæœ¬: ${len(df) * 0.00002:.4f} USD")

        confirm = input("\nç¢ºèªç¹¼çºŒï¼Ÿ(y/N): ").strip().lower()
        if confirm != 'y':
            print("âŒ å·²å–æ¶ˆåŒ¯å…¥")
            return

        # è©¢å•æ¥­è€… ID
        vendor_id_str = input("\nè¼¸å…¥æ¥­è€… IDï¼ˆç•™ç©ºè¡¨ç¤ºé€šç”¨çŸ¥è­˜ï¼‰: ").strip()
        vendor_id = int(vendor_id_str) if vendor_id_str else None

        # è©¢å•æ˜¯å¦å•Ÿç”¨å»é‡
        enable_dedup = input("\nå•Ÿç”¨æ™ºèƒ½å»é‡ï¼Ÿ(Y/n): ").strip().lower()
        enable_deduplication = enable_dedup != 'n'

        # åŒ¯å…¥åˆ°è³‡æ–™åº«
        await importer.import_to_database(
            df,
            vendor_id=vendor_id,
            enable_deduplication=enable_deduplication
        )

        print("\nâœ… å…¨éƒ¨å®Œæˆï¼")
        print(f"\nä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œå›æ¸¬é©—è­‰")
        print(f"python3 scripts/knowledge_extraction/backtest_framework.py")

    except Exception as e:
        print(f"\nâŒ åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # é—œé–‰è³‡æ–™åº«é€£æ¥
        await importer.close_db()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
