#!/usr/bin/env python3
"""
å…¨é‡å›æ¸¬ - æ”¯æŒå¯¦æ™‚è³‡æ–™åº«é€²åº¦è¿½è¹¤

åŠŸèƒ½ï¼š
1. é–‹å§‹æ™‚å‰µå»º backtest_runs è¨˜éŒ„
2. åŸ·è¡Œéç¨‹ä¸­å¯¦æ™‚æ›´æ–°é€²åº¦åˆ°è³‡æ–™åº«
3. å®Œæˆæ™‚ä¿å­˜æ‰€æœ‰çµæœåˆ° backtest_results è¡¨
4. å‰ç«¯å¯ä»¥é€šé API æŸ¥çœ‹å¯¦æ™‚é€²åº¦
"""

import asyncio
import os
import sys
import datetime
import json
import psycopg2
from psycopg2.extras import execute_values

sys.path.insert(0, '/app/scripts/knowledge_extraction')
from backtest_framework_async import AsyncBacktestFramework


class ProgressTracker:
    """é€²åº¦è¿½è¹¤å™¨ - å¯¦æ™‚æ›´æ–°åˆ°è³‡æ–™åº«"""

    def __init__(self, run_id, total_tests):
        self.run_id = run_id
        self.total_tests = total_tests
        self.executed = 0
        self.last_update = datetime.datetime.now()

        # è³‡æ–™åº«é€£æ¥åƒæ•¸
        self.db_params = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'database': os.getenv('DB_NAME', 'aichatbot_admin'),
            'user': os.getenv('DB_USER', 'aichatbot'),
            'password': os.getenv('DB_PASSWORD', '')
        }

    def update_progress(self, executed_count, force=False):
        """æ›´æ–°é€²åº¦ï¼ˆæ¯5ç§’æˆ–å¼·åˆ¶æ›´æ–°ï¼‰"""
        self.executed = executed_count
        now = datetime.datetime.now()

        # æ¯5ç§’æˆ–å¼·åˆ¶æ›´æ–°ä¸€æ¬¡
        if force or (now - self.last_update).total_seconds() >= 5:
            try:
                conn = psycopg2.connect(**self.db_params)
                cur = conn.cursor()

                cur.execute("""
                    UPDATE backtest_runs
                    SET executed_scenarios = %s
                    WHERE id = %s
                """, (executed_count, self.run_id))

                conn.commit()
                cur.close()
                conn.close()

                self.last_update = now
                print(f"âœ“ é€²åº¦å·²æ›´æ–°åˆ°è³‡æ–™åº«: {executed_count}/{self.total_tests}")
            except Exception as e:
                print(f"âš ï¸  æ›´æ–°é€²åº¦å¤±æ•—: {e}")


async def main():
    print(f"\n{'='*60}")
    print(f"å…¨é‡å›æ¸¬é–‹å§‹ï¼ˆæ”¯æŒå¯¦æ™‚é€²åº¦è¿½è¹¤ï¼‰- {datetime.datetime.now()}")
    print(f"{'='*60}\n")

    # ç²å–é…ç½®
    concurrency = int(os.getenv('BACKTEST_CONCURRENCY', '5'))
    quality_mode = os.getenv('BACKTEST_QUALITY_MODE', 'detailed')
    base_url = os.getenv('RAG_API_URL', 'http://localhost:8100')
    vendor_id = int(os.getenv('VENDOR_ID', '1'))

    # è³‡æ–™åº«é€£æ¥åƒæ•¸
    db_params = {
        'host': os.getenv('DB_HOST', 'localhost'),
        'database': os.getenv('DB_NAME', 'aichatbot_admin'),
        'user': os.getenv('DB_USER', 'aichatbot'),
        'password': os.getenv('DB_PASSWORD', '')
    }

    # å‰µå»ºå›æ¸¬æ¡†æ¶
    backtest = AsyncBacktestFramework(
        base_url=base_url,
        vendor_id=vendor_id,
        quality_mode=quality_mode,
        use_database=True,
        concurrency=concurrency
    )

    # è¼‰å…¥æ¸¬è©¦
    print(f"\nğŸ“– è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ...")
    scenarios = backtest.load_test_scenarios(strategy='full')
    total_tests = len(scenarios)
    print(f'âœ… è¼‰å…¥ {total_tests} å€‹æ¸¬è©¦æƒ…å¢ƒ\n')

    # å‰µå»º backtest_runs è¨˜éŒ„
    conn = psycopg2.connect(**db_params)
    cur = conn.cursor()

    start_time = datetime.datetime.now()

    cur.execute("""
        INSERT INTO backtest_runs (
            quality_mode, test_type, total_scenarios, executed_scenarios,
            status, rag_api_url, vendor_id, started_at, created_at
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s
        ) RETURNING id
    """, (
        quality_mode, 'full', total_tests, 0,
        'running', base_url, vendor_id, start_time, start_time
    ))

    run_id = cur.fetchone()[0]
    conn.commit()
    print(f"âœ… å‰µå»ºå›æ¸¬è¨˜éŒ„ (Run ID: {run_id}, Status: running)\n")

    # å‰µå»ºé€²åº¦è¿½è¹¤å™¨
    tracker = ProgressTracker(run_id, total_tests)

    # åŸ·è¡Œå›æ¸¬
    print(f"ğŸ§ª é–‹å§‹åŸ·è¡Œå›æ¸¬ (ä¸¦ç™¼æ•¸: {concurrency})...")

    # ä¿®æ”¹å›æ¸¬æ¡†æ¶ä»¥æ”¯æŒé€²åº¦å›èª¿
    async def run_with_progress():
        results = []
        semaphore = asyncio.Semaphore(concurrency)

        import aiohttp
        connector = aiohttp.TCPConnector(limit=concurrency * 2)

        async with aiohttp.ClientSession(connector=connector) as session:
            async def bounded_test(scenario, index):
                async with semaphore:
                    result = await backtest._test_single_scenario_async(
                        scenario, index, session,
                        backtest.default_timeout,
                        backtest.default_retry_times,
                        0.2
                    )

                    # æ›´æ–°é€²åº¦
                    tracker.update_progress(len(results) + 1)

                    return result

            tasks = [
                bounded_test(scenario, i)
                for i, scenario in enumerate(scenarios, 1)
            ]

            # ä¸¦ç™¼åŸ·è¡Œï¼ˆç„¡é€²åº¦æ¢ï¼Œä½†æœ‰è³‡æ–™åº«è¿½è¹¤ï¼‰
            for coro in asyncio.as_completed(tasks):
                result = await coro
                if result:
                    results.append(result)

        return results

    results = await run_with_progress()

    # æ‰¹é‡ LLM è©•ä¼°
    if backtest.batch_llm_eval and quality_mode in ['detailed', 'hybrid']:
        print(f"\nğŸ“Š åŸ·è¡Œæ‰¹é‡ LLM è©•ä¼°...")
        results = await backtest._batch_llm_evaluation(results, backtest.llm_batch_size)

    end_time = datetime.datetime.now()
    duration = (end_time - start_time).total_seconds()

    # çµ±è¨ˆçµæœ
    passed_count = sum(1 for r in results if r.get('passed'))
    failed_count = sum(1 for r in results if not r.get('passed') and not r.get('error'))
    error_count = sum(1 for r in results if r.get('error'))
    pass_rate = (passed_count / len(results) * 100) if results else 0

    # è¨ˆç®—å¹³å‡å“è³ªæŒ‡æ¨™
    quality_results = [r for r in results if r.get('relevance') is not None]
    if quality_results:
        avg_relevance = sum(r.get('relevance', 0) for r in quality_results) / len(quality_results)
        avg_completeness = sum(r.get('completeness', 0) for r in quality_results) / len(quality_results)
        avg_accuracy = sum(r.get('accuracy', 0) for r in quality_results) / len(quality_results)
        avg_intent_match = sum(r.get('intent_match', 0) for r in quality_results) / len(quality_results)
        avg_quality_overall = sum(r.get('quality_overall', 0) for r in quality_results) / len(quality_results)
    else:
        avg_relevance = avg_completeness = avg_accuracy = avg_intent_match = avg_quality_overall = None

    # ç”Ÿæˆå ±å‘Šæ–‡ä»¶
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = f'/app/output/backtest/backtest_full_c{concurrency}_{timestamp}.xlsx'

    print(f"\nğŸ“Š ç”Ÿæˆ Excel å ±å‘Š...")
    backtest.generate_report(results, output_file)

    # ä¿å­˜çµæœåˆ°è³‡æ–™åº«
    print(f"ğŸ’¾ ä¿å­˜çµæœåˆ°è³‡æ–™åº«...")
    try:
        # æº–å‚™æ‰¹é‡æ’å…¥æ•¸æ“š
        result_values = []
        for r in results:
            result_values.append((
                run_id,
                r.get('scenario_id'),
                r.get('test_question'),
                r.get('system_answer'),
                r.get('actual_intent'),
                r.get('confidence', 0),
                r.get('score', 0),
                r.get('overall_score', 0),
                r.get('passed', False),
                r.get('evaluation', '{}'),
                r.get('optimization_tips', ''),
                r.get('knowledge_sources', ''),
                r.get('source_ids', ''),
                r.get('source_count', 0),
                r.get('difficulty', 'medium'),
                r.get('notes', ''),
                r.get('relevance'),
                r.get('completeness'),
                r.get('accuracy'),
                r.get('intent_match'),
                r.get('quality_overall'),
                r.get('quality_reasoning'),
                r.get('quality_eval'),
                datetime.datetime.now()
            ))

        # æ‰¹é‡æ’å…¥
        execute_values(cur, """
            INSERT INTO backtest_results (
                run_id, scenario_id, test_question, system_answer, actual_intent,
                confidence, score, overall_score, passed, evaluation,
                optimization_tips, knowledge_sources, source_ids, source_count,
                difficulty, notes, relevance, completeness, accuracy,
                intent_match, quality_overall, quality_reasoning, quality_eval, tested_at
            ) VALUES %s
        """, result_values)

        print(f"âœ… å·²ä¿å­˜ {len(results)} æ¢çµæœåˆ°è³‡æ–™åº«")
    except Exception as e:
        print(f"âŒ ä¿å­˜çµæœå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    # æ›´æ–° backtest_runs ç‚ºå®Œæˆç‹€æ…‹
    cur.execute("""
        UPDATE backtest_runs SET
            executed_scenarios = %s,
            status = %s,
            passed_count = %s,
            failed_count = %s,
            error_count = %s,
            pass_rate = %s,
            avg_relevance = %s,
            avg_completeness = %s,
            avg_accuracy = %s,
            avg_intent_match = %s,
            avg_quality_overall = %s,
            completed_at = %s,
            duration_seconds = %s,
            output_file_path = %s
        WHERE id = %s
    """, (
        len(results), 'completed',
        passed_count, failed_count, error_count, pass_rate,
        avg_relevance, avg_completeness, avg_accuracy, avg_intent_match, avg_quality_overall,
        end_time, int(duration), output_file, run_id
    ))

    conn.commit()
    cur.close()
    conn.close()

    # è¼¸å‡ºç¸½çµ
    print(f"\n{'='*60}")
    print(f"å›æ¸¬å®Œæˆç¸½çµ")
    print(f"{'='*60}")
    print(f"Run ID: {run_id}")
    print(f"é–‹å§‹æ™‚é–“: {start_time}")
    print(f"çµæŸæ™‚é–“: {end_time}")
    print(f"ç¸½è€—æ™‚: {duration:.2f} ç§’ ({duration/60:.2f} åˆ†é˜)")
    print(f"æ¸¬è©¦ç¸½æ•¸: {len(results)}")
    print(f"é€šéæ•¸é‡: {passed_count} ({pass_rate:.1f}%)")
    print(f"å¤±æ•—æ•¸é‡: {failed_count}")
    print(f"éŒ¯èª¤æ•¸é‡: {error_count}")
    print(f"ååé‡: {len(results)/duration:.2f} æ¸¬è©¦/ç§’")
    print(f"å ±å‘Šä½ç½®: {output_file}")
    print(f"è³‡æ–™åº«è¨˜éŒ„: Run ID {run_id}")
    print(f"\nå‰ç«¯æŸ¥çœ‹: http://localhost:8087/backtest")
    print(f"{'='*60}\n")

    # ä¿å­˜æ‘˜è¦ JSON
    summary = {
        'run_id': run_id,
        'start_time': start_time.isoformat(),
        'end_time': end_time.isoformat(),
        'duration_seconds': duration,
        'duration_minutes': duration/60,
        'total_tests': len(results),
        'passed': passed_count,
        'failed': failed_count,
        'errors': error_count,
        'pass_rate': pass_rate,
        'throughput': len(results)/duration,
        'report_file': output_file,
        'concurrency': concurrency,
        'quality_mode': quality_mode,
        'avg_relevance': avg_relevance,
        'avg_completeness': avg_completeness,
        'avg_accuracy': avg_accuracy,
        'avg_intent_match': avg_intent_match,
        'avg_quality_overall': avg_quality_overall
    }

    summary_file = f'/app/output/backtest/backtest_summary_{timestamp}.json'
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print(f"âœ… æ‘˜è¦å·²ä¿å­˜: {summary_file}")
    return 0


if __name__ == '__main__':
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
