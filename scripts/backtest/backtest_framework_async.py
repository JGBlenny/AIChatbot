"""
ä¸¦ç™¼å›æ¸¬æ¡†æ¶ V2 (Async Backtest Framework)
ç›¸æ¯” V1 æä¾› 5-10x æ€§èƒ½æå‡

ä¸»è¦æ”¹é€²:
- ä¸¦ç™¼åŸ·è¡Œæ¸¬è©¦ (concurrency å¯é…ç½®)
- ç•°æ­¥ HTTP è«‹æ±‚ (aiohttp)
- æ™ºèƒ½é‡è©¦æ©Ÿåˆ¶ (tenacity)
- å¯¦æ™‚é€²åº¦é¡¯ç¤º (tqdm)
- æ‰¹é‡ LLM è©•ä¼°
"""

import os
import sys
import time
import math
import asyncio
import aiohttp
from typing import List, Dict, Optional
from datetime import datetime
import pandas as pd
import json
from openai import OpenAI
import psycopg2
from psycopg2.extras import RealDictCursor
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# æ·»åŠ çˆ¶ç›®éŒ„åˆ°è·¯å¾‘ä»¥ä¾¿å°å…¥ BacktestFramework
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from backtest_framework import BacktestFramework


class AsyncBacktestFramework(BacktestFramework):
    """
    ç•°æ­¥ä¸¦ç™¼å›æ¸¬æ¡†æ¶

    ç¹¼æ‰¿è‡ª BacktestFrameworkï¼Œæ·»åŠ ä¸¦ç™¼åŸ·è¡Œèƒ½åŠ›
    """

    def __init__(
        self,
        base_url: str = "http://localhost:8100",
        vendor_id: int = 1,
        quality_mode: str = "detailed",
        use_database: bool = True,
        # V2 æ–°å¢åƒæ•¸
        concurrency: int = None,
        default_timeout: int = None,
        default_retry_times: int = None,
        enable_metrics: bool = True
    ):
        # èª¿ç”¨çˆ¶é¡åˆå§‹åŒ–
        super().__init__(base_url, vendor_id, quality_mode, use_database)

        # V2 ä¸¦ç™¼é…ç½®
        self.concurrency = concurrency or int(os.getenv('BACKTEST_CONCURRENCY', '5'))
        self.default_timeout = default_timeout or int(os.getenv('BACKTEST_TIMEOUT', '60'))
        self.default_retry_times = default_retry_times or int(os.getenv('BACKTEST_RETRY_TIMES', '2'))

        # æ‰¹é‡ LLM è©•ä¼°é…ç½®
        self.batch_llm_eval = os.getenv('BACKTEST_BATCH_LLM_EVAL', 'true').lower() == 'true'
        self.llm_batch_size = int(os.getenv('BACKTEST_LLM_BATCH_SIZE', '10'))

        # æ€§èƒ½ç›£æ§
        self.enable_metrics = enable_metrics
        self.metrics = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'timeout_tests': 0,
            'retry_count': 0,
            'total_duration': 0,
            'avg_test_duration': 0,
            'throughput': 0  # tests/second
        }

        # æ…¢æŸ¥è©¢è¿½è¹¤
        self.slow_query_threshold = int(os.getenv('BACKTEST_SLOW_QUERY_THRESHOLD', '10'))
        self.slow_queries = []

        print(f"âœ… ä¸¦ç™¼å›æ¸¬æ¡†æ¶ V2 åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä¸¦ç™¼æ•¸: {self.concurrency}")
        print(f"   è¶…æ™‚æ™‚é–“: {self.default_timeout}s")
        print(f"   é‡è©¦æ¬¡æ•¸: {self.default_retry_times}")
        if self.batch_llm_eval:
            print(f"   æ‰¹é‡ LLM è©•ä¼°: å•Ÿç”¨ (batch_size={self.llm_batch_size})")

    async def _query_rag_async(
        self,
        question: str,
        timeout: int = None,
        session: aiohttp.ClientSession = None
    ) -> Dict:
        """
        ç•°æ­¥æŸ¥è©¢ RAG ç³»çµ±

        Args:
            question: æ¸¬è©¦å•é¡Œ
            timeout: è¶…æ™‚æ™‚é–“ (ç§’)
            session: aiohttp session (å¾©ç”¨é€£æ¥)

        Returns:
            ç³»çµ±å›æ‡‰å­—å…¸
        """
        url = f"{self.base_url}/api/v1/message"

        payload = {
            "message": question,
            "vendor_id": self.vendor_id,
            "mode": "tenant",
            "include_sources": True,
            "skip_sop": True
        }

        # å›æ¸¬å°ˆç”¨é…ç½®
        disable_synthesis = os.getenv("BACKTEST_DISABLE_ANSWER_SYNTHESIS", "false").lower() == "true"
        if disable_synthesis:
            payload["disable_answer_synthesis"] = True

        timeout_val = timeout or self.default_timeout
        timeout_obj = aiohttp.ClientTimeout(total=timeout_val)

        try:
            # ä½¿ç”¨æä¾›çš„ session æˆ–å‰µå»ºæ–°çš„
            if session:
                async with session.post(url, json=payload, timeout=timeout_obj) as response:
                    response.raise_for_status()
                    return await response.json()
            else:
                async with aiohttp.ClientSession() as new_session:
                    async with new_session.post(url, json=payload, timeout=timeout_obj) as response:
                        response.raise_for_status()
                        return await response.json()

        except asyncio.TimeoutError:
            if self.enable_metrics:
                self.metrics['timeout_tests'] += 1
            raise
        except aiohttp.ClientError as e:
            print(f"   âŒ HTTP è«‹æ±‚éŒ¯èª¤: {e}")
            return None
        except Exception as e:
            print(f"   âŒ æœªé æœŸéŒ¯èª¤: {e}")
            return None

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((asyncio.TimeoutError, aiohttp.ClientError))
    )
    async def _query_rag_with_retry(
        self,
        question: str,
        timeout: int = None,
        session: aiohttp.ClientSession = None
    ) -> Dict:
        """
        å¸¶é‡è©¦çš„ RAG æŸ¥è©¢

        ä½¿ç”¨ tenacity è‡ªå‹•é‡è©¦è¶…æ™‚å’Œé€£æ¥éŒ¯èª¤
        """
        if self.enable_metrics:
            self.metrics['retry_count'] += 1

        return await self._query_rag_async(question, timeout, session)

    async def _llm_evaluate_async(
        self,
        question: str,
        answer: str
    ) -> Dict:
        """
        ç•°æ­¥ LLM è©•ä¼° (å–®å€‹)

        æ³¨æ„: OpenAI Python SDK ç›®å‰ä¸æ”¯æŒåŸç”Ÿç•°æ­¥
        é€™è£¡ä½¿ç”¨ run_in_executor åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œ
        """
        loop = asyncio.get_event_loop()

        def sync_llm_eval():
            return self.llm_evaluate_answer(question, answer)

        result = await loop.run_in_executor(None, sync_llm_eval)
        return result

    async def _llm_evaluate_batch_async(
        self,
        qa_pairs: List[Dict[str, str]]
    ) -> List[Dict]:
        """
        æ‰¹é‡ LLM è©•ä¼°

        Args:
            qa_pairs: [{"question": "...", "answer": "..."}, ...]

        Returns:
            è©•ä¼°çµæœåˆ—è¡¨
        """
        if not qa_pairs:
            return []

        # æ§‹å»ºæ‰¹é‡æç¤º
        batch_prompt = "è«‹è©•ä¼°ä»¥ä¸‹å•ç­”å°çš„å“è³ªï¼ˆ1-5åˆ†ï¼Œ5åˆ†æœ€ä½³ï¼‰ï¼š\n\n"

        for i, pair in enumerate(qa_pairs, 1):
            batch_prompt += f"ã€å•ç­” {i}ã€‘\n"
            batch_prompt += f"å•é¡Œï¼š{pair['question']}\n"
            batch_prompt += f"ç­”æ¡ˆï¼š{pair['answer']}\n\n"

        batch_prompt += """
è«‹å°æ¯å€‹å•ç­”å°å¾ä»¥ä¸‹ç¶­åº¦è©•åˆ†ï¼š
1. ç›¸é—œæ€§ (Relevance): ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”å•é¡Œï¼Ÿ
2. å®Œæ•´æ€§ (Completeness): ç­”æ¡ˆæ˜¯å¦å®Œæ•´æ¶µè“‹å•é¡Œæ‰€å•ï¼Ÿ
3. æº–ç¢ºæ€§ (Accuracy): ç­”æ¡ˆå…§å®¹æ˜¯å¦æº–ç¢ºå¯é ï¼Ÿ
4. æ„åœ–ç†è§£ (Intent Match): ç­”æ¡ˆæ˜¯å¦æ­£ç¢ºç†è§£å•é¡Œæ„åœ–ä¸¦å›æ‡‰ï¼Ÿ

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼ˆä½¿ç”¨ evaluations é™£åˆ—ï¼‰ï¼š
{
    "evaluations": [
        {
            "index": 1,
            "relevance": <1-5>,
            "completeness": <1-5>,
            "accuracy": <1-5>,
            "intent_match": <1-5>,
            "overall": <1-5>,
            "reasoning": "ç°¡çŸ­èªªæ˜è©•åˆ†ç†ç”±"
        },
        ...
    ]
}
"""

        loop = asyncio.get_event_loop()

        def sync_batch_eval():
            try:
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": batch_prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.3
                )

                result = json.loads(response.choices[0].message.content)
                return result.get('evaluations', [])

            except Exception as e:
                print(f"âš ï¸  æ‰¹é‡ LLM è©•ä¼°å¤±æ•—: {e}")
                # é™ç´šç‚ºé€å€‹è©•ä¼°
                return []

        results = await loop.run_in_executor(None, sync_batch_eval)

        # å¦‚æœæ‰¹é‡å¤±æ•—ï¼Œé™ç´šç‚ºé€å€‹è©•ä¼°
        if not results:
            print(f"   âš ï¸  æ‰¹é‡è©•ä¼°å¤±æ•—ï¼Œé™ç´šç‚ºé€å€‹è©•ä¼°")
            tasks = [
                self._llm_evaluate_async(pair['question'], pair['answer'])
                for pair in qa_pairs
            ]
            results = await asyncio.gather(*tasks)

        return results

    async def _test_single_scenario_async(
        self,
        scenario: Dict,
        index: int,
        session: aiohttp.ClientSession,
        timeout: int,
        retry_times: int,
        delay: float
    ) -> Dict:
        """
        ç•°æ­¥æ¸¬è©¦å–®å€‹æƒ…å¢ƒ

        Args:
            scenario: æ¸¬è©¦æƒ…å¢ƒ
            index: æ¸¬è©¦ç·¨è™Ÿ
            session: aiohttp session
            timeout: è¶…æ™‚æ™‚é–“
            retry_times: é‡è©¦æ¬¡æ•¸
            delay: è«‹æ±‚å»¶é²

        Returns:
            æ¸¬è©¦çµæœå­—å…¸
        """
        question = scenario.get('test_question', '')
        if not question:
            return None

        start_time = time.time()

        try:
            # æŸ¥è©¢ RAG ç³»çµ± (å¸¶é‡è©¦)
            system_response = None
            for attempt in range(retry_times + 1):
                try:
                    system_response = await self._query_rag_async(
                        question, timeout, session
                    )
                    break
                except (asyncio.TimeoutError, aiohttp.ClientError) as e:
                    if attempt < retry_times:
                        wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿
                        await asyncio.sleep(wait_time)
                    else:
                        raise

            # è©•ä¼°ç­”æ¡ˆ (ç¨å¾Œæ‰¹é‡è™•ç† LLM è©•ä¼°)
            evaluation_result = self.evaluate_answer(scenario, system_response)

            # æš«æ™‚åªè¿”å›åŸºç¤è©•ä¼°ï¼ŒLLM è©•ä¼°ç¨å¾Œæ‰¹é‡è™•ç†
            result = self._build_result_dict(
                scenario, system_response, evaluation_result, index
            )

            # å»¶é² (é¿å… rate limit)
            if delay > 0:
                await asyncio.sleep(delay)

            # è¨˜éŒ„åŸ·è¡Œæ™‚é–“
            duration = time.time() - start_time
            result['test_duration'] = duration

            # è¨˜éŒ„æ…¢æŸ¥è©¢
            if duration > self.slow_query_threshold:
                self.slow_queries.append({
                    'question': question,
                    'duration': duration,
                    'index': index
                })

            return result

        except asyncio.TimeoutError:
            duration = time.time() - start_time
            return {
                'test_id': index,
                'scenario_id': scenario.get('id'),
                'test_question': question,
                'error': 'timeout',
                'test_duration': duration,
                'passed': False,
                'score': 0.0,
                'confidence': 0.0
            }

        except Exception as e:
            duration = time.time() - start_time
            return {
                'test_id': index,
                'scenario_id': scenario.get('id'),
                'test_question': question,
                'error': str(e),
                'test_duration': duration,
                'passed': False,
                'score': 0.0,
                'confidence': 0.0
            }

    def _build_result_dict(
        self,
        scenario: Dict,
        system_response: Dict,
        evaluation: Dict,
        index: int
    ) -> Dict:
        """
        æ§‹å»ºçµæœå­—å…¸ (èˆ‡ V1 å…¼å®¹)
        """
        question = scenario.get('test_question', '')

        # æå–çŸ¥è­˜ä¾†æºè³‡è¨Š
        sources = system_response.get('sources', []) if system_response else []
        if sources is None:
            sources = []

        source_ids = [s.get('id') for s in sources if s.get('id')]
        source_summary = '; '.join([
            f"[{s.get('id', 'N/A')}] {s.get('question_summary', 'N/A')[:40]}"
            for s in sources[:3]
        ]) if sources else 'ç„¡ä¾†æº'

        # ç”ŸæˆçŸ¥è­˜åº«éˆæ¥
        knowledge_urls = []
        if source_ids:
            for kb_id in source_ids[:3]:
                knowledge_urls.append(f"http://localhost:8080/#/knowledge?search={kb_id}")
            ids_param = ','.join(map(str, source_ids))
            batch_url = f"http://localhost:8080/#/knowledge?ids={ids_param}"
        else:
            batch_url = "http://localhost:8080/#/knowledge"

        knowledge_links = '\n'.join(knowledge_urls) if knowledge_urls else 'ç„¡'

        # æ§‹å»ºçµæœ
        result = {
            'test_id': index,
            'scenario_id': scenario.get('id'),
            'test_question': question,
            'actual_intent': system_response.get('intent_name', '') if system_response else '',
            'all_intents': system_response.get('all_intents', []) if system_response else [],
            'system_answer': system_response.get('answer', '')[:200] if system_response else '',
            'confidence': system_response.get('confidence', 0) if system_response else 0,
            'score': evaluation['score'],
            'overall_score': evaluation['score'],
            'passed': evaluation['passed'],
            'evaluation': json.dumps(evaluation.get('checks', {}), ensure_ascii=False),
            'optimization_tips': '\n'.join(evaluation.get('optimization_tips', [])) if isinstance(evaluation.get('optimization_tips'), list) else evaluation.get('optimization_tips', ''),
            'knowledge_sources': source_summary,
            'source_ids': ','.join(map(str, source_ids)),
            'source_count': len(sources),
            'knowledge_links': knowledge_links,
            'batch_url': batch_url,
            'difficulty': scenario.get('difficulty', 'medium'),
            'notes': scenario.get('notes', ''),
            'timestamp': datetime.now().isoformat()
        }

        return result

    async def run_backtest_concurrent(
        self,
        test_scenarios: List[Dict],
        concurrency: int = None,
        timeout: int = None,
        retry_times: int = None,
        delay: float = None,
        sample_size: int = None,
        batch_llm_eval: bool = None,
        batch_size: int = None,
        show_progress: bool = None
    ) -> List[Dict]:
        """
        ä¸¦ç™¼åŸ·è¡Œå›æ¸¬

        Args:
            test_scenarios: æ¸¬è©¦æƒ…å¢ƒåˆ—è¡¨
            concurrency: ä¸¦ç™¼æ•¸ (é»˜èªä½¿ç”¨é…ç½®)
            timeout: è¶…æ™‚æ™‚é–“ (é»˜èªä½¿ç”¨é…ç½®)
            retry_times: é‡è©¦æ¬¡æ•¸ (é»˜èªä½¿ç”¨é…ç½®)
            delay: è«‹æ±‚å»¶é² (é»˜èª 0.2 ç§’)
            sample_size: æŠ½æ¨£æ•¸é‡
            batch_llm_eval: æ˜¯å¦æ‰¹é‡ LLM è©•ä¼°
            batch_size: LLM æ‰¹é‡å¤§å°
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦æ¢

        Returns:
            æ¸¬è©¦çµæœåˆ—è¡¨
        """
        # ä½¿ç”¨é»˜èªå€¼
        concurrency = concurrency or self.concurrency
        timeout = timeout or self.default_timeout
        retry_times = retry_times or self.default_retry_times
        delay = delay if delay is not None else float(os.getenv('BACKTEST_DELAY', '0.2'))
        batch_llm_eval = batch_llm_eval if batch_llm_eval is not None else self.batch_llm_eval
        batch_size = batch_size or self.llm_batch_size
        show_progress = show_progress if show_progress is not None else os.getenv('BACKTEST_SHOW_PROGRESS', 'true').lower() == 'true'

        print(f"\nğŸ§ª é–‹å§‹ä¸¦ç™¼å›æ¸¬...")
        print(f"   æ¸¬è©¦æƒ…å¢ƒæ•¸ï¼š{len(test_scenarios)}")
        print(f"   ä¸¦ç™¼æ•¸ï¼š{concurrency}")
        print(f"   è¶…æ™‚æ™‚é–“ï¼š{timeout}s")
        print(f"   é‡è©¦æ¬¡æ•¸ï¼š{retry_times}")

        if sample_size:
            print(f"   æŠ½æ¨£æ¸¬è©¦ï¼š{sample_size} å€‹")
            test_scenarios = test_scenarios[:sample_size]

        # é‡ç½®æŒ‡æ¨™
        if self.enable_metrics:
            self.metrics['total_tests'] = len(test_scenarios)
            self.slow_queries = []

        start_time = time.time()
        results = []

        # å‰µå»ºä¿¡è™Ÿé‡æ§åˆ¶ä¸¦ç™¼
        semaphore = asyncio.Semaphore(concurrency)

        # å‰µå»º aiohttp session (å¾©ç”¨é€£æ¥)
        connector = aiohttp.TCPConnector(limit=concurrency * 2)
        async with aiohttp.ClientSession(connector=connector) as session:

            async def bounded_test(scenario: Dict, index: int):
                """å¸¶ä¿¡è™Ÿé‡é™åˆ¶çš„æ¸¬è©¦"""
                async with semaphore:
                    return await self._test_single_scenario_async(
                        scenario, index, session, timeout, retry_times, delay
                    )

            # å‰µå»ºæ‰€æœ‰ä»»å‹™
            tasks = [
                bounded_test(scenario, i)
                for i, scenario in enumerate(test_scenarios, 1)
            ]

            # ä¸¦ç™¼åŸ·è¡Œ (å¸¶é€²åº¦æ¢)
            if show_progress:
                # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
                pbar = tqdm(total=len(tasks), desc="åŸ·è¡Œå›æ¸¬", unit="æ¸¬è©¦")

                for coro in asyncio.as_completed(tasks):
                    result = await coro
                    if result:
                        results.append(result)

                        # æ›´æ–°æŒ‡æ¨™
                        if self.enable_metrics:
                            if result.get('passed'):
                                self.metrics['passed_tests'] += 1
                            else:
                                self.metrics['failed_tests'] += 1

                        # æ›´æ–°é€²åº¦æ¢
                        passed = sum(1 for r in results if r.get('passed'))
                        pass_rate = (passed / len(results) * 100) if results else 0
                        pbar.set_postfix({
                            'é€šéç‡': f"{pass_rate:.1f}%",
                            'å¹³å‡æ™‚é•·': f"{result.get('test_duration', 0):.1f}s"
                        })

                    pbar.update(1)

                pbar.close()
            else:
                # ç„¡é€²åº¦æ¢æ¨¡å¼
                results = await asyncio.gather(*tasks)
                results = [r for r in results if r is not None]

        # æ‰¹é‡ LLM è©•ä¼°
        if batch_llm_eval and self.quality_mode in ['detailed', 'hybrid']:
            print(f"\nğŸ“Š åŸ·è¡Œæ‰¹é‡ LLM è©•ä¼°...")
            results = await self._batch_llm_evaluation(results, batch_size)

            # é‡æ–°è¨ˆç®—é€šé/å¤±æ•—æ•¸ (LLM è©•ä¼°å¯èƒ½æ”¹è®Š passed ç‹€æ…‹)
            if self.enable_metrics:
                self.metrics['passed_tests'] = sum(1 for r in results if r.get('passed'))
                self.metrics['failed_tests'] = sum(1 for r in results if not r.get('passed') and not r.get('error'))

        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        total_duration = time.time() - start_time
        if self.enable_metrics:
            self.metrics['total_duration'] = total_duration
            self.metrics['avg_test_duration'] = total_duration / len(results) if results else 0
            self.metrics['throughput'] = len(results) / total_duration if total_duration > 0 else 0

            self._print_metrics()

        return results

    async def _batch_llm_evaluation(
        self,
        results: List[Dict],
        batch_size: int
    ) -> List[Dict]:
        """
        æ‰¹é‡åŸ·è¡Œ LLM è©•ä¼°

        Args:
            results: æ¸¬è©¦çµæœåˆ—è¡¨ (å·²åŒ…å«åŸºç¤è©•ä¼°)
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            æ›´æ–°å¾Œçš„çµæœåˆ—è¡¨
        """
        # ç¯©é¸éœ€è¦è©•ä¼°çš„çµæœ (æœ‰ç­”æ¡ˆçš„)
        to_evaluate = [
            (i, r) for i, r in enumerate(results)
            if r.get('system_answer') and not r.get('error')
        ]

        if not to_evaluate:
            return results

        print(f"   éœ€è©•ä¼°: {len(to_evaluate)} å€‹æ¸¬è©¦")

        # åˆ†æ‰¹è™•ç†
        for batch_start in range(0, len(to_evaluate), batch_size):
            batch_end = min(batch_start + batch_size, len(to_evaluate))
            batch = to_evaluate[batch_start:batch_end]

            # æ§‹å»ºæ‰¹é‡å•ç­”å°
            qa_pairs = [
                {
                    'question': r.get('test_question', ''),
                    'answer': r.get('system_answer', '')
                }
                for _, r in batch
            ]

            # æ‰¹é‡è©•ä¼°
            evaluations = await self._llm_evaluate_batch_async(qa_pairs)

            # æ›´æ–°çµæœ
            for (idx, result), evaluation in zip(batch, evaluations):
                if evaluation and isinstance(evaluation, dict):
                    # æ·»åŠ  LLM è©•ä¼°çµæœ
                    results[idx]['quality_eval'] = json.dumps(evaluation, ensure_ascii=False)
                    results[idx]['relevance'] = evaluation.get('relevance', 0)
                    results[idx]['completeness'] = evaluation.get('completeness', 0)
                    results[idx]['accuracy'] = evaluation.get('accuracy', 0)
                    results[idx]['intent_match'] = evaluation.get('intent_match', 0)
                    results[idx]['quality_overall'] = evaluation.get('overall', 0)
                    results[idx]['quality_reasoning'] = evaluation.get('reasoning', '')

                    # é‡æ–°è¨ˆç®—æ··åˆè©•åˆ†
                    basic_eval = {'score': results[idx]['score'], 'passed': results[idx]['passed']}
                    overall_score = self._calculate_hybrid_score(basic_eval, evaluation)
                    passed = self._determine_pass_status(basic_eval, evaluation, overall_score)

                    results[idx]['overall_score'] = overall_score
                    results[idx]['passed'] = passed

            print(f"   å·²è©•ä¼°: {batch_end}/{len(to_evaluate)}")

        return results

    def _print_metrics(self):
        """æ‰“å°æ€§èƒ½æŒ‡æ¨™"""
        print(f"\n{'='*60}")
        print("æ€§èƒ½æŒ‡æ¨™")
        print(f"{'='*60}")
        print(f"ç¸½æ¸¬è©¦æ•¸ï¼š{self.metrics['total_tests']}")
        print(f"é€šéæ¸¬è©¦ï¼š{self.metrics['passed_tests']}")
        print(f"å¤±æ•—æ¸¬è©¦ï¼š{self.metrics['failed_tests']}")
        print(f"è¶…æ™‚æ¸¬è©¦ï¼š{self.metrics['timeout_tests']}")
        print(f"ç¸½è€—æ™‚ï¼š{self.metrics['total_duration']:.2f} ç§’")
        print(f"å¹³å‡æ¯å€‹æ¸¬è©¦ï¼š{self.metrics['avg_test_duration']:.2f} ç§’")
        print(f"ååé‡ï¼š{self.metrics['throughput']:.2f} æ¸¬è©¦/ç§’")

        if self.slow_queries:
            print(f"\næ…¢æŸ¥è©¢ (>{self.slow_query_threshold}s):")
            for sq in self.slow_queries[:5]:  # åªé¡¯ç¤ºå‰ 5 å€‹
                print(f"  [{sq['index']}] {sq['question'][:50]}... ({sq['duration']:.2f}s)")

        print(f"{'='*60}\n")


async def main():
    """ä¸»ç¨‹å¼"""
    print("="*60)
    print("ä¸¦ç™¼å›æ¸¬æ¡†æ¶ V2")
    print("="*60)

    # é…ç½®
    base_url = os.getenv("RAG_API_URL", "http://localhost:8100")
    vendor_id = int(os.getenv("VENDOR_ID", "1"))
    quality_mode = os.getenv("BACKTEST_QUALITY_MODE", "detailed")

    # å‰µå»ºå›æ¸¬æ¡†æ¶
    backtest = AsyncBacktestFramework(
        base_url=base_url,
        vendor_id=vendor_id,
        quality_mode=quality_mode,
        use_database=True
    )

    # è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒ
    selection_strategy = os.getenv("BACKTEST_SELECTION_STRATEGY", "incremental")
    limit = os.getenv("BACKTEST_LIMIT")
    limit = int(limit) if limit else None

    print(f"\nğŸ¯ æ¸¬è©¦é¸æ“‡ç­–ç•¥: {selection_strategy}")

    try:
        scenarios = backtest.load_test_scenarios(
            strategy=selection_strategy,
            limit=limit
        )
    except Exception as e:
        print(f"âŒ å¾è³‡æ–™åº«è¼‰å…¥æ¸¬è©¦æƒ…å¢ƒå¤±æ•—: {e}")
        print("ğŸ’¡ æç¤ºï¼šè«‹ç¢ºèªè³‡æ–™åº«é€£ç·šæ­£å¸¸")
        return

    # åŸ·è¡Œå›æ¸¬
    non_interactive = os.getenv("BACKTEST_NON_INTERACTIVE", "false").lower() == "true"

    if non_interactive:
        sample_size_str = os.getenv("BACKTEST_SAMPLE_SIZE", "")
        sample_size = int(sample_size_str) if sample_size_str else None
        if sample_size:
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œ {sample_size} å€‹æ¸¬è©¦")
        else:
            print(f"\nğŸ§ª éäº¤äº’æ¨¡å¼ï¼šåŸ·è¡Œå…¨éƒ¨ {len(scenarios)} å€‹æ¸¬è©¦")
    else:
        print(f"\næ˜¯å¦è¦åŸ·è¡Œå®Œæ•´å›æ¸¬ï¼Ÿ")
        print(f"ç¸½å…± {len(scenarios)} å€‹æ¸¬è©¦æƒ…å¢ƒ")
        sample_size = input("è¼¸å…¥è¦æ¸¬è©¦çš„æ•¸é‡ï¼ˆç›´æ¥æŒ‰ Enter æ¸¬è©¦å…¨éƒ¨ï¼‰: ").strip()
        sample_size = int(sample_size) if sample_size else None

    # ä¸¦ç™¼åŸ·è¡Œå›æ¸¬
    results = await backtest.run_backtest_concurrent(
        test_scenarios=scenarios,
        sample_size=sample_size
    )

    # ç”Ÿæˆå ±å‘Š
    project_root = os.getenv("PROJECT_ROOT", os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))
    output_dir = os.getenv("BACKTEST_OUTPUT_DIR", os.path.join(project_root, "output/backtest"))
    output_prefix = os.getenv("BACKTEST_OUTPUT_PREFIX", "backtest_v2")

    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{output_prefix}_results.xlsx")

    summary_data = backtest.generate_report(results, output_path)

    # å„²å­˜åˆ°è³‡æ–™åº«
    backtest.save_results_to_database(results, summary_data, output_path)

    print("âœ… å›æ¸¬å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
