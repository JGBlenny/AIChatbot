"""
çŸ¥è­˜åº«åŒ¯å…¥ API
æ”¯æ´ä¸Šå‚³å¤šç¨®æ ¼å¼çš„æª”æ¡ˆï¼ˆExcel, TXT, JSONï¼‰ï¼Œè‡ªå‹•æå–çŸ¥è­˜åº«ä¸¦å»é‡
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks, Request, Form
from pydantic import BaseModel
from typing import List, Dict, Optional
import tempfile
import os
import uuid
from datetime import datetime
from pathlib import Path

router = APIRouter(prefix="/api/v1/knowledge-import", tags=["Knowledge Import"])


class ImportJobStatus(BaseModel):
    """åŒ¯å…¥ä»»å‹™ç‹€æ…‹"""
    job_id: str
    status: str  # pending, processing, completed, failed
    progress: Optional[Dict] = None  # {current: 50, total: 100, stage: "ç”Ÿæˆå‘é‡"}
    result: Optional[Dict] = None  # {imported: 40, skipped: 5, errors: 0}
    error: Optional[str] = None
    file_name: Optional[str] = None
    vendor_id: Optional[int] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


class ImportOptions(BaseModel):
    """åŒ¯å…¥é¸é …"""
    mode: str = "append"  # append, replace, merge
    enable_deduplication: bool = True
    vendor_id: Optional[int] = None


@router.post("/upload")
async def upload_knowledge_file(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    vendor_id: Optional[int] = Form(None),
    import_mode: str = Form("append"),
    enable_deduplication: bool = Form(True)
):
    """
    ä¸Šå‚³çŸ¥è­˜æª”æ¡ˆä¸¦é–‹å§‹åŒ¯å…¥

    âš ï¸ é‡è¦ï¼šæ‰€æœ‰åŒ¯å…¥çš„çŸ¥è­˜éƒ½æœƒå…ˆé€²å…¥å¯©æ ¸ä½‡åˆ—ï¼Œ
    éœ€ç¶“éäººå·¥å¯©æ ¸é€šéå¾Œæ‰æœƒåŠ å…¥æ­£å¼çŸ¥è­˜åº«ã€‚

    æ”¯æ´æ ¼å¼ï¼š
    - Excel (.xlsx, .xls)
    - ç´”æ–‡å­— (.txt)
    - JSON (.json)

    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œç•™ç©ºè¡¨ç¤ºé€šç”¨çŸ¥è­˜ï¼‰
        import_mode: åŒ¯å…¥æ¨¡å¼ï¼ˆappend=è¿½åŠ , replace=æ›¿æ›, merge=åˆä½µï¼‰
        enable_deduplication: æ˜¯å¦å•Ÿç”¨å»é‡

    Returns:
        Dict: åŒ…å« job_id çš„å›æ‡‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¤ æ”¶åˆ°æª”æ¡ˆä¸Šå‚³è«‹æ±‚")
    print(f"   æª”æ¡ˆåç¨±: {file.filename}")
    print(f"   Content-Type: {file.content_type}")
    print(f"   æ¥­è€… ID: {vendor_id or 'é€šç”¨çŸ¥è­˜'}")
    print(f"   åŒ¯å…¥æ¨¡å¼: {import_mode}")
    print(f"   å•Ÿç”¨å»é‡: {enable_deduplication}")
    print(f"   å¯©æ ¸æ¨¡å¼: å¼·åˆ¶ï¼ˆæ‰€æœ‰çŸ¥è­˜éƒ½éœ€å¯©æ ¸ï¼‰")
    print(f"{'='*60}\n")

    # 1. é©—è­‰æª”æ¡ˆé¡å‹
    allowed_extensions = ['.xlsx', '.xls', '.txt', '.json']
    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}. æ”¯æ´çš„æ ¼å¼: {', '.join(allowed_extensions)}"
        )

    # 2. é©—è­‰æª”æ¡ˆå¤§å°ï¼ˆ50MB é™åˆ¶ï¼‰
    content = await file.read()
    file_size = len(content)

    if file_size > 50 * 1024 * 1024:
        raise HTTPException(
            status_code=400,
            detail=f"æª”æ¡ˆå¤§å°è¶…é 50MB é™åˆ¶ï¼ˆç•¶å‰: {file_size / 1024 / 1024:.2f}MBï¼‰"
        )

    if file_size == 0:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆç‚ºç©º")

    print(f"âœ… æª”æ¡ˆé©—è­‰é€šéï¼ˆå¤§å°: {file_size / 1024:.2f}KBï¼‰")

    # 3. å„²å­˜è‡¨æ™‚æª”æ¡ˆ
    job_id = str(uuid.uuid4())
    temp_dir = tempfile.gettempdir()
    safe_filename = f"{job_id}_{Path(file.filename).name}"
    temp_file_path = os.path.join(temp_dir, safe_filename)

    with open(temp_file_path, 'wb') as f:
        f.write(content)

    print(f"âœ… æª”æ¡ˆå·²å„²å­˜åˆ°è‡¨æ™‚ç›®éŒ„: {temp_file_path}")

    # 4. å–å¾—è³‡æ–™åº«é€£æ¥æ± 
    db_pool = request.app.state.db_pool

    # 5. å»ºç«‹ä½œæ¥­è¨˜éŒ„ï¼ˆåœ¨å•Ÿå‹•èƒŒæ™¯ä»»å‹™å‰ï¼‰
    async with db_pool.acquire() as conn:
        await conn.execute("""
            INSERT INTO knowledge_import_jobs (
                job_id, vendor_id, file_name, file_type, file_size_bytes, file_path,
                import_mode, enable_deduplication, created_by, status,
                created_at, updated_at
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
        """,
            uuid.UUID(job_id),
            vendor_id,
            file.filename,
            file_ext[1:],  # Remove the leading dot
            file_size,
            temp_file_path,
            import_mode,
            enable_deduplication,
            "admin",  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
            "pending"
        )

    print(f"âœ… ä½œæ¥­è¨˜éŒ„å·²å»ºç«‹ (job_id: {job_id})")

    # 6. å•Ÿå‹•èƒŒæ™¯ä»»å‹™
    from services.knowledge_import_service import KnowledgeImportService

    service = KnowledgeImportService(db_pool)

    print(f"ğŸš€ å•Ÿå‹•èƒŒæ™¯è™•ç†ä»»å‹™ (job_id: {job_id})")

    background_tasks.add_task(
        service.process_import_job,
        job_id=job_id,
        file_path=temp_file_path,
        vendor_id=vendor_id,
        import_mode=import_mode,
        enable_deduplication=enable_deduplication,
        user_id="admin"  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
    )

    return {
        "job_id": job_id,
        "status": "processing",
        "message": "æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹è™•ç†ä¸­ã€‚æ‰€æœ‰çŸ¥è­˜å°‡é€²å…¥å¯©æ ¸ä½‡åˆ—ï¼Œéœ€ç¶“äººå·¥å¯©æ ¸å¾Œæ‰æœƒæ­£å¼åŠ å…¥çŸ¥è­˜åº«ã€‚",
        "file_name": file.filename,
        "review_mode": "mandatory"
    }


@router.get("/jobs/{job_id}")
async def get_import_job_status(job_id: str, request: Request):
    """
    ç²å–åŒ¯å…¥ä»»å‹™ç‹€æ…‹ï¼ˆä¾›å‰ç«¯è¼ªè©¢ï¼‰

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        ImportJobStatus: ä»»å‹™ç‹€æ…‹
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        job = await conn.fetchrow("""
            SELECT
                job_id,
                vendor_id,
                file_name,
                status,
                progress,
                result,
                error_message,
                created_at,
                updated_at
            FROM knowledge_import_jobs
            WHERE job_id = $1
        """, uuid.UUID(job_id))

        if not job:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # è§£æ JSON æ¬„ä½
        import json
        progress = json.loads(job['progress']) if job['progress'] else None
        result = json.loads(job['result']) if job['result'] else None

        return {
            "job_id": str(job['job_id']),
            "status": job['status'],
            "progress": progress,
            "result": result,
            "error": job['error_message'],
            "file_name": job['file_name'],
            "vendor_id": job['vendor_id'],
            "created_at": job['created_at'].isoformat() if job['created_at'] else None,
            "updated_at": job['updated_at'].isoformat() if job['updated_at'] else None
        }


@router.get("/jobs")
async def list_import_jobs(
    request: Request,
    vendor_id: Optional[int] = None,
    limit: int = 20,
    offset: int = 0
):
    """
    åˆ—å‡ºåŒ¯å…¥ä»»å‹™æ­·å²

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œéæ¿¾ç‰¹å®šæ¥­è€…ï¼‰
        limit: è¿”å›æ•¸é‡é™åˆ¶
        offset: åç§»é‡

    Returns:
        List[Dict]: ä»»å‹™åˆ—è¡¨
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        if vendor_id:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    file_name,
                    status,
                    imported_count,
                    skipped_count,
                    error_count,
                    created_at,
                    completed_at
                FROM knowledge_import_jobs
                WHERE vendor_id = $1
                ORDER BY created_at DESC
                LIMIT $2 OFFSET $3
            """, vendor_id, limit, offset)
        else:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    file_name,
                    status,
                    imported_count,
                    skipped_count,
                    error_count,
                    created_at,
                    completed_at
                FROM knowledge_import_jobs
                ORDER BY created_at DESC
                LIMIT $1 OFFSET $2
            """, limit, offset)

        # å–å¾—ç¸½æ•¸
        if vendor_id:
            total = await conn.fetchval("""
                SELECT COUNT(*) FROM knowledge_import_jobs WHERE vendor_id = $1
            """, vendor_id)
        else:
            total = await conn.fetchval("SELECT COUNT(*) FROM knowledge_import_jobs")

        return {
            "jobs": [
                {
                    "job_id": str(job['job_id']),
                    "vendor_id": job['vendor_id'],
                    "file_name": job['file_name'],
                    "status": job['status'],
                    "imported_count": job['imported_count'],
                    "skipped_count": job['skipped_count'],
                    "error_count": job['error_count'],
                    "created_at": job['created_at'].isoformat() if job['created_at'] else None,
                    "completed_at": job['completed_at'].isoformat() if job['completed_at'] else None
                }
                for job in jobs
            ],
            "total": total,
            "limit": limit,
            "offset": offset
        }


@router.post("/preview")
async def preview_knowledge_file(file: UploadFile = File(...)):
    """
    é è¦½æª”æ¡ˆå…§å®¹ï¼ˆä¸å‘¼å« LLMï¼Œä¸æ¶ˆè€— tokenï¼‰

    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ

    Returns:
        Dict: é è¦½è³‡è¨Š
    """
    # é©—è­‰æª”æ¡ˆé¡å‹
    allowed_extensions = ['.xlsx', '.xls', '.txt', '.json']
    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}"
        )

    content = await file.read()
    file_size = len(content)

    if file_size > 50 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆå¤§å°è¶…é 50MB é™åˆ¶")

    # æ ¹æ“šæª”æ¡ˆé¡å‹é è¦½
    preview_data = {}

    if file_ext in ['.xlsx', '.xls']:
        # Excel é è¦½
        import pandas as pd
        import io

        df = pd.read_excel(io.BytesIO(content), engine='openpyxl')

        preview_data = {
            "file_type": "excel",
            "total_rows": len(df),
            "columns": list(df.columns),
            "preview_rows": df.head(5).to_dict(orient='records'),
            "estimated_knowledge": len(df)  # ç²—ç•¥ä¼°ç®—
        }

    elif file_ext == '.txt':
        # ç´”æ–‡å­—é è¦½
        content_str = content.decode('utf-8', errors='ignore')
        lines = content_str.split('\n')

        preview_data = {
            "file_type": "text",
            "total_lines": len(lines),
            "preview_lines": lines[:20],
            "estimated_knowledge": len(lines) // 10  # ç²—ç•¥ä¼°ç®—
        }

    elif file_ext == '.json':
        # JSON é è¦½
        import json
        data = json.loads(content.decode('utf-8'))

        if 'knowledge' in data:
            knowledge_list = data['knowledge']
        elif 'knowledge_list' in data:
            knowledge_list = data['knowledge_list']
        elif isinstance(data, list):
            knowledge_list = data
        else:
            knowledge_list = []

        preview_data = {
            "file_type": "json",
            "total_items": len(knowledge_list),
            "preview_items": knowledge_list[:5] if knowledge_list else [],
            "estimated_knowledge": len(knowledge_list)
        }

    return {
        "filename": file.filename,
        "file_size_kb": file_size / 1024,
        **preview_data,
        "message": "é€™æ˜¯é è¦½æ¨¡å¼ï¼Œå°šæœªæ¶ˆè€—ä»»ä½• OpenAI token"
    }


@router.delete("/jobs/{job_id}")
async def delete_import_job(job_id: str, request: Request):
    """
    åˆªé™¤åŒ¯å…¥ä»»å‹™è¨˜éŒ„

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        Dict: åˆªé™¤çµæœ
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        deleted = await conn.fetchval("""
            DELETE FROM knowledge_import_jobs
            WHERE job_id = $1
            RETURNING job_id
        """, uuid.UUID(job_id))

        if not deleted:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    return {
        "message": "ä»»å‹™å·²åˆªé™¤",
        "job_id": job_id
    }


@router.get("/statistics")
async def get_import_statistics(
    request: Request,
    vendor_id: Optional[int] = None,
    days: int = 30
):
    """
    å–å¾—åŒ¯å…¥çµ±è¨ˆè³‡è¨Š

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
        days: çµ±è¨ˆå¤©æ•¸

    Returns:
        Dict: çµ±è¨ˆè³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        stats = await conn.fetchrow("""
            SELECT * FROM get_import_statistics($1, $2)
        """, vendor_id, days)

        return {
            "total_jobs": stats['total_jobs'],
            "completed_jobs": stats['completed_jobs'],
            "failed_jobs": stats['failed_jobs'],
            "processing_jobs": stats['processing_jobs'],
            "total_imported": stats['total_imported'],
            "total_skipped": stats['total_skipped'],
            "total_errors": stats['total_errors'],
            "avg_imported_per_job": float(stats['avg_imported_per_job']) if stats['avg_imported_per_job'] else 0,
            "success_rate": float(stats['success_rate']) if stats['success_rate'] else 0,
            "days": days
        }
