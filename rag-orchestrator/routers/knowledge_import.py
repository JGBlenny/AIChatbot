"""
çŸ¥è­˜åº«åŒ¯å…¥ API
æ”¯æ´ä¸Šå‚³å¤šç¨®æ ¼å¼çš„æª”æ¡ˆï¼ˆExcel, TXT, JSONï¼‰ï¼Œè‡ªå‹•æå–çŸ¥è­˜åº«ä¸¦å»é‡
"""

from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks, Request, Form
from pydantic import BaseModel
from typing import List, Dict, Optional
import tempfile
import os
import uuid
from datetime import datetime
from pathlib import Path

router = APIRouter(prefix="/api/v1/knowledge-import", tags=["Knowledge Import"])


class ImportJobStatus(BaseModel):
    """åŒ¯å…¥ä»»å‹™ç‹€æ…‹"""
    job_id: str
    status: str  # pending, processing, completed, failed
    progress: Optional[Dict] = None  # {current: 50, total: 100, stage: "ç”Ÿæˆå‘é‡"}
    result: Optional[Dict] = None  # {imported: 40, skipped: 5, errors: 0}
    error: Optional[str] = None
    file_name: Optional[str] = None
    vendor_id: Optional[int] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


class ImportOptions(BaseModel):
    """åŒ¯å…¥é¸é …"""
    mode: str = "append"  # append, replace, merge
    enable_deduplication: bool = True
    vendor_id: Optional[int] = None


@router.post("/upload")
async def upload_knowledge_file(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    vendor_id: Optional[int] = Form(None),
    import_mode: str = Form("append"),
    enable_deduplication: bool = Form(True),
    skip_review: bool = Form(False),
    default_priority: int = Form(0),
    enable_quality_evaluation: bool = Form(True),
    business_types: Optional[str] = Form(None)  # JSON string of business type values
):
    """
    ä¸Šå‚³çŸ¥è­˜æª”æ¡ˆä¸¦é–‹å§‹åŒ¯å…¥

    âš ï¸ é‡è¦æé†’ï¼š
    - skip_review=Falseï¼ˆé è¨­ï¼‰ï¼šçŸ¥è­˜æœƒå…ˆé€²å…¥å¯©æ ¸ä½‡åˆ—ï¼Œéœ€ç¶“äººå·¥å¯©æ ¸å¾Œæ‰æœƒåŠ å…¥æ­£å¼çŸ¥è­˜åº«
    - skip_review=Trueï¼šçŸ¥è­˜æœƒç›´æ¥åŠ å…¥æ­£å¼çŸ¥è­˜åº«ï¼ˆè·³éå¯©æ ¸ï¼Œè«‹è¬¹æ…ä½¿ç”¨ï¼‰

    æ”¯æ´æ ¼å¼ï¼š
    - Excel (.xlsx, .xls)
    - ç´”æ–‡å­— (.txt)
    - JSON (.json)
    - CSV (.csv)

    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œç•™ç©ºè¡¨ç¤ºé€šç”¨çŸ¥è­˜ï¼‰
        import_mode: åŒ¯å…¥æ¨¡å¼ï¼ˆappend=è¿½åŠ , replace=æ›¿æ›, merge=åˆä½µï¼‰
        enable_deduplication: æ˜¯å¦å•Ÿç”¨å»é‡
        skip_review: æ˜¯å¦è·³éå¯©æ ¸ç›´æ¥åŠ å…¥çŸ¥è­˜åº«ï¼ˆé è¨­ Falseï¼‰
        default_priority: çµ±ä¸€å„ªå…ˆç´šï¼ˆ0=æœªå•Ÿç”¨ï¼Œ1=å·²å•Ÿç”¨ï¼Œåƒ…åœ¨ skip_review=True æ™‚ç”Ÿæ•ˆï¼‰
        enable_quality_evaluation: æ˜¯å¦å•Ÿç”¨è³ªé‡è©•ä¼°ï¼ˆé è¨­ Trueï¼Œé—œé–‰å¯åŠ é€Ÿå¤§é‡åŒ¯å…¥ï¼‰

    Returns:
        Dict: åŒ…å« job_id çš„å›æ‡‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¤ æ”¶åˆ°æª”æ¡ˆä¸Šå‚³è«‹æ±‚")
    print(f"   æª”æ¡ˆåç¨±: {file.filename}")
    print(f"   Content-Type: {file.content_type}")
    print(f"   æ¥­è€… ID: {vendor_id or 'é€šç”¨çŸ¥è­˜'}")
    print(f"   åŒ¯å…¥æ¨¡å¼: {import_mode}")
    print(f"   å•Ÿç”¨å»é‡: {enable_deduplication}")
    print(f"   è³ªé‡è©•ä¼°: {'å·²å•Ÿç”¨' if enable_quality_evaluation else 'å·²é—œé–‰ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰'}")
    print(f"   å¯©æ ¸æ¨¡å¼: {'è·³éå¯©æ ¸ï¼ˆç›´æ¥åŠ å…¥çŸ¥è­˜åº«ï¼‰' if skip_review else 'éœ€è¦å¯©æ ¸'}")
    if skip_review and default_priority > 0:
        print(f"   å„ªå…ˆç´š: çµ±ä¸€å•Ÿç”¨ (priority={default_priority})")
    print(f"{'='*60}\n")

    # 1. é©—è­‰æª”æ¡ˆé¡å‹
    allowed_extensions = ['.xlsx', '.xls', '.csv', '.txt', '.json']
    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}. æ”¯æ´çš„æ ¼å¼: {', '.join(allowed_extensions)}"
        )

    # 2. é©—è­‰æª”æ¡ˆå¤§å°ï¼ˆ50MB é™åˆ¶ï¼‰
    content = await file.read()
    file_size = len(content)

    if file_size > 50 * 1024 * 1024:
        raise HTTPException(
            status_code=400,
            detail=f"æª”æ¡ˆå¤§å°è¶…é 50MB é™åˆ¶ï¼ˆç•¶å‰: {file_size / 1024 / 1024:.2f}MBï¼‰"
        )

    if file_size == 0:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆç‚ºç©º")

    print(f"âœ… æª”æ¡ˆé©—è­‰é€šéï¼ˆå¤§å°: {file_size / 1024:.2f}KBï¼‰")

    # 3. å„²å­˜è‡¨æ™‚æª”æ¡ˆ
    job_id = str(uuid.uuid4())
    temp_dir = tempfile.gettempdir()
    safe_filename = f"{job_id}_{Path(file.filename).name}"
    temp_file_path = os.path.join(temp_dir, safe_filename)

    with open(temp_file_path, 'wb') as f:
        f.write(content)

    print(f"âœ… æª”æ¡ˆå·²å„²å­˜åˆ°è‡¨æ™‚ç›®éŒ„: {temp_file_path}")

    # 4. å–å¾—è³‡æ–™åº«é€£æ¥æ± 
    db_pool = request.app.state.db_pool

    # 5. ä½¿ç”¨çµ±ä¸€ Job æœå‹™å»ºç«‹ä½œæ¥­è¨˜éŒ„
    from services.knowledge_import_service import KnowledgeImportService
    service = KnowledgeImportService(db_pool)

    job_id = await service.create_job(
        job_type='knowledge_import',
        vendor_id=vendor_id,
        user_id="admin",  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
        job_config={
            'import_mode': import_mode,
            'enable_deduplication': enable_deduplication,
            'skip_review': skip_review,
            'default_priority': default_priority,
            'enable_quality_evaluation': enable_quality_evaluation,
            'file_type': file_ext[1:]
        },
        file_path=temp_file_path,
        file_name=file.filename,
        file_size_bytes=file_size
    )

    print(f"âœ… ä½œæ¥­è¨˜éŒ„å·²å»ºç«‹ (job_id: {job_id})")

    # 6. å•Ÿå‹•èƒŒæ™¯ä»»å‹™

    print(f"ğŸš€ å•Ÿå‹•èƒŒæ™¯è™•ç†ä»»å‹™ (job_id: {job_id})")

    # è§£ææ¥­æ…‹é¡å‹
    business_types_list = []
    if business_types:
        try:
            import json
            business_types_list = json.loads(business_types)
            print(f"ğŸ“‹ æ¥­æ…‹é¡å‹: {business_types_list}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£ææ¥­æ…‹é¡å‹: {e}")

    background_tasks.add_task(
        service.process_import_job,
        job_id=job_id,
        file_path=temp_file_path,
        vendor_id=vendor_id,
        import_mode=import_mode,
        enable_deduplication=enable_deduplication,
        skip_review=skip_review,
        default_priority=default_priority,
        enable_quality_evaluation=enable_quality_evaluation,
        business_types=business_types_list,
        user_id="admin"  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
    )

    # æ ¹æ“šæ¨¡å¼è¿”å›ä¸åŒè¨Šæ¯
    if skip_review:
        message = "æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹è™•ç†ä¸­ã€‚çŸ¥è­˜å°‡ç›´æ¥åŠ å…¥æ­£å¼çŸ¥è­˜åº«ï¼ˆå·²è·³éå¯©æ ¸ï¼‰ã€‚"
        review_mode = "skipped"
    else:
        message = "æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œé–‹å§‹è™•ç†ä¸­ã€‚æ‰€æœ‰çŸ¥è­˜å°‡é€²å…¥å¯©æ ¸ä½‡åˆ—ï¼Œéœ€ç¶“äººå·¥å¯©æ ¸å¾Œæ‰æœƒæ­£å¼åŠ å…¥çŸ¥è­˜åº«ã€‚"
        review_mode = "mandatory"

    return {
        "job_id": job_id,
        "status": "processing",
        "message": message,
        "file_name": file.filename,
        "review_mode": review_mode,
        "skip_review": skip_review
    }


@router.get("/jobs/{job_id}")
async def get_import_job_status(job_id: str, request: Request):
    """
    ç²å–åŒ¯å…¥ä»»å‹™ç‹€æ…‹ï¼ˆä¾›å‰ç«¯è¼ªè©¢ï¼‰

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        ImportJobStatus: ä»»å‹™ç‹€æ…‹
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        job = await conn.fetchrow("""
            SELECT
                job_id,
                vendor_id,
                file_name,
                status,
                progress,
                job_result,
                error_message,
                created_at,
                updated_at
            FROM unified_jobs
            WHERE job_id = $1
        """, uuid.UUID(job_id))

        if not job:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # è§£æ JSON æ¬„ä½
        import json
        progress = json.loads(job['progress']) if job['progress'] else None
        result = json.loads(job['job_result']) if job['job_result'] else None

        return {
            "job_id": str(job['job_id']),
            "status": job['status'],
            "progress": progress,
            "result": result,
            "error": job['error_message'],
            "file_name": job['file_name'],
            "vendor_id": job['vendor_id'],
            "created_at": job['created_at'].isoformat() if job['created_at'] else None,
            "updated_at": job['updated_at'].isoformat() if job['updated_at'] else None
        }


@router.get("/jobs")
async def list_import_jobs(
    request: Request,
    vendor_id: Optional[int] = None,
    limit: int = 20,
    offset: int = 0
):
    """
    åˆ—å‡ºåŒ¯å…¥ä»»å‹™æ­·å²

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œéæ¿¾ç‰¹å®šæ¥­è€…ï¼‰
        limit: è¿”å›æ•¸é‡é™åˆ¶
        offset: åç§»é‡

    Returns:
        List[Dict]: ä»»å‹™åˆ—è¡¨
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        if vendor_id:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    file_name,
                    status,
                    success_records,
                    skipped_records,
                    failed_records,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_import'
                ORDER BY created_at DESC
                LIMIT $2 OFFSET $3
            """, vendor_id, limit, offset)
        else:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    file_name,
                    status,
                    success_records,
                    skipped_records,
                    failed_records,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE job_type = 'knowledge_import'
                ORDER BY created_at DESC
                LIMIT $1 OFFSET $2
            """, limit, offset)

        # å–å¾—ç¸½æ•¸
        if vendor_id:
            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_import'
            """, vendor_id)
        else:
            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs WHERE job_type = 'knowledge_import'
            """)

        return {
            "jobs": [
                {
                    "job_id": str(job['job_id']),
                    "vendor_id": job['vendor_id'],
                    "file_name": job['file_name'],
                    "status": job['status'],
                    "imported_count": job['success_records'],
                    "skipped_count": job['skipped_records'],
                    "error_count": job['failed_records'],
                    "created_at": job['created_at'].isoformat() if job['created_at'] else None,
                    "completed_at": job['completed_at'].isoformat() if job['completed_at'] else None
                }
                for job in jobs
            ],
            "total": total,
            "limit": limit,
            "offset": offset
        }


@router.post("/preview")
async def preview_knowledge_file(file: UploadFile = File(...)):
    """
    é è¦½æª”æ¡ˆå…§å®¹ï¼ˆä¸å‘¼å« LLMï¼Œä¸æ¶ˆè€— tokenï¼‰

    Args:
        file: ä¸Šå‚³çš„æª”æ¡ˆ

    Returns:
        Dict: é è¦½è³‡è¨Šï¼ˆåŒ…å«ä¾†æºé¡å‹åµæ¸¬ï¼‰
    """
    # é©—è­‰æª”æ¡ˆé¡å‹
    allowed_extensions = ['.xlsx', '.xls', '.csv', '.txt', '.json']
    file_ext = Path(file.filename).suffix.lower()

    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}"
        )

    content = await file.read()
    file_size = len(content)

    if file_size > 50 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="æª”æ¡ˆå¤§å°è¶…é 50MB é™åˆ¶")

    # æ ¹æ“šæª”æ¡ˆé¡å‹é è¦½
    preview_data = {}

    # åˆå§‹åŒ–ä¾†æºåµæ¸¬è®Šæ•¸
    source_type = "external_file"
    import_source = "external_unknown"
    detected_source_description = "å¤–éƒ¨æª”æ¡ˆ"

    if file_ext in ['.xlsx', '.xls']:
        # Excel é è¦½
        import pandas as pd
        import io

        # å…ˆè®€å–ç¬¬ä¸€è¡Œï¼Œæª¢æŸ¥æ˜¯å¦æœ‰æ¥­è€…æ¨™ç±¤ï¼ˆç§Ÿç®¡æ¥­ QA æ ¼å¼ï¼‰
        df_first_row = pd.read_excel(io.BytesIO(content), engine='openpyxl', header=None, nrows=1)

        has_vendor_label = False
        vendor_label = None
        if pd.notna(df_first_row.iloc[0, 0]) and 'ç‰©æ¥­' in str(df_first_row.iloc[0, 0]):
            has_vendor_label = True
            if pd.notna(df_first_row.iloc[0, 1]):
                vendor_label = str(df_first_row.iloc[0, 1]).strip()

        # æ ¹æ“šæ ¼å¼é¸æ“‡ header è¡Œ
        if has_vendor_label:
            df = pd.read_excel(io.BytesIO(content), engine='openpyxl', header=1)  # ç¬¬ 2 è¡Œä½œç‚ºæ¨™é¡Œ
            detected_source_description = f"ç§Ÿç®¡æ¥­ QA æ ¼å¼ï¼ˆæ¥­è€…: {vendor_label}ï¼‰"
        else:
            df = pd.read_excel(io.BytesIO(content), engine='openpyxl')  # ç¬¬ 1 è¡Œä½œç‚ºæ¨™é¡Œ

            # ä¾†æºåµæ¸¬ï¼šæª¢æŸ¥æ˜¯å¦ç‚ºç³»çµ±åŒ¯å‡ºæª”æ¡ˆ
            expected_fields = {
                'question_summary', 'answer', 'scope', 'vendor_id',
                'business_types', 'target_user', 'intent_names',
                'keywords', 'priority'
            }
            actual_fields = set(df.columns)

            if expected_fields.issubset(actual_fields):
                # ç³»çµ±åŒ¯å‡ºæª”æ¡ˆ
                source_type = "external_file"
                import_source = "system_export"
                detected_source_description = "ç³»çµ±åŒ¯å‡ºæª”æ¡ˆï¼ˆå¯ç›´æ¥åŒ¯å…¥ï¼‰"
            else:
                # ä¸€èˆ¬ Excel æª”æ¡ˆ
                source_type = "external_file"
                import_source = "external_excel"
                detected_source_description = "å¤–éƒ¨ Excel æª”æ¡ˆ"

        # å°‡ NaN è½‰æ›ç‚ºç©ºå­—ä¸²ï¼Œé¿å… JSON åºåˆ—åŒ–éŒ¯èª¤
        preview_df = df.head(5).fillna('')  # ç”¨ç©ºå­—ä¸²å–ä»£ NaN

        preview_data = {
            "file_type": "excel",
            "total_rows": len(df),
            "columns": list(df.columns),
            "preview_rows": preview_df.to_dict(orient='records'),
            "estimated_knowledge": len(df),  # ç²—ç•¥ä¼°ç®—
            "vendor_label": vendor_label  # æ–°å¢ï¼šæ¥­è€…æ¨™ç±¤
        }

    elif file_ext == '.csv':
        # CSV é è¦½
        import pandas as pd
        import io

        try:
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8')
        except UnicodeDecodeError:
            df = pd.read_csv(io.BytesIO(content), encoding='utf-8-sig')

        # å°‡ NaN è½‰æ›ç‚ºç©ºå­—ä¸²ï¼Œé¿å… JSON åºåˆ—åŒ–éŒ¯èª¤
        preview_df = df.head(5).fillna('')

        preview_data = {
            "file_type": "csv",
            "total_rows": len(df),
            "columns": list(df.columns),
            "preview_rows": preview_df.to_dict(orient='records'),
            "estimated_knowledge": len(df)  # ç²—ç•¥ä¼°ç®—
        }

    elif file_ext == '.txt':
        # ç´”æ–‡å­—é è¦½
        content_str = content.decode('utf-8', errors='ignore')
        lines = content_str.split('\n')

        # ä¾†æºåµæ¸¬ï¼šæª¢æŸ¥æª”åæ˜¯å¦åŒ…å«å°è©±é—œéµå­—
        filename_lower = file.filename.lower()
        if 'chat' in filename_lower or 'conversation' in filename_lower or 'å°è©±' in filename_lower or 'èŠå¤©' in filename_lower:
            source_type = "line_chat"
            import_source = "line_chat_txt"
            detected_source_description = "å°è©±è¨˜éŒ„ï¼ˆå°‡å‰µå»ºæ¸¬è©¦æƒ…å¢ƒï¼‰"
        else:
            source_type = "external_file"
            import_source = "external_txt"
            detected_source_description = "ç´”æ–‡å­—æª”æ¡ˆ"

        preview_data = {
            "file_type": "text",
            "total_lines": len(lines),
            "preview_lines": lines[:20],
            "estimated_knowledge": len(lines) // 10  # ç²—ç•¥ä¼°ç®—
        }

    elif file_ext == '.json':
        # JSON é è¦½
        import json
        data = json.loads(content.decode('utf-8'))

        if 'knowledge' in data:
            knowledge_list = data['knowledge']
        elif 'knowledge_list' in data:
            knowledge_list = data['knowledge_list']
        elif isinstance(data, list):
            knowledge_list = data
        else:
            knowledge_list = []

        # ä¾†æºåµæ¸¬ï¼šJSON æª”æ¡ˆé è¨­ç‚ºå¤–éƒ¨æª”æ¡ˆ
        source_type = "external_file"
        import_source = "external_json"
        detected_source_description = "å¤–éƒ¨ JSON æª”æ¡ˆ"

        preview_data = {
            "file_type": "json",
            "total_items": len(knowledge_list),
            "preview_items": knowledge_list[:5] if knowledge_list else [],
            "estimated_knowledge": len(knowledge_list)
        }

    return {
        "filename": file.filename,
        "file_size_kb": file_size / 1024,
        **preview_data,
        "source_type": source_type,
        "import_source": import_source,
        "detected_source_description": detected_source_description,
        "message": "é€™æ˜¯é è¦½æ¨¡å¼ï¼Œå°šæœªæ¶ˆè€—ä»»ä½• OpenAI token"
    }


@router.post("/jobs/{job_id}/confirm")
async def confirm_test_scenarios(
    job_id: str,
    request: Request,
    body: dict
):
    """
    ç¢ºèªå‰µå»ºé¸ä¸­çš„æ¸¬è©¦æƒ…å¢ƒï¼ˆå°è©±è¨˜éŒ„åŒ¯å…¥å°ˆç”¨ï¼‰

    Args:
        job_id: ä»»å‹™ ID
        body: åŒ…å« selected_indicesï¼ˆç”¨æˆ¶é¸ä¸­çš„æ¸¬è©¦æƒ…å¢ƒç´¢å¼•åˆ—è¡¨ï¼‰

    Returns:
        Dict: å‰µå»ºçµæœ
    """
    db_pool = request.app.state.db_pool
    selected_indices = body.get('selected_indices', [])

    if not selected_indices:
        raise HTTPException(status_code=400, detail="è«‹è‡³å°‘é¸æ“‡ä¸€å€‹æ¸¬è©¦æƒ…å¢ƒ")

    # ç²å–ä»»å‹™ä¿¡æ¯
    async with db_pool.acquire() as conn:
        job = await conn.fetchrow("""
            SELECT job_id, status, job_result, file_name, user_id
            FROM unified_jobs
            WHERE job_id = $1
        """, uuid.UUID(job_id))

        if not job:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        if job['status'] != 'awaiting_confirmation':
            raise HTTPException(
                status_code=400,
                detail=f"ä»»å‹™ç‹€æ…‹å¿…é ˆç‚º awaiting_confirmationï¼ˆç•¶å‰: {job['status']}ï¼‰"
            )

    # å¾ job_result ä¸­ç²å– scenarios
    import json
    result = job['job_result']
    if isinstance(result, str):
        result = json.loads(result)
    scenarios = result.get('scenarios', [])

    if not scenarios:
        raise HTTPException(status_code=400, detail="ä»»å‹™ä¸­æ²’æœ‰æ¸¬è©¦æƒ…å¢ƒæ•¸æ“š")

    # å‰µå»ºé¸ä¸­çš„æ¸¬è©¦æƒ…å¢ƒ
    from services.knowledge_import_service import KnowledgeImportService
    service = KnowledgeImportService(db_pool)

    creation_result = await service._create_selected_scenarios(
        scenarios=scenarios,
        selected_indices=selected_indices,
        created_by=job['user_id']
    )

    # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
    await service.update_status(
        job_id=job_id,
        status="completed",
        progress={"current": 100, "total": 100},
        result=creation_result,
        success_records=creation_result.get('created', 0),
        skipped_records=creation_result.get('skipped', 0),
        failed_records=creation_result.get('errors', 0)
    )

    return {
        "message": "æ¸¬è©¦æƒ…å¢ƒå‰µå»ºå®Œæˆ",
        "job_id": job_id,
        **creation_result
    }


@router.delete("/jobs/{job_id}")
async def delete_import_job(job_id: str, request: Request):
    """
    åˆªé™¤åŒ¯å…¥ä»»å‹™è¨˜éŒ„

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        Dict: åˆªé™¤çµæœ
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        deleted = await conn.fetchval("""
            DELETE FROM unified_jobs
            WHERE job_id = $1
            RETURNING job_id
        """, uuid.UUID(job_id))

        if not deleted:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    return {
        "message": "ä»»å‹™å·²åˆªé™¤",
        "job_id": job_id
    }


@router.get("/statistics")
async def get_import_statistics(
    request: Request,
    vendor_id: Optional[int] = None,
    days: int = 30
):
    """
    å–å¾—åŒ¯å…¥çµ±è¨ˆè³‡è¨Š

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
        days: çµ±è¨ˆå¤©æ•¸

    Returns:
        Dict: çµ±è¨ˆè³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        stats = await conn.fetchrow("""
            SELECT * FROM get_import_statistics($1, $2)
        """, vendor_id, days)

        return {
            "total_jobs": stats['total_jobs'],
            "completed_jobs": stats['completed_jobs'],
            "failed_jobs": stats['failed_jobs'],
            "processing_jobs": stats['processing_jobs'],
            "total_imported": stats['total_imported'],
            "total_skipped": stats['total_skipped'],
            "total_errors": stats['total_errors'],
            "avg_imported_per_job": float(stats['avg_imported_per_job']) if stats['avg_imported_per_job'] else 0,
            "success_rate": float(stats['success_rate']) if stats['success_rate'] else 0,
            "days": days
        }
