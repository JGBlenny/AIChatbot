"""
Streaming Chat API
æä¾› Server-Sent Events (SSE) æµå¼è¼¸å‡º
Phase 3: ç”¨æˆ¶é«”é©—å„ªåŒ– - å³æ™‚åé¥‹
"""
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, Dict
import json
import asyncio
import time

router = APIRouter()


class ChatStreamRequest(BaseModel):
    """æµå¼èŠå¤©è«‹æ±‚"""
    question: str = Field(..., min_length=1, max_length=1000)
    vendor_id: int = Field(..., ge=1)
    user_role: str = Field(..., description="customer æˆ– staff")
    user_id: Optional[str] = None
    context: Optional[Dict] = None


async def generate_sse_event(event_type: str, data: dict) -> str:
    """ç”Ÿæˆ SSE æ ¼å¼çš„äº‹ä»¶"""
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


async def chat_stream_generator(
    request: ChatStreamRequest,
    app_state
):
    """
    æµå¼èŠå¤©ç”Ÿæˆå™¨

    è¼¸å‡ºäº‹ä»¶é¡å‹:
    - start: é–‹å§‹è™•ç†
    - intent: æ„åœ–åˆ†é¡å®Œæˆ
    - search: æª¢ç´¢å®Œæˆ
    - confidence: ä¿¡å¿ƒåº¦è©•ä¼°å®Œæˆ
    - answer_chunk: ç­”æ¡ˆç‰‡æ®µï¼ˆé€å­—è¼¸å‡ºï¼‰
    - answer_complete: ç­”æ¡ˆå®Œæˆ
    - metadata: è™•ç†å…ƒæ•¸æ“š
    - done: å…¨éƒ¨å®Œæˆ
    """
    start_time = time.time()

    try:
        # 1. ç™¼é€é–‹å§‹äº‹ä»¶
        yield await generate_sse_event("start", {
            "message": "é–‹å§‹è™•ç†å•é¡Œ...",
            "question": request.question
        })

        # ç²å–æœå‹™å¯¦ä¾‹
        intent_classifier = app_state.intent_classifier
        rag_engine = app_state.rag_engine
        confidence_evaluator = app_state.confidence_evaluator
        llm_optimizer = app_state.llm_answer_optimizer
        vendor_config_service = app_state.vendor_config_service

        # 2. ä¸¦è¡ŒåŸ·è¡Œæ„åœ–åˆ†é¡å’Œæª¢ç´¢ï¼ˆPhase 2 å„ªåŒ– + Vendor SOP æ•´åˆï¼‰
        # NOTE: ç§»é™¤äº†æ—©æœŸåƒæ•¸æª¢æŸ¥ï¼Œæ”¹ç‚ºåœ¨æ‰¾ä¸åˆ°çŸ¥è­˜åº«/SOPçµæœæ™‚æ‰ä½¿ç”¨åƒæ•¸ç­”æ¡ˆ
        import os
        intent_task = asyncio.to_thread(intent_classifier.classify, request.question)
        fallback_similarity_threshold = float(os.getenv("FALLBACK_SIMILARITY_THRESHOLD", "0.55"))
        rag_top_k = int(os.getenv("RAG_TOP_K", "5"))
        search_task = rag_engine.search(
            query=request.question,
            limit=rag_top_k,
            similarity_threshold=fallback_similarity_threshold,
            vendor_id=request.vendor_id
        )

        intent_result, search_results = await asyncio.gather(intent_task, search_task)

        # 2.5. å¦‚æœæœ‰æ˜ç¢ºæ„åœ–ï¼Œä¹Ÿæª¢ç´¢ Vendor SOPï¼ˆä½¿ç”¨å…±ç”¨æ¨¡çµ„ï¼‰
        intent_ids = intent_result.get('intent_ids', [])
        print(f"ğŸ” Intent IDs: {intent_ids}, Vendor ID: {request.vendor_id}")
        print(f"ğŸ”‘ Question Keywords: {intent_result.get('keywords', [])}")
        print(f"ğŸ“Š Search Results: {len(search_results)} items")
        if search_results:
            print(f"   First Result Keywords: {search_results[0].get('keywords', [])}")

        sop_items = []
        if intent_ids and request.vendor_id:
            try:
                # ä½¿ç”¨å…±ç”¨æ¨¡çµ„çš„ SOP æª¢ç´¢å‡½æ•¸
                from routers.chat_shared import retrieve_sop_async, convert_sop_to_search_results

                # æª¢ç´¢ SOPï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼‰
                sop_items = await retrieve_sop_async(
                    vendor_id=request.vendor_id,
                    intent_ids=intent_ids,
                    top_k=rag_top_k
                )

                # å¦‚æœæ‰¾åˆ° SOPï¼Œè½‰æ›ç‚ºæ¨™æº–æ ¼å¼ä¸¦å„ªå…ˆä½¿ç”¨
                if sop_items:
                    # ä½¿ç”¨å…±ç”¨å‡½æ•¸è½‰æ›æ ¼å¼ï¼ˆè‡ªå‹•è¨­å®š similarity=1.0ï¼‰
                    sop_search_results = convert_sop_to_search_results(sop_items)

                    # åˆä½µï¼šSOP åœ¨å‰ï¼Œä¸€èˆ¬çŸ¥è­˜åœ¨å¾Œ
                    search_results = sop_search_results + search_results[:2]
                    print(f"âœ¨ åˆä½µå¾Œå…± {len(search_results)} å€‹çµæœï¼ˆ{len(sop_search_results)} SOP + {min(2, len(search_results) - len(sop_search_results))} çŸ¥è­˜åº«ï¼‰")
            except Exception as e:
                print(f"âš ï¸  Vendor SOP æª¢ç´¢å¤±æ•—: {e}ï¼Œä½¿ç”¨ä¸€èˆ¬çŸ¥è­˜åº«çµæœ")

        # 3. ç™¼é€æ„åœ–åˆ†é¡çµæœ
        yield await generate_sse_event("intent", {
            "intent_type": intent_result['intent_type'],
            "intent_name": intent_result.get('intent_name', 'unknown'),
            "confidence": intent_result.get('confidence', 0)
        })

        # 4. ç™¼é€æª¢ç´¢çµæœ
        yield await generate_sse_event("search", {
            "doc_count": len(search_results),
            "has_results": len(search_results) > 0
        })

        # 5. ä¿¡å¿ƒåº¦è©•ä¼°ï¼ˆSOP æª¢ç´¢æ™‚å¼·åˆ¶ä½¿ç”¨é«˜ä¿¡å¿ƒåº¦ï¼‰
        from routers.chat_shared import has_sop_results

        has_sop = has_sop_results(search_results)

        if has_sop:
            # SOP ç²¾æº–åŒ¹é…ï¼Œå¼·åˆ¶ä½¿ç”¨é«˜ä¿¡å¿ƒåº¦ï¼ˆèˆ‡ chat.py çµ±ä¸€ï¼‰
            evaluation = {
                'confidence_score': 0.95,
                'confidence_level': 'high',
                'decision': 'direct_answer'
            }
            print(f"ğŸ“‹ [SOP] å¼·åˆ¶ä½¿ç”¨é«˜ä¿¡å¿ƒåº¦ï¼ˆsimilarity=1.0ï¼‰")
        else:
            # æ­£å¸¸ä¿¡å¿ƒåº¦è©•ä¼°
            evaluation = confidence_evaluator.evaluate(
                search_results=search_results,
                question_keywords=intent_result['keywords']
            )

        yield await generate_sse_event("confidence", {
            "score": evaluation['confidence_score'],
            "level": evaluation['confidence_level'],
            "decision": evaluation['decision']
        })

        # 6. æ ¹æ“šæ±ºç­–ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ streamingï¼‰
        if evaluation['decision'] == 'direct_answer' and search_results:
            # é«˜ä¿¡å¿ƒåº¦ï¼šä½¿ç”¨æ¢ä»¶å¼å„ªåŒ–ï¼ˆPhase 1ï¼‰
            async for chunk in stream_optimized_answer(
                request.question,
                search_results,
                evaluation,
                intent_result,
                llm_optimizer,
                vendor_id=request.vendor_id
            ):
                yield chunk

        elif evaluation['decision'] == 'needs_enhancement' and search_results:
            # ä¸­ç­‰ä¿¡å¿ƒåº¦
            async for chunk in stream_optimized_answer(
                request.question,
                search_results,
                evaluation,
                intent_result,
                llm_optimizer,
                vendor_id=request.vendor_id,
                add_warning=True
            ):
                yield chunk

        else:
            # ä½ä¿¡å¿ƒåº¦æˆ–ç„¡çµæœï¼šå˜—è©¦åƒæ•¸ç­”æ¡ˆï¼Œå¦å‰‡è¿”å› unclear
            from routers.chat_shared import check_param_question
            param_category, param_answer = await check_param_question(
                vendor_config_service=vendor_config_service,
                question=request.question,
                vendor_id=request.vendor_id
            )

            if param_answer:
                # ä½¿ç”¨åƒæ•¸å‹ç­”æ¡ˆ
                print(f"   â„¹ï¸  çŸ¥è­˜åº«/SOPç„¡çµæœï¼Œä½¿ç”¨åƒæ•¸å‹ç­”æ¡ˆï¼ˆcategory={param_category}ï¼‰")
                yield await generate_sse_event("config_param", {
                    "category": param_category,
                    "config_used": param_answer.get('config_used', {})
                })

                # æµå¼è¼¸å‡ºç­”æ¡ˆ
                answer = param_answer['answer']
                words = answer.split()
                for i, word in enumerate(words):
                    chunk = word + (" " if i < len(words) - 1 else "")
                    yield await generate_sse_event("answer_chunk", {"chunk": chunk})
                    await asyncio.sleep(0.02)
            else:
                # unclear ç­”æ¡ˆ
                yield await generate_sse_event("answer_chunk", {
                    "chunk": "æŠ±æ­‰ï¼Œæˆ‘å°é€™å€‹å•é¡Œä¸å¤ªç¢ºå®šå¦‚ä½•å›ç­”ã€‚\n\n"
                })
                yield await generate_sse_event("answer_chunk", {
                    "chunk": "æ‚¨çš„å•é¡Œå·²ç¶“è¨˜éŒ„ä¸‹ä¾†ï¼Œæˆ‘å€‘æœƒç›¡å¿«è™•ç†ã€‚\n"
                })
                yield await generate_sse_event("answer_chunk", {
                    "chunk": "å¦‚éœ€ç«‹å³å”åŠ©ï¼Œè«‹è¯ç¹«å®¢æœäººå“¡ã€‚"
                })

        # 7. ç™¼é€å®Œæˆäº‹ä»¶
        processing_time = int((time.time() - start_time) * 1000)
        yield await generate_sse_event("answer_complete", {
            "processing_time_ms": processing_time
        })

        # 8. ç™¼é€å…ƒæ•¸æ“š
        yield await generate_sse_event("metadata", {
            "confidence_score": evaluation['confidence_score'],
            "confidence_level": evaluation['confidence_level'],
            "intent_type": intent_result['intent_type'],
            "doc_count": len(search_results),
            "processing_time_ms": processing_time
        })

        # 9. ç™¼é€å®Œæˆæ¨™è¨˜
        yield await generate_sse_event("done", {
            "message": "è™•ç†å®Œæˆ",
            "success": True
        })

    except Exception as e:
        # éŒ¯èª¤è™•ç†
        yield await generate_sse_event("error", {
            "message": str(e),
            "type": type(e).__name__
        })
        yield await generate_sse_event("done", {
            "message": "è™•ç†å¤±æ•—",
            "success": False
        })


async def stream_optimized_answer(
    question: str,
    search_results: list,
    evaluation: dict,
    intent_result: dict,
    llm_optimizer,
    vendor_id: int = None,
    add_warning: bool = False
):
    """
    æµå¼è¼¸å‡ºå„ªåŒ–å¾Œçš„ç­”æ¡ˆ

    Phase 3 æ“´å±•ï¼šæ•´åˆç­”æ¡ˆåˆæˆåŠŸèƒ½
    - ä½¿ç”¨ llm_optimizer.optimize_answer() çµ±ä¸€è™•ç†æ‰€æœ‰å„ªåŒ–ç­–ç•¥
    - æ”¯æ´ç­”æ¡ˆåˆæˆï¼ˆè‡ªå‹•åˆä½µå¤šå€‹ SOP é …ç›®ï¼‰
    - æ¢ä»¶å¼å„ªåŒ–ï¼ˆå¿«é€Ÿè·¯å¾‘ã€æ¨¡æ¿ã€LLMï¼‰ç”± optimizer å…§éƒ¨æ±ºå®š

    Phase 4 æ“´å±•ï¼šæ¥­æ…‹èªæ°£èª¿æ•´
    - æ ¹æ“š vendor_id å–å¾—æ¥­è€…çš„ business_types
    - å°‡ vendor_info å‚³éçµ¦å„ªåŒ–å™¨ä»¥æ‡‰ç”¨èªæ°£èª¿æ•´
    """
    confidence_score = evaluation['confidence_score']
    confidence_level = evaluation['confidence_level']

    # å–å¾—æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_typesï¼Œç”¨æ–¼èªæ°£èª¿æ•´ï¼‰
    vendor_info = None
    vendor_name = None
    if vendor_id:
        try:
            from services.vendor_parameter_resolver import VendorParameterResolver
            param_resolver = VendorParameterResolver()
            vendor_info = param_resolver.get_vendor_info(vendor_id)
            if vendor_info:
                vendor_name = vendor_info.get('name')
                # ç¢ºä¿ business_type æ¬„ä½å­˜åœ¨ï¼ˆå¾ business_types é™£åˆ—å–ç¬¬ä¸€å€‹ï¼‰
                if 'business_types' in vendor_info and vendor_info['business_types']:
                    vendor_info['business_type'] = vendor_info['business_types'][0]
                print(f"ğŸ“‹ æ¥­è€…è³‡è¨Š: {vendor_name}, æ¥­æ…‹: {vendor_info.get('business_type', 'N/A')}")
        except Exception as e:
            print(f"âš ï¸  å–å¾—æ¥­è€…è³‡è¨Šå¤±æ•—: {e}")

    try:
        # ä½¿ç”¨ llm_optimizer é€²è¡Œç­”æ¡ˆå„ªåŒ–ï¼ˆåŒ…å«ç­”æ¡ˆåˆæˆå’Œèªæ°£èª¿æ•´ï¼‰
        # åœ¨ asyncio ç·šç¨‹æ± ä¸­åŸ·è¡ŒåŒæ­¥æ–¹æ³•
        optimization_result = await asyncio.to_thread(
            llm_optimizer.optimize_answer,
            question=question,
            search_results=search_results,
            confidence_level=confidence_level,
            intent_info=intent_result,
            confidence_score=confidence_score,
            vendor_name=vendor_name,
            vendor_info=vendor_info
        )

        # å–å¾—å„ªåŒ–å¾Œçš„ç­”æ¡ˆ
        answer = optimization_result['optimized_answer']
        synthesis_applied = optimization_result.get('synthesis_applied', False)
        optimization_method = optimization_result.get('optimization_method', 'unknown')

        # è¨˜éŒ„å„ªåŒ–çµæœ
        if synthesis_applied:
            print(f"ğŸ”„ [Streaming] ç­”æ¡ˆåˆæˆå·²æ‡‰ç”¨ ({len(search_results)} å€‹ä¾†æº)")
        else:
            print(f"ğŸ“¤ [Streaming] å„ªåŒ–æ–¹æ³•: {optimization_method} (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")

        # ç™¼é€åˆæˆç‹€æ…‹äº‹ä»¶ï¼ˆå¦‚æœæœ‰åˆæˆï¼‰
        if synthesis_applied:
            yield await generate_sse_event("synthesis", {
                "applied": True,
                "source_count": len(search_results),
                "method": optimization_method
            })

        # æµå¼è¼¸å‡ºç­”æ¡ˆï¼ˆé€å­—è¼¸å‡ºä»¥æå‡ç”¨æˆ¶é«”é©—ï¼‰
        words = answer.split()
        for i, word in enumerate(words):
            chunk = word + (" " if i < len(words) - 1 else "")
            yield await generate_sse_event("answer_chunk", {"chunk": chunk})

            # æ ¹æ“šå„ªåŒ–æ–¹æ³•èª¿æ•´è¼¸å‡ºé€Ÿåº¦
            if optimization_method == "fast_path":
                await asyncio.sleep(0.015)  # å¿«é€Ÿè·¯å¾‘ï¼š15ms
            elif optimization_method == "template":
                await asyncio.sleep(0.02)   # æ¨¡æ¿ï¼š20ms
            else:
                await asyncio.sleep(0.025)  # LLM/åˆæˆï¼š25msï¼ˆæ¨¡æ“¬ LLM ç”Ÿæˆï¼‰

    except Exception as e:
        # å„ªåŒ–å¤±æ•—æ™‚çš„é™ç´šè™•ç†
        print(f"âŒ [Streaming] ç­”æ¡ˆå„ªåŒ–å¤±æ•—: {e}")

        # ä½¿ç”¨ç¬¬ä¸€å€‹æœå°‹çµæœä½œç‚ºé™ç´šç­”æ¡ˆ
        if search_results:
            answer = search_results[0].get('content', 'æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆç­”æ¡ˆã€‚')
            words = answer.split()
            for i, word in enumerate(words):
                chunk = word + (" " if i < len(words) - 1 else "")
                yield await generate_sse_event("answer_chunk", {"chunk": chunk})
                await asyncio.sleep(0.02)
        else:
            yield await generate_sse_event("answer_chunk", {
                "chunk": "æŠ±æ­‰ï¼Œç„¡æ³•ç”Ÿæˆç­”æ¡ˆã€‚"
            })

    # å¦‚æœéœ€è¦æ·»åŠ è­¦å‘Š
    if add_warning:
        yield await generate_sse_event("answer_chunk", {
            "chunk": f"\n\nâš ï¸ æ³¨æ„ï¼šæ­¤ç­”æ¡ˆä¿¡å¿ƒåº¦ç‚ºä¸­ç­‰ï¼ˆ{confidence_score:.2f}ï¼‰ï¼Œå»ºè­°æ‚¨è¯ç¹«å®¢æœäººå“¡é€²ä¸€æ­¥ç¢ºèªã€‚"
        })


@router.post("/chat/stream")
async def chat_stream(request: ChatStreamRequest, req: Request):
    """
    æµå¼èŠå¤© API (SSE)

    è¿”å› Server-Sent Events æµï¼ŒåŒ…å«ä»¥ä¸‹äº‹ä»¶:
    - start: é–‹å§‹è™•ç†
    - intent: æ„åœ–åˆ†é¡
    - search: æª¢ç´¢çµæœ
    - confidence: ä¿¡å¿ƒåº¦è©•ä¼°
    - answer_chunk: ç­”æ¡ˆç‰‡æ®µï¼ˆé€å­—è¼¸å‡ºï¼‰
    - answer_complete: ç­”æ¡ˆå®Œæˆ
    - metadata: è™•ç†å…ƒæ•¸æ“š
    - done: å…¨éƒ¨å®Œæˆ

    å®¢æˆ¶ç«¯ç¯„ä¾‹:
    ```javascript
    const eventSource = new EventSource('/api/v1/chat/stream');

    eventSource.addEventListener('answer_chunk', (event) => {
        const data = JSON.parse(event.data);
        answerDiv.textContent += data.chunk;
    });

    eventSource.addEventListener('done', (event) => {
        eventSource.close();
    });
    ```
    """
    return StreamingResponse(
        chat_stream_generator(request, req.app.state),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ nginx ç·©è¡
        }
    )


# æ¸¬è©¦ç«¯é»
@router.get("/chat/stream/test")
async def test_stream():
    """æ¸¬è©¦ SSE é€£æ¥"""
    async def generate():
        for i in range(5):
            yield f"data: {{\"message\": \"æ¸¬è©¦è¨Šæ¯ {i+1}\"}}\n\n"
            await asyncio.sleep(0.5)
        yield "data: {\"message\": \"å®Œæˆ\"}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
