"""
Streaming Chat API
æä¾› Server-Sent Events (SSE) æµå¼è¼¸å‡º
Phase 3: ç”¨æˆ¶é«”é©—å„ªåŒ– - å³æ™‚åé¥‹
"""
from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, Dict
import json
import asyncio
import time

router = APIRouter()


class ChatStreamRequest(BaseModel):
    """æµå¼èŠå¤©è«‹æ±‚"""
    question: str = Field(..., min_length=1, max_length=1000)
    vendor_id: int = Field(..., ge=1)
    user_role: str = Field(..., description="customer æˆ– staff")
    user_id: Optional[str] = None
    context: Optional[Dict] = None


async def generate_sse_event(event_type: str, data: dict) -> str:
    """ç”Ÿæˆ SSE æ ¼å¼çš„äº‹ä»¶"""
    return f"event: {event_type}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


async def chat_stream_generator(
    request: ChatStreamRequest,
    app_state
):
    """
    æµå¼èŠå¤©ç”Ÿæˆå™¨

    è¼¸å‡ºäº‹ä»¶é¡å‹:
    - start: é–‹å§‹è™•ç†
    - intent: æ„åœ–åˆ†é¡å®Œæˆ
    - search: æª¢ç´¢å®Œæˆ
    - confidence: ä¿¡å¿ƒåº¦è©•ä¼°å®Œæˆ
    - answer_chunk: ç­”æ¡ˆç‰‡æ®µï¼ˆé€å­—è¼¸å‡ºï¼‰
    - answer_complete: ç­”æ¡ˆå®Œæˆ
    - metadata: è™•ç†å…ƒæ•¸æ“š
    - done: å…¨éƒ¨å®Œæˆ
    """
    start_time = time.time()

    try:
        # 1. ç™¼é€é–‹å§‹äº‹ä»¶
        yield await generate_sse_event("start", {
            "message": "é–‹å§‹è™•ç†å•é¡Œ...",
            "question": request.question
        })

        # ç²å–æœå‹™å¯¦ä¾‹
        intent_classifier = app_state.intent_classifier
        rag_engine = app_state.rag_engine
        confidence_evaluator = app_state.confidence_evaluator
        llm_optimizer = app_state.llm_answer_optimizer
        vendor_config_service = app_state.vendor_config_service

        from services.business_scope_utils import get_allowed_audiences_for_scope
        business_scope = "external" if request.user_role == "customer" else "internal"
        allowed_audiences = get_allowed_audiences_for_scope(business_scope)

        # 1.5 å±¤ç´š2å„ªå…ˆï¼šæª¢æŸ¥æ˜¯å¦ç‚ºåƒæ•¸å‹å•é¡Œï¼ˆVendor Configs æ•´åˆï¼‰
        param_category = vendor_config_service.is_param_question(request.question)
        if param_category:
            param_answer = await vendor_config_service.create_param_answer(
                vendor_id=request.vendor_id,
                question=request.question,
                param_category=param_category
            )

            if param_answer:
                # åƒæ•¸å‹å•é¡Œç›´æ¥å›ç­”
                yield await generate_sse_event("config_param", {
                    "category": param_category,
                    "config_used": param_answer.get('config_used', {})
                })

                # æµå¼è¼¸å‡ºç­”æ¡ˆ
                answer = param_answer['answer']
                words = answer.split()
                for i, word in enumerate(words):
                    chunk = word + (" " if i < len(words) - 1 else "")
                    yield await generate_sse_event("answer_chunk", {"chunk": chunk})
                    await asyncio.sleep(0.02)

                # å®Œæˆäº‹ä»¶
                processing_time = int((time.time() - start_time) * 1000)
                yield await generate_sse_event("answer_complete", {
                    "processing_time_ms": processing_time
                })
                yield await generate_sse_event("metadata", {
                    "source": "vendor_config",
                    "confidence_score": 1.0,
                    "confidence_level": "high",
                    "processing_time_ms": processing_time
                })
                yield await generate_sse_event("done", {
                    "message": "è™•ç†å®Œæˆï¼ˆåƒæ•¸å‹ç­”æ¡ˆï¼‰",
                    "success": True
                })
                return

        # 2. ä¸¦è¡ŒåŸ·è¡Œæ„åœ–åˆ†é¡å’Œæª¢ç´¢ï¼ˆPhase 2 å„ªåŒ–ï¼‰
        intent_task = asyncio.to_thread(intent_classifier.classify, request.question)
        search_task = rag_engine.search(
            query=request.question,
            limit=5,
            similarity_threshold=0.60,
            allowed_audiences=allowed_audiences
        )

        intent_result, search_results = await asyncio.gather(intent_task, search_task)

        # 3. ç™¼é€æ„åœ–åˆ†é¡çµæœ
        yield await generate_sse_event("intent", {
            "intent_type": intent_result['intent_type'],
            "intent_name": intent_result.get('intent_name', 'unknown'),
            "confidence": intent_result.get('confidence', 0)
        })

        # 4. ç™¼é€æª¢ç´¢çµæœ
        yield await generate_sse_event("search", {
            "doc_count": len(search_results),
            "has_results": len(search_results) > 0
        })

        # 5. ä¿¡å¿ƒåº¦è©•ä¼°
        evaluation = confidence_evaluator.evaluate(
            search_results=search_results,
            question_keywords=intent_result['keywords']
        )

        yield await generate_sse_event("confidence", {
            "score": evaluation['confidence_score'],
            "level": evaluation['confidence_level'],
            "decision": evaluation['decision']
        })

        # 6. æ ¹æ“šæ±ºç­–ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ streamingï¼‰
        if evaluation['decision'] == 'direct_answer' and search_results:
            # é«˜ä¿¡å¿ƒåº¦ï¼šä½¿ç”¨æ¢ä»¶å¼å„ªåŒ–ï¼ˆPhase 1ï¼‰
            async for chunk in stream_optimized_answer(
                request.question,
                search_results,
                evaluation,
                intent_result,
                llm_optimizer
            ):
                yield chunk

        elif evaluation['decision'] == 'needs_enhancement' and search_results:
            # ä¸­ç­‰ä¿¡å¿ƒåº¦
            async for chunk in stream_optimized_answer(
                request.question,
                search_results,
                evaluation,
                intent_result,
                llm_optimizer,
                add_warning=True
            ):
                yield chunk

        else:
            # ä½ä¿¡å¿ƒåº¦ï¼šunclear
            yield await generate_sse_event("answer_chunk", {
                "chunk": "æŠ±æ­‰ï¼Œæˆ‘å°é€™å€‹å•é¡Œä¸å¤ªç¢ºå®šå¦‚ä½•å›ç­”ã€‚\n\n"
            })
            yield await generate_sse_event("answer_chunk", {
                "chunk": "æ‚¨çš„å•é¡Œå·²ç¶“è¨˜éŒ„ä¸‹ä¾†ï¼Œæˆ‘å€‘æœƒç›¡å¿«è™•ç†ã€‚\n"
            })
            yield await generate_sse_event("answer_chunk", {
                "chunk": "å¦‚éœ€ç«‹å³å”åŠ©ï¼Œè«‹è¯ç¹«å®¢æœäººå“¡ã€‚"
            })

        # 7. ç™¼é€å®Œæˆäº‹ä»¶
        processing_time = int((time.time() - start_time) * 1000)
        yield await generate_sse_event("answer_complete", {
            "processing_time_ms": processing_time
        })

        # 8. ç™¼é€å…ƒæ•¸æ“š
        yield await generate_sse_event("metadata", {
            "confidence_score": evaluation['confidence_score'],
            "confidence_level": evaluation['confidence_level'],
            "intent_type": intent_result['intent_type'],
            "doc_count": len(search_results),
            "processing_time_ms": processing_time
        })

        # 9. ç™¼é€å®Œæˆæ¨™è¨˜
        yield await generate_sse_event("done", {
            "message": "è™•ç†å®Œæˆ",
            "success": True
        })

    except Exception as e:
        # éŒ¯èª¤è™•ç†
        yield await generate_sse_event("error", {
            "message": str(e),
            "type": type(e).__name__
        })
        yield await generate_sse_event("done", {
            "message": "è™•ç†å¤±æ•—",
            "success": False
        })


async def stream_optimized_answer(
    question: str,
    search_results: list,
    evaluation: dict,
    intent_result: dict,
    llm_optimizer,
    add_warning: bool = False
):
    """
    æµå¼è¼¸å‡ºå„ªåŒ–å¾Œçš„ç­”æ¡ˆ

    æ ¹æ“šä¿¡å¿ƒåº¦ä½¿ç”¨ä¸åŒçš„ç­–ç•¥ï¼ˆPhase 1 æ¢ä»¶å¼å„ªåŒ–ï¼‰:
    - æ¥µé«˜ä¿¡å¿ƒåº¦: å¿«é€Ÿè·¯å¾‘ï¼ˆé€å­—è¼¸å‡ºé å­˜ç­”æ¡ˆï¼‰
    - é«˜ä¿¡å¿ƒåº¦: æ¨¡æ¿æ ¼å¼åŒ–ï¼ˆé€å­—è¼¸å‡ºæ ¼å¼åŒ–ç­”æ¡ˆï¼‰
    - å…¶ä»–: LLM streamingï¼ˆOpenAI streaming APIï¼‰
    """
    confidence_score = evaluation['confidence_score']
    confidence_level = evaluation['confidence_level']

    # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å¿«é€Ÿè·¯å¾‘
    from services.answer_formatter import AnswerFormatter
    formatter = AnswerFormatter()

    if confidence_score >= 0.75 and formatter.is_content_complete(search_results[0]):
        # å¿«é€Ÿè·¯å¾‘ï¼šé€å­—è¼¸å‡ºé å­˜ç­”æ¡ˆ
        print(f"âš¡ [Streaming] å¿«é€Ÿè·¯å¾‘è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")
        result = formatter.format_simple_answer(search_results)
        answer = result['answer']

        # æ¨¡æ“¬é€å­—è¼¸å‡ºï¼ˆæå‡ç”¨æˆ¶é«”é©—ï¼‰
        words = answer.split()
        for i, word in enumerate(words):
            chunk = word + (" " if i < len(words) - 1 else "")
            yield await generate_sse_event("answer_chunk", {"chunk": chunk})
            await asyncio.sleep(0.02)  # 20ms å»¶é²ï¼Œæ¨¡æ“¬æ‰“å­—æ•ˆæœ

    elif 0.55 <= confidence_score < 0.75 and confidence_level in ['high', 'medium']:
        # æ¨¡æ¿æ ¼å¼åŒ–ï¼šé€å­—è¼¸å‡º
        print(f"ğŸ“‹ [Streaming] æ¨¡æ¿æ ¼å¼åŒ–è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f}, ç´šåˆ¥: {confidence_level})")
        result = formatter.format_with_template(
            question,
            search_results,
            intent_type=intent_result.get('intent_type')
        )
        answer = result['answer']

        # é€å­—è¼¸å‡º
        words = answer.split()
        print(f"ğŸ“¤ [Streaming] æº–å‚™è¼¸å‡º {len(words)} å€‹è©å¡Šï¼Œé è¨ˆè€—æ™‚ {len(words) * 20}ms")
        for i, word in enumerate(words):
            chunk = word + (" " if i < len(words) - 1 else "")
            yield await generate_sse_event("answer_chunk", {"chunk": chunk})
            await asyncio.sleep(0.02)

    else:
        # å®Œæ•´ LLM å„ªåŒ– - ä½¿ç”¨ OpenAI Streaming API
        print(f"ğŸ¤– [Streaming] LLM ä¸²æµè§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f}, ç´šåˆ¥: {confidence_level})")
        import os
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # æº–å‚™ prompt
        content = search_results[0].get('content', '') if search_results else ''
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹çŸ¥è­˜å›ç­”ç”¨æˆ¶å•é¡Œã€‚

ç”¨æˆ¶å•é¡Œ: {question}

ç›¸é—œçŸ¥è­˜:
{content}

è«‹æä¾›æ¸…æ™°ã€æº–ç¢ºçš„ç­”æ¡ˆã€‚"""

        # ä½¿ç”¨ streaming API
        stream = await client.chat.completions.create(
            model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å®¢æœåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            stream=True,
            temperature=0.7,
            max_tokens=800
        )

        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield await generate_sse_event("answer_chunk", {
                    "chunk": chunk.choices[0].delta.content
                })

    # å¦‚æœéœ€è¦æ·»åŠ è­¦å‘Š
    if add_warning:
        yield await generate_sse_event("answer_chunk", {
            "chunk": f"\n\nâš ï¸ æ³¨æ„ï¼šæ­¤ç­”æ¡ˆä¿¡å¿ƒåº¦ç‚ºä¸­ç­‰ï¼ˆ{confidence_score:.2f}ï¼‰ï¼Œå»ºè­°æ‚¨è¯ç¹«å®¢æœäººå“¡é€²ä¸€æ­¥ç¢ºèªã€‚"
        })


@router.post("/chat/stream")
async def chat_stream(request: ChatStreamRequest, req: Request):
    """
    æµå¼èŠå¤© API (SSE)

    è¿”å› Server-Sent Events æµï¼ŒåŒ…å«ä»¥ä¸‹äº‹ä»¶:
    - start: é–‹å§‹è™•ç†
    - intent: æ„åœ–åˆ†é¡
    - search: æª¢ç´¢çµæœ
    - confidence: ä¿¡å¿ƒåº¦è©•ä¼°
    - answer_chunk: ç­”æ¡ˆç‰‡æ®µï¼ˆé€å­—è¼¸å‡ºï¼‰
    - answer_complete: ç­”æ¡ˆå®Œæˆ
    - metadata: è™•ç†å…ƒæ•¸æ“š
    - done: å…¨éƒ¨å®Œæˆ

    å®¢æˆ¶ç«¯ç¯„ä¾‹:
    ```javascript
    const eventSource = new EventSource('/api/v1/chat/stream');

    eventSource.addEventListener('answer_chunk', (event) => {
        const data = JSON.parse(event.data);
        answerDiv.textContent += data.chunk;
    });

    eventSource.addEventListener('done', (event) => {
        eventSource.close();
    });
    ```
    """
    return StreamingResponse(
        chat_stream_generator(request, req.app.state),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ nginx ç·©è¡
        }
    )


# æ¸¬è©¦ç«¯é»
@router.get("/chat/stream/test")
async def test_stream():
    """æ¸¬è©¦ SSE é€£æ¥"""
    async def generate():
        for i in range(5):
            yield f"data: {{\"message\": \"æ¸¬è©¦è¨Šæ¯ {i+1}\"}}\n\n"
            await asyncio.sleep(0.5)
        yield "data: {\"message\": \"å®Œæˆ\"}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream"
    )
