"""
çŸ¥è­˜åº«åŒ¯å‡º API
æ”¯æ´åŒ¯å‡ºçŸ¥è­˜åº«ç‚º Excel æ ¼å¼ï¼ˆæ¨™æº–æ ¼å¼ï¼Œèˆ‡åŒ¯å…¥æ ¼å¼å…¼å®¹ï¼Œæ”¯æ´å¤§é‡è³‡æ–™åˆ†æ‰¹è™•ç†ï¼‰
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Request, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Optional
import uuid
from datetime import datetime
from pathlib import Path
import os
import json
from urllib.parse import quote

router = APIRouter(prefix="/api/v1/knowledge-export", tags=["Knowledge Export"])


class ExportJobStatus(BaseModel):
    """åŒ¯å‡ºä»»å‹™ç‹€æ…‹"""
    job_id: str
    status: str  # pending, processing, completed, failed
    progress: Optional[Dict] = None  # {current: 5000, total: 10000, percentage: 50}
    result: Optional[Dict] = None  # {exported: 10000, file_size_kb: 1234, file_path: "..."}
    error: Optional[str] = None
    vendor_id: Optional[int] = None
    export_mode: Optional[str] = None  # çµ±ä¸€ä½¿ç”¨ standard
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


class ExportRequest(BaseModel):
    """åŒ¯å‡ºè«‹æ±‚"""
    vendor_id: Optional[int] = None
    export_mode: str = "standard"  # çµ±ä¸€ä½¿ç”¨ standard æ¨™æº–æ ¼å¼
    include_intents: bool = False  # ä¿ç•™åƒæ•¸ä½†ä¸ä½¿ç”¨ï¼ˆæ¨™æº–æ ¼å¼ä¸éœ€è¦å¤šå·¥ä½œè¡¨ï¼‰
    include_metadata: bool = False  # ä¿ç•™åƒæ•¸ä½†ä¸ä½¿ç”¨ï¼ˆæ¨™æº–æ ¼å¼ä¸éœ€è¦å¤šå·¥ä½œè¡¨ï¼‰


@router.post("/export")
async def create_export_job(
    request: Request,
    background_tasks: BackgroundTasks,
    export_request: ExportRequest
):
    """
    å‰µå»ºåŒ¯å‡ºä»»å‹™

    çµ±ä¸€ä½¿ç”¨æ¨™æº–åŒ¯å‡ºæ ¼å¼ï¼š
    - èˆ‡åŒ¯å…¥æ ¼å¼å®Œå…¨å…¼å®¹
    - æ”¯æ´å¤§é‡è³‡æ–™åˆ†æ‰¹è™•ç†ï¼ˆ10è¬+ ç­†ï¼‰
    - å–®å·¥ä½œè¡¨ï¼Œæ¨™æº–æ¬„ä½çµæ§‹
    - åŒ¯å‡ºçš„æª”æ¡ˆå¯ç›´æ¥ç”¨æ–¼åŒ¯å…¥åŠŸèƒ½

    Args:
        export_request: åŒ¯å‡ºè«‹æ±‚åƒæ•¸
            - vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œç•™ç©ºè¡¨ç¤ºåŒ¯å‡ºé€šç”¨çŸ¥è­˜ï¼‰
            - export_mode: åŒ¯å‡ºæ¨¡å¼ï¼ˆçµ±ä¸€ä½¿ç”¨ 'standard'ï¼‰

    Returns:
        Dict: åŒ…å« job_id çš„å›æ‡‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ æ”¶åˆ°åŒ¯å‡ºè«‹æ±‚")
    print(f"   æ¥­è€… ID: {export_request.vendor_id or 'é€šç”¨çŸ¥è­˜'}")
    print(f"   åŒ¯å‡ºæ ¼å¼: æ¨™æº–æ ¼å¼ï¼ˆèˆ‡åŒ¯å…¥å…¼å®¹ï¼‰")
    print(f"{'='*60}\n")

    # 1. é©—è­‰åŒ¯å‡ºæ¨¡å¼ï¼ˆåªå…è¨± standardï¼‰
    if export_request.export_mode != 'standard':
        # è‡ªå‹•è½‰æ›ç‚º standardï¼ˆå‘å¾Œå…¼å®¹ï¼‰
        print(f"âš ï¸  åŒ¯å‡ºæ¨¡å¼ '{export_request.export_mode}' å·²æ£„ç”¨ï¼Œè‡ªå‹•è½‰æ›ç‚º 'standard'")
        export_request.export_mode = 'standard'

    # 2. é©—è­‰æ¥­è€… IDï¼ˆå¦‚æœæä¾›ï¼‰
    db_pool = request.app.state.db_pool
    if export_request.vendor_id:
        async with db_pool.acquire() as conn:
            vendor_exists = await conn.fetchval(
                "SELECT EXISTS(SELECT 1 FROM vendors WHERE id = $1)",
                export_request.vendor_id
            )
            if not vendor_exists:
                raise HTTPException(status_code=404, detail=f"æ¥­è€… ID {export_request.vendor_id} ä¸å­˜åœ¨")

    # 3. ä½¿ç”¨çµ±ä¸€ Job æœå‹™å‰µå»ºä½œæ¥­è¨˜éŒ„
    from services.knowledge_export_service import KnowledgeExportService
    service = KnowledgeExportService(db_pool)

    job_id = await service.create_job(
        job_type='knowledge_export',
        vendor_id=export_request.vendor_id,
        user_id="admin",  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
        job_config={
            'export_mode': export_request.export_mode,
            'include_intents': export_request.include_intents,
            'include_metadata': export_request.include_metadata
        }
    )

    print(f"âœ… åŒ¯å‡ºä½œæ¥­å·²å»ºç«‹ (job_id: {job_id})")

    # 4. å•Ÿå‹•èƒŒæ™¯ä»»å‹™
    print(f"ğŸš€ å•Ÿå‹•èƒŒæ™¯åŒ¯å‡ºä»»å‹™ (job_id: {job_id})")

    background_tasks.add_task(
        service.process_export_job,
        job_id=job_id,
        vendor_id=export_request.vendor_id,
        export_mode=export_request.export_mode,
        include_intents=export_request.include_intents,
        include_metadata=export_request.include_metadata,
        user_id="admin"  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
    )

    return {
        "job_id": job_id,
        "status": "processing",
        "message": "åŒ¯å‡ºä»»å‹™å·²å»ºç«‹ï¼Œé–‹å§‹è™•ç†ä¸­ã€‚ä½¿ç”¨æ¨™æº–æ ¼å¼ï¼ˆèˆ‡åŒ¯å…¥æ ¼å¼å…¼å®¹ï¼Œæ”¯æ´å¤§é‡è³‡æ–™åˆ†æ‰¹è™•ç†ï¼‰",
        "export_mode": "standard",
        "vendor_id": export_request.vendor_id
    }


@router.get("/jobs/{job_id}")
async def get_export_job_status(job_id: str, request: Request):
    """
    ç²å–åŒ¯å‡ºä»»å‹™ç‹€æ…‹ï¼ˆä¾›å‰ç«¯è¼ªè©¢ï¼‰

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        ExportJobStatus: ä»»å‹™ç‹€æ…‹
    """
    from services.knowledge_export_service import KnowledgeExportService
    db_pool = request.app.state.db_pool
    service = KnowledgeExportService(db_pool)

    job = await service.get_job(job_id)

    if not job:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    # çµ±ä¸€è¿”å› standard æ¨¡å¼
    return {
        "job_id": str(job['job_id']),
        "status": job['status'],
        "progress": job.get('progress'),
        "result": job.get('result'),
        "error": job.get('error_message'),
        "vendor_id": job.get('vendor_id'),
        "export_mode": "standard",  # çµ±ä¸€ä½¿ç”¨æ¨™æº–æ ¼å¼
        "created_at": job['created_at'],
        "updated_at": job['updated_at'],
        "completed_at": job.get('completed_at')
    }


@router.get("/jobs/{job_id}/download")
async def download_export_file(job_id: str, request: Request):
    """
    ä¸‹è¼‰åŒ¯å‡ºçš„ Excel æª”æ¡ˆ

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        FileResponse: Excel æª”æ¡ˆ
    """
    from services.knowledge_export_service import KnowledgeExportService
    db_pool = request.app.state.db_pool
    service = KnowledgeExportService(db_pool)

    job = await service.get_job(job_id)

    if not job:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    if job['status'] != 'completed':
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç‹€æ…‹: {job['status']}ï¼‰ï¼Œç„¡æ³•ä¸‹è¼‰"
        )

    # å¾å¤šå€‹ä¾†æºå˜—è©¦å–å¾—æª”æ¡ˆè·¯å¾‘
    # å„ªå…ˆé †åº: 1. ç›´æ¥çš„ file_path æ¬„ä½  2. result ä¸­çš„ file_path
    file_path = job.get('file_path')
    if not file_path:
        result = job.get('result', {})
        if isinstance(result, dict):
            file_path = result.get('file_path')

    print(f"ğŸ” æª¢æŸ¥æª”æ¡ˆè·¯å¾‘: {file_path}")
    print(f"   Job è³‡æ–™: status={job['status']}, file_pathæ¬„ä½={job.get('file_path')}, result={job.get('result')}")

    if not file_path:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°åŒ¯å‡ºæª”æ¡ˆè·¯å¾‘")

    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail=f"åŒ¯å‡ºæª”æ¡ˆä¸å­˜åœ¨æ–¼è·¯å¾‘: {file_path}")

    # ç”Ÿæˆä¸‹è¼‰æª”å
    vendor_id = job.get('vendor_id')
    vendor_name = "é€šç”¨çŸ¥è­˜" if vendor_id is None else f"æ¥­è€…{vendor_id}"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    download_filename = f"çŸ¥è­˜åº«åŒ¯å‡º_{vendor_name}_{timestamp}.xlsx"

    # URL ç·¨ç¢¼æª”åï¼ˆæ”¯æ´ä¸­æ–‡ï¼‰
    encoded_filename = quote(download_filename)

    print(f"ğŸ“¥ ä¸‹è¼‰åŒ¯å‡ºæª”æ¡ˆ: {download_filename} (job_id: {job_id})")

    # ä½¿ç”¨æ¨™æº–çš„ Content-Disposition æ ¼å¼ï¼ŒåŒæ™‚æä¾› filename å’Œ filename* ä»¥æé«˜ç›¸å®¹æ€§
    return FileResponse(
        path=file_path,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        filename=download_filename,
        headers={
            "Content-Disposition": f'attachment; filename="knowledge_export.xlsx"; filename*=UTF-8\'\'{encoded_filename}'
        }
    )


@router.get("/jobs")
async def list_export_jobs(
    request: Request,
    vendor_id: Optional[int] = Query(None),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """
    åˆ—å‡ºåŒ¯å‡ºä»»å‹™æ­·å²

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œéæ¿¾ç‰¹å®šæ¥­è€…ï¼‰
        limit: è¿”å›æ•¸é‡é™åˆ¶ï¼ˆ1-100ï¼‰
        offset: åç§»é‡

    Returns:
        Dict: ä»»å‹™åˆ—è¡¨
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        if vendor_id is not None:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    job_config,
                    status,
                    progress,
                    success_records,
                    file_size_bytes,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_export'
                ORDER BY created_at DESC
                LIMIT $2 OFFSET $3
            """, vendor_id, limit, offset)

            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_export'
            """, vendor_id)
        else:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    job_config,
                    status,
                    progress,
                    success_records,
                    file_size_bytes,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                ORDER BY created_at DESC
                LIMIT $1 OFFSET $2
            """, limit, offset)

            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs WHERE job_type = 'knowledge_export'
            """)

        return {
            "jobs": [
                {
                    "job_id": str(job['job_id']),
                    "vendor_id": job['vendor_id'],
                    "export_mode": "standard",  # çµ±ä¸€ä½¿ç”¨æ¨™æº–æ ¼å¼
                    "status": job['status'],
                    "progress": (json.loads(job['progress']) if isinstance(job['progress'], str) else job['progress']).get('current', 0) if job['progress'] else 0,
                    "exported_count": job['success_records'],
                    "file_size_kb": round(job['file_size_bytes'] / 1024, 2) if job['file_size_bytes'] else None,
                    "created_at": job['created_at'].isoformat() if job['created_at'] else None,
                    "completed_at": job['completed_at'].isoformat() if job['completed_at'] else None
                }
                for job in jobs
            ],
            "total": total,
            "limit": limit,
            "offset": offset
        }


@router.delete("/jobs/{job_id}")
async def delete_export_job(job_id: str, request: Request):
    """
    åˆªé™¤åŒ¯å‡ºä»»å‹™è¨˜éŒ„èˆ‡æª”æ¡ˆ

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        Dict: åˆªé™¤çµæœ
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # å–å¾—æª”æ¡ˆè·¯å¾‘
        job = await conn.fetchrow("""
            SELECT file_path FROM unified_jobs WHERE job_id = $1
        """, uuid.UUID(job_id))

        if not job:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # åˆªé™¤å¯¦é«”æª”æ¡ˆ
        file_path = job['file_path']
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
                print(f"âœ… å·²åˆªé™¤åŒ¯å‡ºæª”æ¡ˆ: {file_path}")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•åˆªé™¤æª”æ¡ˆ: {e}")

        # åˆªé™¤è³‡æ–™åº«è¨˜éŒ„
        deleted = await conn.fetchval("""
            DELETE FROM unified_jobs
            WHERE job_id = $1
            RETURNING job_id
        """, uuid.UUID(job_id))

    return {
        "message": "åŒ¯å‡ºä»»å‹™å·²åˆªé™¤",
        "job_id": job_id
    }


@router.get("/statistics")
async def get_export_statistics(
    request: Request,
    vendor_id: Optional[int] = Query(None),
    days: int = Query(30, ge=1, le=365)
):
    """
    å–å¾—åŒ¯å‡ºçµ±è¨ˆè³‡è¨Š

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
        days: çµ±è¨ˆå¤©æ•¸ï¼ˆ1-365ï¼‰

    Returns:
        Dict: çµ±è¨ˆè³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # åŸºç¤çµ±è¨ˆ
        if vendor_id is not None:
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_jobs,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_jobs,
                    COUNT(*) FILTER (WHERE status = 'processing') as processing_jobs,
                    COALESCE(SUM(success_records), 0) as total_exported,
                    COALESCE(SUM(file_size_bytes), 0) as total_file_size,
                    COALESCE(AVG(success_records) FILTER (WHERE status = 'completed'), 0) as avg_exported_per_job
                FROM unified_jobs
                WHERE vendor_id = $1
                  AND job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - CAST($2 || ' days' AS INTERVAL)
            """, vendor_id, str(days))
        else:
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_jobs,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_jobs,
                    COUNT(*) FILTER (WHERE status = 'processing') as processing_jobs,
                    COALESCE(SUM(success_records), 0) as total_exported,
                    COALESCE(SUM(file_size_bytes), 0) as total_file_size,
                    COALESCE(AVG(success_records) FILTER (WHERE status = 'completed'), 0) as avg_exported_per_job
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - CAST($1 || ' days' AS INTERVAL)
            """, str(days))

        # è¨ˆç®—æˆåŠŸç‡
        total_jobs = stats['total_jobs'] or 0
        completed_jobs = stats['completed_jobs'] or 0
        success_rate = (completed_jobs / total_jobs * 100) if total_jobs > 0 else 0

        # æ¨¡å¼çµ±è¨ˆ
        if vendor_id is not None:
            mode_stats = await conn.fetch("""
                SELECT
                    job_config->>'export_mode' as export_mode,
                    COUNT(*) as count,
                    COALESCE(AVG(success_records), 0) as avg_exported
                FROM unified_jobs
                WHERE vendor_id = $1
                  AND job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - CAST($2 || ' days' AS INTERVAL)
                  AND status = 'completed'
                GROUP BY job_config->>'export_mode'
            """, vendor_id, str(days))
        else:
            mode_stats = await conn.fetch("""
                SELECT
                    job_config->>'export_mode' as export_mode,
                    COUNT(*) as count,
                    COALESCE(AVG(success_records), 0) as avg_exported
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - CAST($1 || ' days' AS INTERVAL)
                  AND status = 'completed'
                GROUP BY job_config->>'export_mode'
            """, str(days))

        return {
            "total_jobs": total_jobs,
            "completed_jobs": completed_jobs,
            "failed_jobs": stats['failed_jobs'],
            "processing_jobs": stats['processing_jobs'],
            "total_exported": stats['total_exported'],
            "total_file_size_mb": round(stats['total_file_size'] / 1024 / 1024, 2),
            "avg_exported_per_job": round(float(stats['avg_exported_per_job']), 2),
            "success_rate": round(success_rate, 2),
            "mode_statistics": [
                {
                    "mode": mode['export_mode'],
                    "count": mode['count'],
                    "avg_exported": round(float(mode['avg_exported']), 2)
                }
                for mode in mode_stats
            ],
            "days": days
        }


@router.get("/preview")
async def preview_export(
    request: Request,
    vendor_id: Optional[int] = Query(None)
):
    """
    é è¦½åŒ¯å‡ºè³‡æ–™ï¼ˆä¸å¯¦éš›åŒ¯å‡ºï¼Œåªè¿”å›çµ±è¨ˆè³‡è¨Šï¼‰

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰

    Returns:
        Dict: é è¦½è³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # çµ±è¨ˆçŸ¥è­˜æ•¸é‡
        if vendor_id is not None:
            knowledge_count = await conn.fetchval("""
                SELECT COUNT(*) FROM knowledge_base WHERE vendor_id = $1
            """, vendor_id)

            intent_count = await conn.fetchval("""
                SELECT COUNT(*) FROM intents WHERE is_enabled = TRUE
            """)
        else:
            knowledge_count = await conn.fetchval("""
                SELECT COUNT(*) FROM knowledge_base WHERE vendor_id IS NULL
            """)

            intent_count = await conn.fetchval("""
                SELECT COUNT(*) FROM intents WHERE is_enabled = TRUE
            """)

        # ä¼°ç®—æª”æ¡ˆå¤§å°ï¼ˆç²—ç•¥è¨ˆç®—ï¼‰
        # å‡è¨­æ¯ç­†çŸ¥è­˜ç´„ 500 bytesï¼ŒåŠ ä¸Šæ ¼å¼åŒ–é–‹éŠ·
        estimated_size_kb = (knowledge_count * 0.5) + (intent_count * 0.2)

        # æä¾›è³‡æ–™é‡èªªæ˜
        if knowledge_count < 10000:
            data_size_info = "è³‡æ–™é‡è¼ƒå°ï¼ŒåŒ¯å‡ºé€Ÿåº¦å¿«"
        elif knowledge_count < 50000:
            data_size_info = "è³‡æ–™é‡é©ä¸­ï¼ŒåŒ¯å‡ºæ™‚é–“ç´„æ•¸ç§’"
        else:
            data_size_info = "è³‡æ–™é‡è¼ƒå¤§ï¼Œå°‡ä½¿ç”¨åˆ†æ‰¹è™•ç†ç¢ºä¿æ•ˆèƒ½"

        return {
            "knowledge_count": knowledge_count,
            "intent_count": intent_count,
            "estimated_file_size_kb": round(estimated_size_kb, 2),
            "estimated_file_size_mb": round(estimated_size_kb / 1024, 2),
            "export_mode": "standard",
            "data_size_info": data_size_info,
            "vendor_id": vendor_id,
            "message": "é€™æ˜¯é è¦½æ¨¡å¼ï¼Œå°šæœªå¯¦éš›åŸ·è¡ŒåŒ¯å‡ºã€‚çµ±ä¸€ä½¿ç”¨æ¨™æº–æ ¼å¼ï¼ˆèˆ‡åŒ¯å…¥å…¼å®¹ï¼Œæ”¯æ´å¤§é‡è³‡æ–™åˆ†æ‰¹è™•ç†ï¼‰"
        }
