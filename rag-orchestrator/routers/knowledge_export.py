"""
çŸ¥è­˜åº«åŒ¯å‡º API
æ”¯æ´åŒ¯å‡ºçŸ¥è­˜åº«ç‚º Excel æ ¼å¼ï¼ˆåŸºç¤ã€é€²éšæ ¼å¼åŒ–ã€æ•ˆèƒ½å„ªåŒ–ä¸‰ç¨®æ¨¡å¼ï¼‰
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks, Request, Query
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import List, Dict, Optional
import uuid
from datetime import datetime
from pathlib import Path
import os

router = APIRouter(prefix="/api/v1/knowledge-export", tags=["Knowledge Export"])


class ExportJobStatus(BaseModel):
    """åŒ¯å‡ºä»»å‹™ç‹€æ…‹"""
    job_id: str
    status: str  # pending, processing, completed, failed
    progress: Optional[Dict] = None  # {current: 5000, total: 10000, percentage: 50}
    result: Optional[Dict] = None  # {exported: 10000, file_size_kb: 1234, file_path: "..."}
    error: Optional[str] = None
    vendor_id: Optional[int] = None
    export_mode: Optional[str] = None  # basic, formatted, optimized
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


class ExportRequest(BaseModel):
    """åŒ¯å‡ºè«‹æ±‚"""
    vendor_id: Optional[int] = None
    export_mode: str = "formatted"  # basic, formatted, optimized
    include_intents: bool = True  # æ˜¯å¦åŒ…å«æ„åœ–å°ç…§è¡¨
    include_metadata: bool = True  # æ˜¯å¦åŒ…å«åŒ¯å‡ºè³‡è¨Šå·¥ä½œè¡¨


@router.post("/export")
async def create_export_job(
    request: Request,
    background_tasks: BackgroundTasks,
    export_request: ExportRequest
):
    """
    å‰µå»ºåŒ¯å‡ºä»»å‹™

    æ”¯æ´ä¸‰ç¨®åŒ¯å‡ºæ¨¡å¼ï¼š
    1. basic: åŸºç¤åŒ¯å‡ºï¼ˆå–®å·¥ä½œè¡¨ï¼ŒåŸºæœ¬æ ¼å¼ï¼‰
    2. formatted: é€²éšæ ¼å¼åŒ–ï¼ˆå¤šå·¥ä½œè¡¨ï¼Œå°ˆæ¥­æ ¼å¼ï¼Œæ¨è–¦ä½¿ç”¨ï¼‰
    3. optimized: æ•ˆèƒ½å„ªåŒ–ï¼ˆæ”¯æ´ 10 è¬+ ç­†è³‡æ–™ï¼Œåˆ†æ‰¹è™•ç†ï¼‰

    Args:
        export_request: åŒ¯å‡ºè«‹æ±‚åƒæ•¸
            - vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œç•™ç©ºè¡¨ç¤ºåŒ¯å‡ºé€šç”¨çŸ¥è­˜ï¼‰
            - export_mode: åŒ¯å‡ºæ¨¡å¼ï¼ˆbasic, formatted, optimizedï¼‰
            - include_intents: æ˜¯å¦åŒ…å«æ„åœ–å°ç…§è¡¨
            - include_metadata: æ˜¯å¦åŒ…å«åŒ¯å‡ºè³‡è¨Š

    Returns:
        Dict: åŒ…å« job_id çš„å›æ‡‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ æ”¶åˆ°åŒ¯å‡ºè«‹æ±‚")
    print(f"   æ¥­è€… ID: {export_request.vendor_id or 'é€šç”¨çŸ¥è­˜'}")
    print(f"   åŒ¯å‡ºæ¨¡å¼: {export_request.export_mode}")
    print(f"   åŒ…å«æ„åœ–: {export_request.include_intents}")
    print(f"   åŒ…å«è³‡è¨Š: {export_request.include_metadata}")
    print(f"{'='*60}\n")

    # 1. é©—è­‰åŒ¯å‡ºæ¨¡å¼
    allowed_modes = ['basic', 'formatted', 'optimized']
    if export_request.export_mode not in allowed_modes:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æ´çš„åŒ¯å‡ºæ¨¡å¼: {export_request.export_mode}. æ”¯æ´çš„æ¨¡å¼: {', '.join(allowed_modes)}"
        )

    # 2. é©—è­‰æ¥­è€… IDï¼ˆå¦‚æœæä¾›ï¼‰
    db_pool = request.app.state.db_pool
    if export_request.vendor_id:
        async with db_pool.acquire() as conn:
            vendor_exists = await conn.fetchval(
                "SELECT EXISTS(SELECT 1 FROM vendors WHERE id = $1)",
                export_request.vendor_id
            )
            if not vendor_exists:
                raise HTTPException(status_code=404, detail=f"æ¥­è€… ID {export_request.vendor_id} ä¸å­˜åœ¨")

    # 3. ä½¿ç”¨çµ±ä¸€ Job æœå‹™å‰µå»ºä½œæ¥­è¨˜éŒ„
    from services.knowledge_export_service import KnowledgeExportService
    service = KnowledgeExportService(db_pool)

    job_id = await service.create_job(
        job_type='knowledge_export',
        vendor_id=export_request.vendor_id,
        user_id="admin",  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
        job_config={
            'export_mode': export_request.export_mode,
            'include_intents': export_request.include_intents,
            'include_metadata': export_request.include_metadata
        }
    )

    print(f"âœ… åŒ¯å‡ºä½œæ¥­å·²å»ºç«‹ (job_id: {job_id})")

    # 4. å•Ÿå‹•èƒŒæ™¯ä»»å‹™
    print(f"ğŸš€ å•Ÿå‹•èƒŒæ™¯åŒ¯å‡ºä»»å‹™ (job_id: {job_id})")

    background_tasks.add_task(
        service.process_export_job,
        job_id=job_id,
        vendor_id=export_request.vendor_id,
        export_mode=export_request.export_mode,
        include_intents=export_request.include_intents,
        include_metadata=export_request.include_metadata,
        user_id="admin"  # TODO: å¾èªè­‰å–å¾—çœŸå¯¦ä½¿ç”¨è€… ID
    )

    # æ ¹æ“šæ¨¡å¼è¿”å›ä¸åŒè¨Šæ¯
    mode_descriptions = {
        'basic': 'åŸºç¤åŒ¯å‡ºæ¨¡å¼ï¼ˆå–®å·¥ä½œè¡¨ï¼Œå¿«é€ŸåŒ¯å‡ºï¼‰',
        'formatted': 'é€²éšæ ¼å¼åŒ–æ¨¡å¼ï¼ˆå¤šå·¥ä½œè¡¨ï¼Œå°ˆæ¥­æ ¼å¼ï¼‰',
        'optimized': 'æ•ˆèƒ½å„ªåŒ–æ¨¡å¼ï¼ˆæ”¯æ´å¤§é‡è³‡æ–™ï¼Œåˆ†æ‰¹è™•ç†ï¼‰'
    }

    return {
        "job_id": job_id,
        "status": "processing",
        "message": f"åŒ¯å‡ºä»»å‹™å·²å»ºç«‹ï¼Œé–‹å§‹è™•ç†ä¸­ã€‚{mode_descriptions[export_request.export_mode]}",
        "export_mode": export_request.export_mode,
        "vendor_id": export_request.vendor_id
    }


@router.get("/jobs/{job_id}")
async def get_export_job_status(job_id: str, request: Request):
    """
    ç²å–åŒ¯å‡ºä»»å‹™ç‹€æ…‹ï¼ˆä¾›å‰ç«¯è¼ªè©¢ï¼‰

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        ExportJobStatus: ä»»å‹™ç‹€æ…‹
    """
    from services.knowledge_export_service import KnowledgeExportService
    db_pool = request.app.state.db_pool
    service = KnowledgeExportService(db_pool)

    job = await service.get_job(job_id)

    if not job:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    # å¾ config ä¸­æå– export_mode
    export_mode = job.get('config', {}).get('export_mode', 'formatted')

    return {
        "job_id": str(job['job_id']),
        "status": job['status'],
        "progress": job.get('progress'),
        "result": job.get('result'),
        "error": job.get('error_message'),
        "vendor_id": job.get('vendor_id'),
        "export_mode": export_mode,
        "created_at": job['created_at'],
        "updated_at": job['updated_at'],
        "completed_at": job.get('completed_at')
    }


@router.get("/jobs/{job_id}/download")
async def download_export_file(job_id: str, request: Request):
    """
    ä¸‹è¼‰åŒ¯å‡ºçš„ Excel æª”æ¡ˆ

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        FileResponse: Excel æª”æ¡ˆ
    """
    from services.knowledge_export_service import KnowledgeExportService
    db_pool = request.app.state.db_pool
    service = KnowledgeExportService(db_pool)

    job = await service.get_job(job_id)

    if not job:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

    if job['status'] != 'completed':
        raise HTTPException(
            status_code=400,
            detail=f"ä»»å‹™å°šæœªå®Œæˆï¼ˆç‹€æ…‹: {job['status']}ï¼‰ï¼Œç„¡æ³•ä¸‹è¼‰"
        )

    # å¾ result å–å¾—æª”æ¡ˆè·¯å¾‘
    result = job.get('result', {})
    file_path = result.get('file_path')

    if not file_path or not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="åŒ¯å‡ºæª”æ¡ˆä¸å­˜åœ¨")

    # ç”Ÿæˆä¸‹è¼‰æª”å
    vendor_id = job.get('vendor_id')
    vendor_name = "é€šç”¨çŸ¥è­˜" if vendor_id is None else f"æ¥­è€…{vendor_id}"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    download_filename = f"çŸ¥è­˜åº«åŒ¯å‡º_{vendor_name}_{timestamp}.xlsx"

    print(f"ğŸ“¥ ä¸‹è¼‰åŒ¯å‡ºæª”æ¡ˆ: {download_filename} (job_id: {job_id})")

    return FileResponse(
        path=file_path,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        filename=download_filename,
        headers={
            "Content-Disposition": f'attachment; filename="{download_filename}"'
        }
    )


@router.get("/jobs")
async def list_export_jobs(
    request: Request,
    vendor_id: Optional[int] = Query(None),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    """
    åˆ—å‡ºåŒ¯å‡ºä»»å‹™æ­·å²

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼Œéæ¿¾ç‰¹å®šæ¥­è€…ï¼‰
        limit: è¿”å›æ•¸é‡é™åˆ¶ï¼ˆ1-100ï¼‰
        offset: åç§»é‡

    Returns:
        Dict: ä»»å‹™åˆ—è¡¨
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        if vendor_id is not None:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    job_config,
                    status,
                    success_records,
                    file_size_bytes,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_export'
                ORDER BY created_at DESC
                LIMIT $2 OFFSET $3
            """, vendor_id, limit, offset)

            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs
                WHERE vendor_id = $1 AND job_type = 'knowledge_export'
            """, vendor_id)
        else:
            jobs = await conn.fetch("""
                SELECT
                    job_id,
                    vendor_id,
                    job_config,
                    status,
                    success_records,
                    file_size_bytes,
                    created_at,
                    completed_at
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                ORDER BY created_at DESC
                LIMIT $1 OFFSET $2
            """, limit, offset)

            total = await conn.fetchval("""
                SELECT COUNT(*) FROM unified_jobs WHERE job_type = 'knowledge_export'
            """)

        return {
            "jobs": [
                {
                    "job_id": str(job['job_id']),
                    "vendor_id": job['vendor_id'],
                    "export_mode": job['job_config'].get('export_mode', 'formatted') if job['job_config'] else 'formatted',
                    "status": job['status'],
                    "exported_count": job['success_records'],
                    "file_size_kb": round(job['file_size_bytes'] / 1024, 2) if job['file_size_bytes'] else None,
                    "created_at": job['created_at'].isoformat() if job['created_at'] else None,
                    "completed_at": job['completed_at'].isoformat() if job['completed_at'] else None
                }
                for job in jobs
            ],
            "total": total,
            "limit": limit,
            "offset": offset
        }


@router.delete("/jobs/{job_id}")
async def delete_export_job(job_id: str, request: Request):
    """
    åˆªé™¤åŒ¯å‡ºä»»å‹™è¨˜éŒ„èˆ‡æª”æ¡ˆ

    Args:
        job_id: ä»»å‹™ ID

    Returns:
        Dict: åˆªé™¤çµæœ
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # å–å¾—æª”æ¡ˆè·¯å¾‘
        job = await conn.fetchrow("""
            SELECT result FROM unified_jobs WHERE job_id = $1
        """, uuid.UUID(job_id))

        if not job:
            raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")

        # åˆªé™¤å¯¦é«”æª”æ¡ˆ
        if job['result']:
            import json
            result = json.loads(job['result'])
            file_path = result.get('file_path')
            if file_path and os.path.exists(file_path):
                try:
                    os.remove(file_path)
                    print(f"âœ… å·²åˆªé™¤åŒ¯å‡ºæª”æ¡ˆ: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ ç„¡æ³•åˆªé™¤æª”æ¡ˆ: {e}")

        # åˆªé™¤è³‡æ–™åº«è¨˜éŒ„
        deleted = await conn.fetchval("""
            DELETE FROM unified_jobs
            WHERE job_id = $1
            RETURNING job_id
        """, uuid.UUID(job_id))

    return {
        "message": "åŒ¯å‡ºä»»å‹™å·²åˆªé™¤",
        "job_id": job_id
    }


@router.get("/statistics")
async def get_export_statistics(
    request: Request,
    vendor_id: Optional[int] = Query(None),
    days: int = Query(30, ge=1, le=365)
):
    """
    å–å¾—åŒ¯å‡ºçµ±è¨ˆè³‡è¨Š

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
        days: çµ±è¨ˆå¤©æ•¸ï¼ˆ1-365ï¼‰

    Returns:
        Dict: çµ±è¨ˆè³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # åŸºç¤çµ±è¨ˆ
        if vendor_id is not None:
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_jobs,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_jobs,
                    COUNT(*) FILTER (WHERE status = 'processing') as processing_jobs,
                    COALESCE(SUM(success_records), 0) as total_exported,
                    COALESCE(SUM(file_size_bytes), 0) as total_file_size,
                    COALESCE(AVG(success_records) FILTER (WHERE status = 'completed'), 0) as avg_exported_per_job
                FROM unified_jobs
                WHERE vendor_id = $1
                  AND job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - ($2 || ' days')::INTERVAL
            """, vendor_id, days)
        else:
            stats = await conn.fetchrow("""
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_jobs,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_jobs,
                    COUNT(*) FILTER (WHERE status = 'processing') as processing_jobs,
                    COALESCE(SUM(success_records), 0) as total_exported,
                    COALESCE(SUM(file_size_bytes), 0) as total_file_size,
                    COALESCE(AVG(success_records) FILTER (WHERE status = 'completed'), 0) as avg_exported_per_job
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - ($1 || ' days')::INTERVAL
            """, days)

        # è¨ˆç®—æˆåŠŸç‡
        total_jobs = stats['total_jobs'] or 0
        completed_jobs = stats['completed_jobs'] or 0
        success_rate = (completed_jobs / total_jobs * 100) if total_jobs > 0 else 0

        # æ¨¡å¼çµ±è¨ˆ
        if vendor_id is not None:
            mode_stats = await conn.fetch("""
                SELECT
                    job_config->>'export_mode' as export_mode,
                    COUNT(*) as count,
                    COALESCE(AVG(success_records), 0) as avg_exported
                FROM unified_jobs
                WHERE vendor_id = $1
                  AND job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - ($2 || ' days')::INTERVAL
                  AND status = 'completed'
                GROUP BY job_config->>'export_mode'
            """, vendor_id, days)
        else:
            mode_stats = await conn.fetch("""
                SELECT
                    job_config->>'export_mode' as export_mode,
                    COUNT(*) as count,
                    COALESCE(AVG(success_records), 0) as avg_exported
                FROM unified_jobs
                WHERE job_type = 'knowledge_export'
                  AND created_at > CURRENT_TIMESTAMP - ($1 || ' days')::INTERVAL
                  AND status = 'completed'
                GROUP BY job_config->>'export_mode'
            """, days)

        return {
            "total_jobs": total_jobs,
            "completed_jobs": completed_jobs,
            "failed_jobs": stats['failed_jobs'],
            "processing_jobs": stats['processing_jobs'],
            "total_exported": stats['total_exported'],
            "total_file_size_mb": round(stats['total_file_size'] / 1024 / 1024, 2),
            "avg_exported_per_job": round(float(stats['avg_exported_per_job']), 2),
            "success_rate": round(success_rate, 2),
            "mode_statistics": [
                {
                    "mode": mode['export_mode'],
                    "count": mode['count'],
                    "avg_exported": round(float(mode['avg_exported']), 2)
                }
                for mode in mode_stats
            ],
            "days": days
        }


@router.get("/preview")
async def preview_export(
    request: Request,
    vendor_id: Optional[int] = Query(None)
):
    """
    é è¦½åŒ¯å‡ºè³‡æ–™ï¼ˆä¸å¯¦éš›åŒ¯å‡ºï¼Œåªè¿”å›çµ±è¨ˆè³‡è¨Šï¼‰

    Args:
        vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰

    Returns:
        Dict: é è¦½è³‡è¨Š
    """
    db_pool = request.app.state.db_pool

    async with db_pool.acquire() as conn:
        # çµ±è¨ˆçŸ¥è­˜æ•¸é‡
        if vendor_id is not None:
            knowledge_count = await conn.fetchval("""
                SELECT COUNT(*) FROM knowledge WHERE vendor_id = $1
            """, vendor_id)

            intent_count = await conn.fetchval("""
                SELECT COUNT(*) FROM intents WHERE vendor_id = $1
            """, vendor_id)
        else:
            knowledge_count = await conn.fetchval("""
                SELECT COUNT(*) FROM knowledge WHERE vendor_id IS NULL
            """)

            intent_count = await conn.fetchval("""
                SELECT COUNT(*) FROM intents WHERE vendor_id IS NULL
            """)

        # ä¼°ç®—æª”æ¡ˆå¤§å°ï¼ˆç²—ç•¥è¨ˆç®—ï¼‰
        # å‡è¨­æ¯ç­†çŸ¥è­˜ç´„ 500 bytesï¼ŒåŠ ä¸Šæ ¼å¼åŒ–é–‹éŠ·
        estimated_size_kb = (knowledge_count * 0.5) + (intent_count * 0.2)

        # æ¨è–¦åŒ¯å‡ºæ¨¡å¼
        if knowledge_count < 10000:
            recommended_mode = "formatted"
            recommendation = "æ¨è–¦ä½¿ç”¨é€²éšæ ¼å¼åŒ–æ¨¡å¼ï¼Œæä¾›æœ€ä½³é–±è®€é«”é©—"
        elif knowledge_count < 50000:
            recommended_mode = "formatted"
            recommendation = "è³‡æ–™é‡é©ä¸­ï¼Œæ¨è–¦ä½¿ç”¨é€²éšæ ¼å¼åŒ–æ¨¡å¼"
        else:
            recommended_mode = "optimized"
            recommendation = "è³‡æ–™é‡è¼ƒå¤§ï¼Œæ¨è–¦ä½¿ç”¨æ•ˆèƒ½å„ªåŒ–æ¨¡å¼"

        return {
            "knowledge_count": knowledge_count,
            "intent_count": intent_count,
            "estimated_file_size_kb": round(estimated_size_kb, 2),
            "estimated_file_size_mb": round(estimated_size_kb / 1024, 2),
            "recommended_mode": recommended_mode,
            "recommendation": recommendation,
            "vendor_id": vendor_id,
            "message": "é€™æ˜¯é è¦½æ¨¡å¼ï¼Œå°šæœªå¯¦éš›åŸ·è¡ŒåŒ¯å‡º"
        }
