"""
LLM Provider æŠ½è±¡å±¤
çµ±ä¸€è™•ç†æ‰€æœ‰ LLM API èª¿ç”¨ï¼Œæ”¯æ´å¤šç¨® Providerï¼ˆOpenAI, OpenRouter, Ollamaï¼‰
"""
import os
import httpx
from abc import ABC, abstractmethod
from typing import Optional, List, Dict, Any, Union
from openai import OpenAI, AsyncOpenAI


class LLMProvider(ABC):
    """
    LLM Provider æŠ½è±¡åŸºé¡
    å®šç¾©æ‰€æœ‰ LLM Provider å¿…é ˆå¯¦ç¾çš„ä»‹é¢
    """

    @abstractmethod
    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        åŸ·è¡ŒèŠå¤©å®Œæˆè«‹æ±‚

        Args:
            model: æ¨¡å‹åç¨±
            messages: è¨Šæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            temperature: æº«åº¦åƒæ•¸ (0-1)
            max_tokens: æœ€å¤§ç”Ÿæˆ tokens æ•¸
            **kwargs: å…¶ä»–åƒæ•¸ï¼ˆå¦‚ functions, function_call ç­‰ï¼‰

        Returns:
            {
                'content': str,           # ç”Ÿæˆçš„æ–‡æœ¬
                'usage': dict,            # ä½¿ç”¨çµ±è¨ˆï¼ˆå¯é¸ï¼‰
                'raw_response': object    # åŸå§‹ API å›æ‡‰ï¼ˆç”¨æ–¼ç‰¹æ®ŠåŠŸèƒ½å¦‚ function callingï¼‰
            }
        """
        pass

    @abstractmethod
    def embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> Optional[List[float]]:
        """
        ç”Ÿæˆæ–‡æœ¬ embedding

        Args:
            text: è¦ç”Ÿæˆ embedding çš„æ–‡æœ¬
            model: embedding æ¨¡å‹åç¨±ï¼ˆå¯é¸ï¼‰

        Returns:
            embedding å‘é‡åˆ—è¡¨ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        pass


class OpenAIProvider(LLMProvider):
    """
    OpenAI Provider å¯¦ç¾
    ä½¿ç”¨å®˜æ–¹ OpenAI API
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– OpenAI Provider

        Args:
            api_key: OpenAI API Keyï¼ˆå¯é¸ï¼Œé»˜èªä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼‰
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY æœªè¨­å®š")

        self.client = OpenAI(api_key=self.api_key)
        self.async_client = AsyncOpenAI(api_key=self.api_key)
        self.provider_name = "OpenAI"

    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ OpenAI èŠå¤©å®Œæˆè«‹æ±‚"""
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )

            # åŸºæœ¬å›æ‡‰çµæ§‹
            result = {
                'content': response.choices[0].message.content,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                },
                'model': response.model,
                'provider': self.provider_name,
                'raw_response': response  # ä¿ç•™åŸå§‹å›æ‡‰ä»¥æ”¯æ´ function calling ç­‰ç‰¹æ®ŠåŠŸèƒ½
            }

            return result
        except Exception as e:
            raise Exception(f"OpenAI API èª¿ç”¨å¤±æ•—: {str(e)}")

    def embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> Optional[List[float]]:
        """ç”Ÿæˆ OpenAI embedding"""
        try:
            model = model or os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            response = self.client.embeddings.create(
                model=model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ OpenAI Embedding ç”Ÿæˆå¤±æ•—: {e}")
            return None

    async def async_chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """åŸ·è¡Œç•°æ­¥ OpenAI èŠå¤©å®Œæˆè«‹æ±‚"""
        try:
            response = await self.async_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )

            # çµ±ä¸€å›å‚³æ ¼å¼
            return {
                'content': response.choices[0].message.content,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens if response.usage else 0,
                    'completion_tokens': response.usage.completion_tokens if response.usage else 0,
                    'total_tokens': response.usage.total_tokens if response.usage else 0
                },
                'raw_response': response  # ä¿ç•™åŸå§‹å›æ‡‰ä¾› function calling ä½¿ç”¨
            }
        except Exception as e:
            print(f"âŒ OpenAI Async Chat Completion å¤±æ•—: {e}")
            raise

    async def async_embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> Optional[List[float]]:
        """ç”Ÿæˆç•°æ­¥ OpenAI embedding"""
        try:
            model = model or os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
            response = await self.async_client.embeddings.create(
                model=model,
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âŒ OpenAI Async Embedding ç”Ÿæˆå¤±æ•—: {e}")
            return None


class OpenRouterProvider(LLMProvider):
    """
    OpenRouter Provider å¯¦ç¾
    ä½¿ç”¨ OpenRouter APIï¼ˆæ”¯æ´å¤šç¨®é–‹æºæ¨¡å‹ï¼‰
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        åˆå§‹åŒ– OpenRouter Provider

        Args:
            api_key: OpenRouter API Keyï¼ˆå¯é¸ï¼Œé»˜èªä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼‰
        """
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY æœªè¨­å®š")

        # OpenRouter ä½¿ç”¨ OpenAI SDK ä½†æ”¹è®Š base_url
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://openrouter.ai/api/v1"
        )
        self.provider_name = "OpenRouter"

    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ OpenRouter èŠå¤©å®Œæˆè«‹æ±‚"""
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )

            return {
                'content': response.choices[0].message.content,
                'usage': {
                    'prompt_tokens': response.usage.prompt_tokens,
                    'completion_tokens': response.usage.completion_tokens,
                    'total_tokens': response.usage.total_tokens
                } if response.usage else {},
                'model': response.model,
                'provider': self.provider_name,
                'raw_response': response  # ä¿ç•™åŸå§‹å›æ‡‰
            }
        except Exception as e:
            raise Exception(f"OpenRouter API èª¿ç”¨å¤±æ•—: {str(e)}")

    def embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> Optional[List[float]]:
        """
        OpenRouter ä¸ç›´æ¥æ”¯æ´ embedding
        å»ºè­°ä½¿ç”¨ Ollama æˆ– OpenAI çš„ embedding
        """
        raise NotImplementedError(
            "OpenRouter ä¸æ”¯æ´ embeddingï¼Œè«‹ä½¿ç”¨ OpenAI æˆ– Ollama"
        )


class OllamaProvider(LLMProvider):
    """
    Ollama Provider å¯¦ç¾
    ä½¿ç”¨æœ¬åœ°æˆ–é ç«¯ Ollama æœå‹™
    """

    def __init__(self, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ– Ollama Provider

        Args:
            base_url: Ollama API URLï¼ˆå¯é¸ï¼Œé»˜èªä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼‰
        """
        self.base_url = base_url or os.getenv(
            "OLLAMA_API_URL",
            "http://localhost:11434"
        )
        self.provider_name = "Ollama"

    def chat_completion(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """åŸ·è¡Œ Ollama èŠå¤©å®Œæˆè«‹æ±‚"""
        try:
            # Ollama API æ ¼å¼
            payload = {
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }

            if max_tokens:
                payload["options"]["num_predict"] = max_tokens

            response = httpx.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=120.0
            )

            if response.status_code != 200:
                raise Exception(f"Ollama API éŒ¯èª¤: {response.status_code}")

            data = response.json()

            return {
                'content': data.get('message', {}).get('content', ''),
                'usage': {
                    'prompt_tokens': data.get('prompt_eval_count', 0),
                    'completion_tokens': data.get('eval_count', 0),
                    'total_tokens': data.get('prompt_eval_count', 0) + data.get('eval_count', 0)
                },
                'model': data.get('model', model),
                'provider': self.provider_name,
                'raw_response': data  # ä¿ç•™åŸå§‹å›æ‡‰
            }
        except Exception as e:
            raise Exception(f"Ollama API èª¿ç”¨å¤±æ•—: {str(e)}")

    def embedding(
        self,
        text: str,
        model: Optional[str] = None
    ) -> Optional[List[float]]:
        """ç”Ÿæˆ Ollama embedding"""
        try:
            model = model or os.getenv("OLLAMA_EMBEDDING_MODEL", "nomic-embed-text")

            response = httpx.post(
                f"{self.base_url}/api/embeddings",
                json={
                    "model": model,
                    "prompt": text
                },
                timeout=60.0
            )

            if response.status_code != 200:
                print(f"âš ï¸ Ollama Embedding API éŒ¯èª¤: {response.status_code}")
                return None

            data = response.json()
            return data.get('embedding')

        except Exception as e:
            print(f"âŒ Ollama Embedding ç”Ÿæˆå¤±æ•—: {e}")
            return None


# ============================================================
# Factory å‡½æ•¸ï¼ˆæ”¯æ´æ··åˆ Provider é…ç½®ï¼‰
# ============================================================

# Provider å¯¦ä¾‹ç·©å­˜ï¼ˆæ”¯æ´å¤šå€‹ Provider å…±å­˜ï¼‰
_provider_cache: Dict[str, LLMProvider] = {}


def get_llm_provider(
    provider_type: Optional[str] = None,
    service_name: Optional[str] = None,
    reset: bool = False
) -> LLMProvider:
    """
    ç²å– LLM Providerï¼ˆæ”¯æ´æ··åˆé…ç½®ï¼‰

    Args:
        provider_type: Provider é¡å‹ ('openai', 'openrouter', 'ollama')
                      è‹¥ç‚º Noneï¼Œå‰‡å¾ç’°å¢ƒè®Šæ•¸è®€å–
        service_name: æœå‹™åç¨±ï¼ˆç”¨æ–¼æŸ¥æ‰¾æœå‹™å°ˆå±¬çš„ Provider é…ç½®ï¼‰
                     ä¾‹å¦‚: 'intent_classifier', 'answer_optimizer', 'embedding'
        reset: æ˜¯å¦é‡ç½®ç·©å­˜ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰

    Returns:
        LLMProvider å¯¦ä¾‹

    ç’°å¢ƒè®Šæ•¸ï¼ˆæŒ‰å„ªå…ˆç´šï¼‰:
        1. {SERVICE_NAME}_PROVIDER: æœå‹™å°ˆå±¬ Providerï¼ˆæœ€é«˜å„ªå…ˆç´šï¼‰
        2. LLM_PROVIDER: å…¨åŸŸé è¨­ Provider
        3. é»˜èª: 'openai'

    ç¯„ä¾‹:
        # ä½¿ç”¨å…¨åŸŸ Provider
        provider = get_llm_provider()

        # ç‚ºç‰¹å®šæœå‹™ä½¿ç”¨å°ˆå±¬ Provider
        provider = get_llm_provider(service_name='intent_classifier')
        # æœƒæŸ¥æ‰¾ INTENT_CLASSIFIER_PROVIDER ç’°å¢ƒè®Šæ•¸

        # ç›´æ¥æŒ‡å®š Provider é¡å‹
        provider = get_llm_provider(provider_type='openrouter')
    """
    global _provider_cache

    if reset:
        _provider_cache = {}

    # æ±ºå®šä½¿ç”¨å“ªå€‹ Provider
    if provider_type is None:
        if service_name:
            # å„ªå…ˆä½¿ç”¨æœå‹™å°ˆå±¬çš„ Provider é…ç½®
            env_key = f"{service_name.upper()}_PROVIDER"
            provider_type = os.getenv(env_key)

        # è‹¥æœå‹™å°ˆå±¬é…ç½®ä¸å­˜åœ¨ï¼Œä½¿ç”¨å…¨åŸŸé…ç½®
        if not provider_type:
            provider_type = os.getenv("LLM_PROVIDER", "openai")

    provider_type = provider_type.lower()

    # å¾ç·©å­˜ç²å–æˆ–å‰µå»ºæ–°çš„ Provider å¯¦ä¾‹
    if provider_type not in _provider_cache:
        if provider_type == "openrouter":
            _provider_cache[provider_type] = OpenRouterProvider()
        elif provider_type == "ollama":
            _provider_cache[provider_type] = OllamaProvider()
        else:  # é»˜èªä½¿ç”¨ openai
            _provider_cache[provider_type] = OpenAIProvider()

        print(f"ğŸ”§ [LLM Provider] åˆå§‹åŒ– {provider_type.upper()} Provider")

    provider = _provider_cache[provider_type]

    # åªåœ¨é¦–æ¬¡èª¿ç”¨æˆ–æœå‹™åˆ‡æ›æ™‚æ‰“å°æ—¥èªŒ
    if service_name:
        print(f"ğŸ”§ [LLM Provider] {service_name} ä½¿ç”¨: {provider.provider_name}")

    return provider


# ============================================================
# ä¾¿åˆ©å‡½æ•¸ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
# ============================================================

def chat_completion(
    model: str,
    messages: List[Dict[str, str]],
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    provider_type: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    åŸ·è¡ŒèŠå¤©å®Œæˆè«‹æ±‚ï¼ˆä¾¿åˆ©å‡½æ•¸ï¼‰

    Args:
        model: æ¨¡å‹åç¨±
        messages: è¨Šæ¯åˆ—è¡¨
        temperature: æº«åº¦åƒæ•¸
        max_tokens: æœ€å¤§ tokens
        provider_type: Provider é¡å‹ï¼ˆå¯é¸ï¼‰
        **kwargs: å…¶ä»–åƒæ•¸

    Returns:
        åŒ…å« content å’Œ usage çš„å­—å…¸
    """
    provider = get_llm_provider(provider_type)
    return provider.chat_completion(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


# ============================================================
# ä½¿ç”¨ç¯„ä¾‹èˆ‡æ¸¬è©¦
# ============================================================

if __name__ == "__main__":
    import asyncio

    def test_llm_provider():
        """æ¸¬è©¦ LLM Provider"""
        print("=" * 60)
        print("æ¸¬è©¦ LLM Provider")
        print("=" * 60)

        # æ¸¬è©¦ 1: OpenAI Provider
        print("\nğŸ“ æ¸¬è©¦ 1: OpenAI Provider")
        try:
            provider = OpenAIProvider()
            print(f"âœ… Provider åˆå§‹åŒ–æˆåŠŸ: {provider.provider_name}")

            messages = [
                {"role": "user", "content": "è«‹ç”¨ä¸€å¥è©±ä»‹ç´¹å°ç£"}
            ]
            result = provider.chat_completion(
                model="gpt-3.5-turbo",
                messages=messages,
                temperature=0.7,
                max_tokens=100
            )

            print(f"   å›æ‡‰: {result['content'][:100]}...")
            print(f"   ä½¿ç”¨: {result['usage']}")
            print(f"   Provider: {result['provider']}")
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦ 2: Factory å‡½æ•¸
        print("\nğŸ“ æ¸¬è©¦ 2: Factory å‡½æ•¸")
        try:
            # ä½¿ç”¨ç’°å¢ƒè®Šæ•¸æ±ºå®š provider
            provider = get_llm_provider()
            print(f"âœ… ä½¿ç”¨ Provider: {provider.provider_name}")

            result = chat_completion(
                model=os.getenv("INTENT_MODEL", "gpt-3.5-turbo"),
                messages=[{"role": "user", "content": "ä½ å¥½"}],
                temperature=0.5
            )

            print(f"   å›æ‡‰: {result['content']}")
        except Exception as e:
            print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")

        # æ¸¬è©¦ 3: OpenRouterï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        print("\nğŸ“ æ¸¬è©¦ 3: OpenRouter Providerï¼ˆéœ€è¦é…ç½®ï¼‰")
        if os.getenv("OPENROUTER_API_KEY"):
            try:
                provider = OpenRouterProvider()
                print(f"âœ… Provider åˆå§‹åŒ–æˆåŠŸ: {provider.provider_name}")

                messages = [
                    {"role": "user", "content": "Say hello in Chinese"}
                ]
                result = provider.chat_completion(
                    model="anthropic/claude-3.5-sonnet",
                    messages=messages,
                    temperature=0.7
                )

                print(f"   å›æ‡‰: {result['content']}")
                print(f"   Provider: {result['provider']}")
            except Exception as e:
                print(f"âš ï¸ è·³éï¼ˆæœªé…ç½®æˆ–éŒ¯èª¤ï¼‰: {e}")
        else:
            print("âš ï¸ è·³éï¼ˆæœªè¨­å®š OPENROUTER_API_KEYï¼‰")

        # æ¸¬è©¦ 4: Ollamaï¼ˆå¦‚æœæœ‰æœ¬åœ°æœå‹™ï¼‰
        print("\nğŸ“ æ¸¬è©¦ 4: Ollama Providerï¼ˆéœ€è¦æœ¬åœ°æœå‹™ï¼‰")
        try:
            provider = OllamaProvider()
            print(f"âœ… Provider åˆå§‹åŒ–æˆåŠŸ: {provider.provider_name}")

            messages = [
                {"role": "user", "content": "ä½ å¥½ï¼Œè«‹ç”¨ä¸­æ–‡å›ç­”"}
            ]
            result = provider.chat_completion(
                model="llama2",
                messages=messages,
                temperature=0.7
            )

            print(f"   å›æ‡‰: {result['content'][:100]}...")
            print(f"   Provider: {result['provider']}")
        except Exception as e:
            print(f"âš ï¸ è·³éï¼ˆOllama æœªé‹è¡Œæˆ–éŒ¯èª¤ï¼‰: {e}")

        print("\n" + "=" * 60)
        print("æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)

    # åŸ·è¡Œæ¸¬è©¦
    test_llm_provider()
