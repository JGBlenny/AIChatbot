"""
LLM ç­”æ¡ˆå„ªåŒ–æœå‹™
ä½¿ç”¨ GPT æ¨¡å‹å„ªåŒ– RAG æª¢ç´¢çµæœï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´ç²¾æº–çš„ç­”æ¡ˆ
Phase 1 æ“´å±•ï¼šæ”¯æ´æ¥­è€…åƒæ•¸å‹•æ…‹æ³¨å…¥
Phase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–ï¼ˆå¿«é€Ÿè·¯å¾‘ + æ¨¡æ¿æ ¼å¼åŒ–ï¼‰
Phase 4 æ“´å±•ï¼šæ¥­æ…‹èªæ°£é…ç½®å¾è³‡æ–™åº«å‹•æ…‹è¼‰å…¥
"""
import os
import re
from typing import List, Dict, Optional
from openai import OpenAI
import time
import psycopg2
import psycopg2.extras
from .answer_formatter import AnswerFormatter
from .db_utils import get_db_config

# æ¥­æ…‹èªæ°£é…ç½®å¿«å–ï¼ˆé¿å…é »ç¹æŸ¥è©¢è³‡æ–™åº«ï¼‰
_TONE_CONFIG_CACHE: Optional[Dict[str, Dict]] = None
_TONE_CACHE_TIMESTAMP: Optional[float] = None
_TONE_CACHE_TTL = 300  # 5 åˆ†é˜å¿«å–


class LLMAnswerOptimizer:
    """LLM ç­”æ¡ˆå„ªåŒ–å™¨"""

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ– LLM ç­”æ¡ˆå„ªåŒ–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        # å»¶é²åˆå§‹åŒ–ï¼šåªæœ‰åœ¨éœ€è¦æ™‚æ‰æª¢æŸ¥ API key
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key) if api_key else None

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨¡å‹é…ç½®ï¼ˆç”¨æ–¼é™ä½æ¸¬è©¦æˆæœ¬ï¼‰
        # é è¨­ä½¿ç”¨ gpt-3.5-turboï¼ˆé€Ÿåº¦å¿« 2-3å€ï¼Œæˆæœ¬ä½ 70%ï¼‰
        default_model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
        synthesis_threshold = float(os.getenv("SYNTHESIS_THRESHOLD", "0.80"))
        fast_path_threshold = float(os.getenv("FAST_PATH_THRESHOLD", "0.75"))
        template_min_score = float(os.getenv("TEMPLATE_MIN_SCORE", "0.55"))
        template_max_score = float(os.getenv("TEMPLATE_MAX_SCORE", "0.75"))

        # é è¨­é…ç½®
        default_config = {
            "model": default_model,
            "temperature": float(os.getenv("LLM_ANSWER_TEMPERATURE", "0.7")),
            "max_tokens": int(os.getenv("LLM_ANSWER_MAX_TOKENS", "800")),
            "enable_optimization": True,
            "optimize_for_confidence": ["high", "medium"],  # åªå„ªåŒ–é«˜/ä¸­ä¿¡å¿ƒåº¦
            "fallback_on_error": True,  # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
            # Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆåŠŸèƒ½
            "enable_synthesis": True,  # æ˜¯å¦å•Ÿç”¨ç­”æ¡ˆåˆæˆï¼ˆå·²å•Ÿç”¨ï¼Œæ•´åˆå¤šå€‹ SOP é …ç›®ï¼‰
            "synthesis_min_results": 2,  # æœ€å°‘éœ€è¦å¹¾å€‹çµæœæ‰è€ƒæ…®åˆæˆ
            "synthesis_max_results": 5,  # æœ€å¤šåˆæˆå¹¾å€‹ç­”æ¡ˆï¼ˆèª¿æ•´ç‚º 5ï¼Œæ”¯æ´æ›´å¤š SOP é …ç›®ï¼‰
            "synthesis_threshold": synthesis_threshold,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.80ï¼‰
            # Phase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–
            "enable_fast_path": True,  # æ˜¯å¦å•Ÿç”¨å¿«é€Ÿè·¯å¾‘
            "fast_path_threshold": fast_path_threshold,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.75ï¼‰
            "enable_template": True,  # æ˜¯å¦å•Ÿç”¨æ¨¡æ¿æ ¼å¼åŒ–
            "template_min_score": template_min_score,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.55ï¼‰
            "template_max_score": template_max_score  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.75ï¼‰
        }

        # åˆå§‹åŒ–ç­”æ¡ˆæ ¼å¼åŒ–å™¨
        self.formatter = AnswerFormatter()

        # åˆä½µç”¨æˆ¶é…ç½®èˆ‡é è¨­é…ç½®
        if config:
            self.config = {**default_config, **config}
        else:
            self.config = default_config

    @staticmethod
    def _load_tone_configs_from_db() -> Dict[str, str]:
        """
        å¾é…ç½®æ–‡ä»¶è¼‰å…¥æ¥­æ…‹èªæ°£é…ç½®

        Returns:
            Dict[business_type, tone_prompt]
        """
        try:
            from config.business_types import get_all_tone_prompts
            configs = get_all_tone_prompts()
            print(f"âœ… å¾é…ç½®æ–‡ä»¶è¼‰å…¥ {len(configs)} å€‹æ¥­æ…‹èªæ°£é…ç½®")
            return configs
        except Exception as e:
            print(f"âš ï¸ è¼‰å…¥æ¥­æ…‹èªæ°£é…ç½®å¤±æ•—: {e}")
            return {}

    @staticmethod
    def _get_tone_config(business_type: str) -> Optional[str]:
        """
        å–å¾—æ¥­æ…‹èªæ°£é…ç½®ï¼ˆå«å¿«å–æ©Ÿåˆ¶ï¼‰

        Args:
            business_type: æ¥­æ…‹é¡å‹ (å¦‚: 'full_service', 'property_management')

        Returns:
            èªæ°£ prompt æ–‡å­—ï¼Œæˆ– None
        """
        global _TONE_CONFIG_CACHE, _TONE_CACHE_TIMESTAMP

        current_time = time.time()

        # æª¢æŸ¥å¿«å–æ˜¯å¦æœ‰æ•ˆ
        if _TONE_CONFIG_CACHE is not None and _TONE_CACHE_TIMESTAMP is not None:
            if current_time - _TONE_CACHE_TIMESTAMP < _TONE_CACHE_TTL:
                return _TONE_CONFIG_CACHE.get(business_type)

        # é‡æ–°è¼‰å…¥é…ç½®
        _TONE_CONFIG_CACHE = LLMAnswerOptimizer._load_tone_configs_from_db()
        _TONE_CACHE_TIMESTAMP = current_time

        return _TONE_CONFIG_CACHE.get(business_type)

    @staticmethod
    def clear_tone_cache():
        """æ¸…ç©ºæ¥­æ…‹èªæ°£é…ç½®å¿«å–"""
        global _TONE_CONFIG_CACHE, _TONE_CACHE_TIMESTAMP
        _TONE_CONFIG_CACHE = None
        _TONE_CACHE_TIMESTAMP = None
        print("âœ… æ¥­æ…‹èªæ°£é…ç½®å¿«å–å·²æ¸…ç©º")

    def optimize_answer(
        self,
        question: str,
        search_results: List[Dict],
        confidence_level: str,
        intent_info: Dict,
        confidence_score: Optional[float] = None,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None,
        enable_synthesis_override: Optional[bool] = None
    ) -> Dict:
        """
        å„ªåŒ–ç­”æ¡ˆï¼ˆPhase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–ï¼‰

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: RAG æª¢ç´¢çµæœåˆ—è¡¨
            confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š
            intent_info: æ„åœ–è³‡è¨Š
            confidence_score: ä¿¡å¿ƒåº¦åˆ†æ•¸ (0-1)ï¼Œç”¨æ–¼æ¢ä»¶å¼å„ªåŒ–åˆ¤æ–·
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_name: æ¥­è€…åç¨±ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼ŒPhase 1 SOP æ“´å±•ï¼‰
            enable_synthesis_override: è¦†è“‹ç­”æ¡ˆåˆæˆé…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            å„ªåŒ–çµæœå­—å…¸ï¼ŒåŒ…å«:
            - optimized_answer: å„ªåŒ–å¾Œçš„ç­”æ¡ˆ
            - original_answer: åŸå§‹ç­”æ¡ˆ
            - optimization_applied: æ˜¯å¦ä½¿ç”¨äº†å„ªåŒ–
            - optimization_method: å„ªåŒ–æ–¹æ³• (fast_path/template/llm/none)
            - synthesis_applied: æ˜¯å¦ä½¿ç”¨äº†ç­”æ¡ˆåˆæˆ
            - tokens_used: ä½¿ç”¨çš„ token æ•¸
            - processing_time_ms: è™•ç†æ™‚é–“
        """
        start_time = time.time()

        # 1. æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ–
        if not self._should_optimize(confidence_level, search_results):
            return self._create_fallback_response(search_results, start_time)

        # DEBUG: è¨˜éŒ„æ¢ä»¶å¼å„ªåŒ–åƒæ•¸
        print(f"ğŸ”§ æ¢ä»¶å¼å„ªåŒ–æª¢æŸ¥: confidence_score={confidence_score}, confidence_level={confidence_level}")

        # 2. å„ªå…ˆæª¢æŸ¥ï¼šå¤šå€‹é«˜å“è³ªçµæœ â†’ ç­”æ¡ˆåˆæˆï¼ˆç²¾ç…‰ç‰ˆé‚è¼¯ï¼‰
        synthesis_threshold = self.config.get("synthesis_threshold", 0.80)
        high_quality_results = [r for r in search_results if r.get('similarity', 0) > synthesis_threshold]
        force_synthesis = False

        if len(high_quality_results) >= 2 and self.config.get("enable_synthesis", True):
            # æª¢æ¸¬æµç¨‹å•é¡Œ
            process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
            is_process_question = any(kw in question for kw in process_keywords)

            # æª¢æ¸¬å»£æ³›æŸ¥è©¢ï¼ˆæ¶µè“‹å¤šå€‹ä¸»é¡Œçš„å•é¡Œï¼‰
            broad_keywords = ["æ¢æ¬¾", "è¦å®š", "èªªæ˜", "å…§å®¹", "åŒ…æ‹¬", "æœ‰å“ªäº›", "ä»€éº¼", "åŒ…å«"]
            is_broad_query = any(kw in question for kw in broad_keywords)

            # æª¢æŸ¥çµæœå¤šæ¨£æ€§ï¼ˆä¸åŒä¸»é¡Œæ•¸é‡ï¼‰
            topics = set()
            for r in high_quality_results[:5]:  # æª¢æŸ¥å‰ 5 å€‹çµæœ
                # æ”¯æ´ SOP (item_name)ã€KB (question_summary)ã€å’Œæ¨™æº–æ ¼å¼ (title)
                if 'item_name' in r:
                    topics.add(r['item_name'])
                elif 'question_summary' in r:
                    topics.add(r['question_summary'])
                elif 'title' in r:
                    topics.add(r['title'])
            has_diverse_topics = len(topics) >= 3  # è‡³å°‘ 3 å€‹ä¸åŒä¸»é¡Œ

            # æª¢æŸ¥æ˜¯å¦æœ‰å®Œç¾åŒ¹é…ï¼ˆä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦ï¼Œä¸å«æ„åœ–åŠ æˆï¼‰
            # æ„åœ–åŠ æˆåªç”¨æ–¼æ’åºï¼Œä¸æ‡‰å½±éŸ¿å®Œç¾åŒ¹é…åˆ¤æ–·
            max_similarity = max(
                r.get('original_similarity', r['similarity'])
                for r in high_quality_results
            )
            perfect_match_threshold = self.config.get("perfect_match_threshold", 0.95)
            has_perfect_match = max_similarity >= perfect_match_threshold

            # å¼·åˆ¶åˆæˆçš„æ¢ä»¶ï¼ˆå„ªå…ˆç´šå¾é«˜åˆ°ä½ï¼‰ï¼š
            # 1. æµç¨‹å•é¡Œï¼ˆéœ€è¦å®Œæ•´æ­¥é©Ÿï¼‰
            # 2. å»£æ³›æŸ¥è©¢ + ä¸»é¡Œå¤šæ¨£ï¼ˆæ¶µè“‹å¤šå€‹å­ä¸»é¡Œï¼‰- å„ªå…ˆæ–¼å®Œç¾åŒ¹é…æª¢æŸ¥
            # 3. æ²’æœ‰å®Œç¾åŒ¹é…ï¼ˆéœ€è¦çµ„åˆå¤šå€‹ç­”æ¡ˆï¼‰
            if is_process_question:
                force_synthesis = True
                print(f"ğŸ”„ æª¢æ¸¬åˆ° {len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼ˆæµç¨‹å•é¡Œï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨ç­”æ¡ˆåˆæˆ")
                print(f"   ç›¸ä¼¼åº¦ç¯„åœ: {min(r['similarity'] for r in high_quality_results):.3f}-{max_similarity:.3f}")
            elif is_broad_query and has_diverse_topics:
                # å»£æ³›æŸ¥è©¢å„ªå…ˆæ–¼å®Œç¾åŒ¹é…ï¼ˆå³ä½¿ SOP similarity=1.0 ä¹Ÿè¦åˆæˆï¼‰
                force_synthesis = True
                print(f"ğŸ”„ æª¢æ¸¬åˆ° {len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼ˆå»£æ³›æŸ¥è©¢ + {len(topics)} å€‹ä¸åŒä¸»é¡Œï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨ç­”æ¡ˆåˆæˆ")
                print(f"   ä¸»é¡Œ: {', '.join(list(topics)[:5])}")
                print(f"   ç›¸ä¼¼åº¦: {max_similarity:.3f} (å¿½ç•¥å®Œç¾åŒ¹é…ï¼Œå› ç‚ºæ˜¯å»£æ³›æŸ¥è©¢)")
            elif not has_perfect_match:
                force_synthesis = True
                print(f"ğŸ”„ æª¢æ¸¬åˆ° {len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼ˆç„¡å®Œç¾åŒ¹é…: max={max_similarity:.3f} < {perfect_match_threshold}ï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨ç­”æ¡ˆåˆæˆ")
            else:
                print(f"â„¹ï¸  æª¢æ¸¬åˆ°å®Œç¾åŒ¹é…ï¼ˆ{max_similarity:.3f} >= {perfect_match_threshold}ï¼‰ï¼Œä½¿ç”¨å–®ä¸€ç­”æ¡ˆå„ªåŒ–")
                force_synthesis = False
        # 3. Phase 3 æ¢ä»¶å¼å„ªåŒ–ï¼šæª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¿«é€Ÿè·¯å¾‘ï¼ˆå–®ä¸€çµæœæˆ–ä½å“è³ªå¤šçµæœï¼‰
        elif confidence_score is not None and self.config.get("enable_fast_path", True):
            if self.formatter.should_use_fast_path(
                confidence_score,
                search_results,
                threshold=self.config.get("fast_path_threshold", 0.85)
            ):
                # å¿«é€Ÿè·¯å¾‘ï¼šç›´æ¥è¿”å›æ ¼å¼åŒ–ç­”æ¡ˆï¼Œç„¡éœ€ LLM
                print(f"âš¡ å¿«é€Ÿè·¯å¾‘è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")
                result = self.formatter.format_simple_answer(search_results)
                answer = result["answer"]

                # âœ… Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šå¿«é€Ÿè·¯å¾‘é€²è¡Œåƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
                if vendor_params and vendor_name:
                    print(f"   ğŸ’‰ å¿«é€Ÿè·¯å¾‘ - æ³¨å…¥æ¥­è€…åƒæ•¸ + èªæ°£èª¿æ•´ ({len(vendor_params)} å€‹)")
                    answer = self.inject_vendor_params(answer, vendor_params, vendor_name, vendor_info)

                processing_time = int((time.time() - start_time) * 1000)

                return {
                    "optimized_answer": answer,
                    "original_answer": search_results[0].get('content', ''),
                    "optimization_applied": True,
                    "optimization_method": "fast_path",
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "model": "none"
                }

        # 4. Phase 3 æ¢ä»¶å¼å„ªåŒ–ï¼šæª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–ï¼ˆéé«˜å“è³ªå¤šçµæœæ™‚ï¼‰
        elif confidence_score is not None and self.config.get("enable_template", True):
            if self.formatter.should_use_template(
                confidence_score,
                confidence_level,
                search_results
            ):
                # æ¨¡æ¿æ ¼å¼åŒ–ï¼šä½¿ç”¨é å®šç¾©æ¨¡æ¿ï¼Œç„¡éœ€ LLM
                print(f"ğŸ“‹ æ¨¡æ¿æ ¼å¼åŒ–è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")
                result = self.formatter.format_with_template(
                    question,
                    search_results,
                    intent_type=intent_info.get('intent_type')
                )
                answer = result["answer"]

                # âœ… Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šæ¨¡æ¿æ ¼å¼åŒ–é€²è¡Œåƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
                if vendor_params and vendor_name:
                    print(f"   ğŸ’‰ æ¨¡æ¿æ ¼å¼åŒ– - æ³¨å…¥æ¥­è€…åƒæ•¸ + èªæ°£èª¿æ•´ ({len(vendor_params)} å€‹)")
                    answer = self.inject_vendor_params(answer, vendor_params, vendor_name, vendor_info)

                processing_time = int((time.time() - start_time) * 1000)

                return {
                    "optimized_answer": answer,
                    "original_answer": search_results[0].get('content', ''),
                    "optimization_applied": True,
                    "optimization_method": "template",
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "model": "none"
                }

        # 4. æº–å‚™åŸå§‹ç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦å®Œæ•´ LLM å„ªåŒ–ï¼‰
        original_answer = self._create_original_answer(search_results)

        # 5. åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆï¼ˆPhase 2 æ“´å±•ï¼‰
        # æ”¯æ´å‹•æ…‹è¦†è“‹ï¼šå¦‚æœæœ‰å¤šå€‹é«˜å“è³ªçµæœï¼Œå¼·åˆ¶å•Ÿç”¨åˆæˆï¼›å¦å‰‡ä½¿ç”¨åŸæœ‰é‚è¼¯
        if force_synthesis:
            should_synthesize = True
            print(f"âœ… å¼·åˆ¶å•Ÿç”¨ç­”æ¡ˆåˆæˆï¼ˆ{len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼‰")
        else:
            should_synthesize = self._should_synthesize(
                question,
                search_results,
                enable_synthesis_override
            )

        # 6. åŸ·è¡Œå®Œæ•´ LLM å„ªåŒ–ï¼ˆPhase 1 æ“´å±•ï¼šåŠ å…¥æ¥­è€…åƒæ•¸æ³¨å…¥ï¼›Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆï¼‰
        try:
            if should_synthesize:
                # ä½¿ç”¨ç­”æ¡ˆåˆæˆæ¨¡å¼
                conf_str = f"{confidence_score:.3f}" if confidence_score is not None else "N/A"
                print(f"ğŸ”„ ç­”æ¡ˆåˆæˆæ¨¡å¼ (ä¿¡å¿ƒåº¦: {conf_str})")
                optimized_answer, tokens_used = self.synthesize_answer(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = True
            else:
                # ä½¿ç”¨å‚³çµ±å„ªåŒ–æ¨¡å¼
                conf_str = f"{confidence_score:.3f}" if confidence_score is not None else "N/A"
                print(f"ğŸ¤– å®Œæ•´ LLM å„ªåŒ– (ä¿¡å¿ƒåº¦: {conf_str})")
                optimized_answer, tokens_used = self._call_llm(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = False

            processing_time = int((time.time() - start_time) * 1000)

            return {
                "optimized_answer": optimized_answer,
                "original_answer": original_answer,
                "optimization_applied": True,
                "optimization_method": "synthesis" if synthesis_applied else "llm",
                "synthesis_applied": synthesis_applied,  # æ–°å¢ï¼šæ¨™è¨˜æ˜¯å¦ä½¿ç”¨äº†åˆæˆ
                "tokens_used": tokens_used,
                "processing_time_ms": processing_time,
                "model": self.config["model"]
            }

        except Exception as e:
            print(f"âŒ LLM å„ªåŒ–å¤±æ•—: {e}")

            if self.config["fallback_on_error"]:
                # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
                processing_time = int((time.time() - start_time) * 1000)
                return {
                    "optimized_answer": original_answer,
                    "original_answer": original_answer,
                    "optimization_applied": False,
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "error": str(e)
                }
            else:
                raise

    def _should_optimize(self, confidence_level: str, search_results: List[Dict]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„ªåŒ–"""
        if not self.config["enable_optimization"]:
            return False

        if not search_results:
            return False

        if confidence_level not in self.config["optimize_for_confidence"]:
            return False

        return True

    def _should_synthesize(
        self,
        question: str,
        search_results: List[Dict],
        enable_synthesis_override: Optional[bool] = None
    ) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆ

        è§¸ç™¼æ¢ä»¶ï¼ˆå…¨éƒ¨æ»¿è¶³ï¼‰ï¼š
        1. å•Ÿç”¨åˆæˆåŠŸèƒ½
        2. è‡³å°‘æœ‰æŒ‡å®šæ•¸é‡çš„æª¢ç´¢çµæœ
        3. å•é¡ŒåŒ…å«è¤‡åˆéœ€æ±‚é—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ < é–¾å€¼ï¼‰

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: æª¢ç´¢çµæœåˆ—è¡¨
            enable_synthesis_override: è¦†è“‹é…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            æ˜¯å¦æ‡‰è©²åˆæˆç­”æ¡ˆ
        """
        # 1. åŠŸèƒ½é–‹é—œï¼ˆæ”¯æ´å‹•æ…‹è¦†è“‹ï¼‰
        if enable_synthesis_override is not None:
            # å¦‚æœå‚³å…¥è¦†è“‹å€¼ï¼Œä½¿ç”¨è¦†è“‹å€¼
            if not enable_synthesis_override:
                return False
        else:
            # å¦å‰‡ä½¿ç”¨é…ç½®
            if not self.config.get("enable_synthesis", False):
                return False

        # 2. çµæœæ•¸é‡
        min_results = self.config.get("synthesis_min_results", 2)
        if len(search_results) < min_results:
            return False

        # 3. è¤‡åˆå•é¡Œé—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        complex_keywords = ["å¦‚ä½•", "æ€éº¼", "æµç¨‹", "æ­¥é©Ÿ", "éœ€è¦", "ä»€éº¼æ™‚å€™", "æ³¨æ„", "æº–å‚™", "è¾¦ç†"]
        has_complex_pattern = any(kw in question for kw in complex_keywords)

        # 4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆè¡¨ç¤ºå¯èƒ½éœ€è¦çµ„åˆå¤šå€‹ç­”æ¡ˆï¼‰
        threshold = self.config.get("synthesis_threshold", 0.7)
        max_similarity = max(r['similarity'] for r in search_results[:min_results])
        no_perfect_match = max_similarity < threshold

        # è¨˜éŒ„åˆ¤æ–·çµæœï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        if has_complex_pattern and no_perfect_match:
            print(f"ğŸ”„ ç­”æ¡ˆåˆæˆè§¸ç™¼ï¼šå•é¡Œé¡å‹={has_complex_pattern}, æœ€é«˜ç›¸ä¼¼åº¦={max_similarity:.3f} < {threshold}")

        return has_complex_pattern and no_perfect_match

    def _get_result_title(self, result: Dict) -> str:
        """
        ç²å–æª¢ç´¢çµæœçš„æ¨™é¡Œï¼ˆæ”¯æ´ KB å’Œ SOP æ ¼å¼ï¼‰

        Args:
            result: æª¢ç´¢çµæœ

        Returns:
            æ¨™é¡Œå­—ä¸²ï¼ˆKB ä½¿ç”¨ question_summaryï¼ŒSOP ä½¿ç”¨ item_nameï¼‰
        """
        if 'question_summary' in result:
            return result['question_summary']
        elif 'item_name' in result:
            return result['item_name']
        else:
            return "ï¼ˆç„¡æ¨™é¡Œï¼‰"

    def _create_original_answer(self, search_results: List[Dict]) -> str:
        """å»ºç«‹åŸå§‹ç­”æ¡ˆï¼ˆæœªå„ªåŒ–ï¼‰"""
        if not search_results:
            return ""

        best_result = search_results[0]
        title = self._get_result_title(best_result)
        content = best_result.get('content', '')

        return f"{title}\n\n{content}"

    def _create_fallback_response(self, search_results: List[Dict], start_time: float) -> Dict:
        """å»ºç«‹å‚™ç”¨å›æ‡‰ï¼ˆä¸å„ªåŒ–ï¼‰"""
        original_answer = self._create_original_answer(search_results)
        processing_time = int((time.time() - start_time) * 1000)

        return {
            "optimized_answer": original_answer,
            "original_answer": original_answer,
            "optimization_applied": False,
            "tokens_used": 0,
            "processing_time_ms": processing_time
        }

    def _replace_params_deterministic(
        self,
        content: str,
        vendor_params: Dict,
        vendor_name: str
    ) -> str:
        """
        éšæ®µ 1ï¼šç¢ºå®šæ€§åƒæ•¸æ›¿æ›ï¼ˆä¸ä½¿ç”¨ LLMï¼‰

        ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å’Œå­—ç¬¦ä¸²åŒ¹é…ï¼Œ100% å¯é åœ°æ›¿æ›åƒæ•¸å€¼

        Args:
            content: åŸå§‹å…§å®¹
            vendor_params: æ¥­è€…åƒæ•¸å­—å…¸
            vendor_name: æ¥­è€…åç¨±

        Returns:
            åƒæ•¸æ›¿æ›å¾Œçš„å…§å®¹
        """
        result = content
        replacements_made = []

        # 1. æ›¿æ›æ˜ç¢ºçš„æ¨¡æ¿è®Šæ•¸ {{xxx}}
        for key, value in vendor_params.items():
            pattern = f"{{{{{key}}}}}"
            if pattern in result:
                result = result.replace(pattern, str(value))
                replacements_made.append(f"{{{{{{key}}}}}} â†’ {value}")

        # 2. æ™ºèƒ½åŒ¹é…å¸¸è¦‹æ¨¡å¼ä¸¦æ›¿æ›

        # 2a. é›»è©±è™Ÿç¢¼æ¨¡å¼ï¼ˆå¦‚ 0800-123-456, 02-1234-5678ï¼‰
        if 'service_hotline' in vendor_params:
            phone_patterns = [
                r'\d{4}-\d{3}-\d{3}',  # 0800-123-456
                r'\d{2}-\d{4}-\d{4}',  # 02-1234-5678
                r'\d{4}-\d{6}',        # 0800-123456
            ]
            for pattern in phone_patterns:
                matches = re.findall(pattern, result)
                for match in matches:
                    # æ’é™¤ç·Šæ€¥å°ˆç·šç­‰ç‰¹å®šè™Ÿç¢¼
                    if '0911' not in match and '119' not in match:
                        result = result.replace(match, vendor_params['service_hotline'])
                        replacements_made.append(f"{match} â†’ {vendor_params['service_hotline']} (é›»è©±)")
                        break  # åªæ›¿æ›ç¬¬ä¸€å€‹åŒ¹é…

        # 2b. å·¥ä½œå¤©æ•¸æ¨¡å¼ï¼ˆå¦‚ "3å€‹å·¥ä½œå¤©"ï¼‰- æš«æ™‚åœç”¨
        # åŸå› ï¼šrepair_response_time çš„å–®ä½æ˜¯ã€Œå°æ™‚ã€ï¼Œç„¡æ³•ç›´æ¥æ›¿æ›ã€Œå¤©ã€
        # å¦‚æœéœ€è¦æ›¿æ›å¤©æ•¸ï¼Œæ‡‰è©²åœ¨è³‡æ–™åº«ä¸­æ–°å¢ repair_response_days åƒæ•¸
        # if 'repair_response_days' in vendor_params:
        #     time_pattern = r'(\d+)\s*(å€‹)?å·¥ä½œå¤©'
        #     matches = re.finditer(time_pattern, result)
        #     for match in matches:
        #         old_value = match.group(1)
        #         if int(old_value) <= 7:
        #             full_match = match.group(0)
        #             new_text = f"{vendor_params['repair_response_days']}å€‹å·¥ä½œå¤©"
        #             result = result.replace(full_match, new_text, 1)
        #             replacements_made.append(f"{full_match} â†’ {new_text} (æ™‚æ•ˆ)")

        # 2c. å°æ™‚æ•¸æ¨¡å¼ï¼ˆå¦‚ "24å°æ™‚"ï¼‰- é‡å° repair_response_time
        if 'repair_response_time' in vendor_params:
            hour_pattern = r'(\d+)\s*å°æ™‚'
            # åªåœ¨æåˆ°"å›æ‡‰"æˆ–"è™•ç†"çš„ä¸Šä¸‹æ–‡ä¸­æ›¿æ›
            if 'å›æ‡‰' in result or 'è™•ç†' in result:
                matches = list(re.finditer(hour_pattern, result))
                for match in matches:
                    old_value = match.group(1)
                    # åªæ›¿æ›åˆç†ç¯„åœå…§çš„å°æ™‚æ•¸
                    if 12 <= int(old_value) <= 72:
                        full_match = match.group(0)
                        # æª¢æŸ¥å‰å¾Œæ–‡ï¼Œç¢ºä¿æ˜¯ç¶­ä¿®ç›¸é—œ
                        start = max(0, match.start() - 20)
                        end = min(len(result), match.end() + 20)
                        context = result[start:end]
                        if 'ç·Šæ€¥' not in context:  # ä¸æ›¿æ›ç·Šæ€¥å°ˆç·šçš„24å°æ™‚
                            new_text = f"{vendor_params['repair_response_time']} å°æ™‚"
                            result = result.replace(full_match, new_text, 1)
                            replacements_made.append(f"{full_match} â†’ {new_text} (æ™‚æ•ˆ)")
                            break

        if replacements_made:
            print(f"      âœ… ç¢ºå®šæ€§æ›¿æ›å®Œæˆï¼š{len(replacements_made)} é …")
            for r in replacements_made:
                print(f"         - {r}")
        else:
            print(f"      â„¹ï¸  ç„¡éœ€ç¢ºå®šæ€§æ›¿æ›")

        return result

    def inject_vendor_params(
        self,
        content: str,
        vendor_params: Dict,
        vendor_name: str,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """
        ä½¿ç”¨å…©éšæ®µæ–¹æ³•é€²è¡Œåƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´ï¼ˆæ–¹æ¡ˆ Cï¼‰

        éšæ®µ 1: ç¢ºå®šæ€§åƒæ•¸æ›¿æ› - ä½¿ç”¨æ­£å‰‡å’Œå­—ç¬¦ä¸²åŒ¹é…ï¼Œ100% å¯é 
        éšæ®µ 2: èªæ°£èª¿æ•´ - ä½¿ç”¨ LLM èª¿æ•´è¡¨é”æ–¹å¼ï¼ˆä¸åšåƒæ•¸æ›¿æ›ï¼‰

        Args:
            content: åŸå§‹çŸ¥è­˜å…§å®¹
            vendor_params: æ¥­è€…åƒæ•¸å­—å…¸
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type ç­‰ï¼‰

        Returns:
            èª¿æ•´å¾Œçš„å…§å®¹
        """
        if not vendor_params:
            return content

        print(f"      ğŸ” å…©éšæ®µåƒæ•¸æ³¨å…¥ + èªæ°£èª¿æ•´ - åŸå§‹å…§å®¹é•·åº¦: {len(content)} å­—å…ƒ")
        print(f"      ğŸ“‹ æ¥­è€…åƒæ•¸: {list(vendor_params.keys())}")

        # === éšæ®µ 1ï¼šç¢ºå®šæ€§åƒæ•¸æ›¿æ›ï¼ˆä¸ç”¨ LLMï¼‰===
        content = self._replace_params_deterministic(content, vendor_params, vendor_name)

        # === éšæ®µ 2ï¼šèªæ°£èª¿æ•´ï¼ˆä½¿ç”¨ LLMï¼‰===
        # æª¢æŸ¥æ˜¯å¦éœ€è¦èªæ°£èª¿æ•´
        business_type = 'property_management'  # é è¨­å€¼
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            print(f"      ğŸ¢ æ¥­ç¨®é¡å‹: {business_type}")

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£
        tone_prompt = self._get_tone_config(business_type)

        # å¦‚æœæ²’æœ‰èªæ°£é…ç½®ï¼Œç›´æ¥è¿”å›ï¼ˆè·³ééšæ®µ 2ï¼‰
        if not tone_prompt:
            print(f"      â„¹ï¸  æ¥­æ…‹é¡å‹ '{business_type}' ç„¡èªæ°£é…ç½®ï¼Œè·³é LLM èª¿æ•´")
            return content

        # æœ‰èªæ°£é…ç½®ï¼Œä½¿ç”¨ LLM èª¿æ•´
        system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„èªæ°£èª¿æ•´åŠ©ç†ã€‚

**é‡è¦åŸå‰‡**ï¼š
1. âŒ **ç¦æ­¢ä¿®æ”¹ä»»ä½•æ•¸å€¼**ï¼ˆé›»è©±ã€æ—¥æœŸã€é‡‘é¡ã€æ™‚é–“ç­‰ï¼‰
2. âŒ **ç¦æ­¢è¼¸å‡ºæ¨¡æ¿è®Šæ•¸æ ¼å¼**ï¼ˆå¦‚ {{{{service_hotline}}}}ã€@vendorA ç­‰ï¼‰
3. âœ… **åªèª¿æ•´èªæ°£å’Œè¡¨é”æ–¹å¼**ï¼ˆä½¿å…§å®¹æ›´ç¬¦åˆæ¥­æ…‹é¢¨æ ¼ï¼‰
4. âœ… **ä¿æŒå…§å®¹çµæ§‹å’Œæ ¼å¼**ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ã€æ®µè½ï¼‰

æ¥­è€…åç¨±ï¼š{vendor_name}
æ¥­ç¨®é¡å‹ï¼š{business_type}

ã€èªæ°£èª¿æ•´è¦ç¯„ã€‘
{tone_prompt}

æ³¨æ„ï¼š
- å…§å®¹ä¸­çš„æ‰€æœ‰æ•¸å€¼éƒ½å·²ç¶“æ˜¯æ­£ç¢ºçš„æ¥­è€…åƒæ•¸ï¼Œè«‹å‹¿ä¿®æ”¹
- åªèª¿æ•´ç”¨è©ã€èªæ°£ã€è¡¨é”æ–¹å¼
- åªè¼¸å‡ºèª¿æ•´å¾Œçš„å…§å®¹ï¼Œä¸è¦åŠ ä¸Šä»»ä½•èªªæ˜"""

        user_prompt = f"""è«‹æ ¹æ“šæ¥­ç¨®èªæ°£èª¿æ•´ä»¥ä¸‹å…§å®¹ï¼ˆè«‹å‹¿ä¿®æ”¹ä»»ä½•æ•¸å€¼ï¼‰ï¼š

{content}"""

        try:
            if not self.client:
                raise Exception("OpenAI client not initialized (missing API key)")

            # æ–¹æ¡ˆ C: èªæ°£èª¿æ•´å°ˆç”¨ï¼Œtemperature 0.3 è¶³å¤ ï¼ˆä¸åšåƒæ•¸æ›¿æ›ï¼‰
            tone_adjustment_temp = float(os.getenv("LLM_TONE_ADJUSTMENT_TEMP", "0.3"))
            response = self.client.chat.completions.create(
                model=self.config["model"],
                temperature=tone_adjustment_temp,  # é è¨­ 0.3
                max_tokens=self.config["max_tokens"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )

            adjusted_content = response.choices[0].message.content.strip()

            # æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰è®ŠåŒ–
            if adjusted_content != content:
                print(f"      âœ… èªæ°£èª¿æ•´å®Œæˆ - å…§å®¹å·²èª¿æ•´")
                print(f"         åŸå§‹: {content[:100]}...")
                print(f"         èª¿æ•´: {adjusted_content[:100]}...")
            else:
                print(f"      â„¹ï¸  å…§å®¹æœªè®ŠåŒ–ï¼ˆç„¡éœ€èªæ°£èª¿æ•´ï¼‰")

            return adjusted_content

        except Exception as e:
            print(f"      âš ï¸  èªæ°£èª¿æ•´å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å…§å®¹: {e}")
            return content

    def synthesize_answer(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        åˆæˆå¤šå€‹ç­”æ¡ˆç‚ºä¸€å€‹å®Œæ•´ç­”æ¡ˆï¼ˆPhase 2 æ“´å±•åŠŸèƒ½ï¼‰

        ç•¶æª¢ç´¢åˆ°çš„å¤šå€‹ç­”æ¡ˆå„æœ‰å´é‡æ™‚ï¼Œä½¿ç”¨ LLM å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´ã€çµæ§‹åŒ–çš„ç­”æ¡ˆã€‚
        é€™å¯ä»¥æå‡ç­”æ¡ˆçš„å®Œæ•´æ€§ï¼Œç‰¹åˆ¥é©ç”¨æ–¼è¤‡é›œå•é¡Œã€‚

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: å¤šå€‹æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (åˆæˆå¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # æº–å‚™å¤šå€‹ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        max_results = self.config.get("synthesis_max_results", 3)
        answers_to_synthesize = []

        for i, result in enumerate(search_results[:max_results], 1):
            content = result['content']

            # å¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name, vendor_info)

            answers_to_synthesize.append({
                "index": i,
                "title": self._get_result_title(result),
                "content": content,
                "similarity": result['similarity']
            })

        # æ ¼å¼åŒ–ç­”æ¡ˆåˆ—è¡¨
        formatted_answers = "\n\n".join([
            f"ã€ç­”æ¡ˆ {ans['index']}ã€‘\n"
            f"æ¨™é¡Œï¼š{ans['title']}\n"
            f"ç›¸ä¼¼åº¦ï¼š{ans['similarity']:.2f}\n"
            f"å…§å®¹ï¼š\n{ans['content']}"
            for ans in answers_to_synthesize
        ])

        # å»ºç«‹åˆæˆ Prompt
        system_prompt = self._create_synthesis_system_prompt(intent_info, vendor_name, vendor_info)
        user_prompt = self._create_synthesis_user_prompt(question, formatted_answers, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # å‘¼å« OpenAI API é€²è¡Œåˆæˆ
        synthesis_temp = float(os.getenv("LLM_SYNTHESIS_TEMP", "0.5"))
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=synthesis_temp,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œç¨ä½æº«åº¦ä»¥ç¢ºä¿æº–ç¢ºæ€§å’Œçµæ§‹
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        synthesized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        print(f"âœ¨ ç­”æ¡ˆåˆæˆå®Œæˆï¼šä½¿ç”¨äº† {len(answers_to_synthesize)} å€‹ä¾†æºï¼Œtokens: {tokens_used}")

        return synthesized_answer, tokens_used

    def _create_synthesis_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜æ•´åˆåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡å¤šå€‹ç›¸é—œä½†å„æœ‰å´é‡çš„ç­”æ¡ˆï¼Œåˆæˆç‚ºä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚

åˆæˆè¦æ±‚ï¼š
1. **å®Œæ•´æ€§**ï¼šæ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Šï¼Œä¸éºæ¼ä»»ä½•é—œéµæ­¥é©Ÿæˆ–ç´°ç¯€
2. **æº–ç¢ºæ€§**ï¼šè³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„ç­”æ¡ˆï¼Œä¸è¦ç·¨é€ æˆ–æ¨æ¸¬
3. **çµæ§‹åŒ–**ï¼šä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿç·¨è™Ÿï¼Œä½¿ç­”æ¡ˆæ˜“æ–¼é–±è®€
4. **å»é‡**ï¼šå¦‚æœå¤šå€‹ç­”æ¡ˆæåˆ°ç›¸åŒè³‡è¨Šï¼Œåªä¿ç•™ä¸€æ¬¡ï¼Œé¿å…é‡è¤‡
5. **å„ªå…ˆç´š**ï¼šå„ªå…ˆä½¿ç”¨ç›¸ä¼¼åº¦è¼ƒé«˜çš„ç­”æ¡ˆå…§å®¹
6. **èªæ°£**ï¼šä¿æŒå°ˆæ¥­ã€å‹å–„ã€æ˜“æ‡‚çš„ç¹é«”ä¸­æ–‡è¡¨é”
7. **Markdown**ï¼šé©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆ## æ¨™é¡Œã€- åˆ—è¡¨ã€**ç²—é«”**ï¼‰"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. **æ¥­è€…èº«ä»½**ï¼šä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 4 æ“´å±•ï¼šå¾è³‡æ–™åº«è¼‰å…¥ - ç°¡åŒ–ç‰ˆï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            tone_prompt = self._get_tone_config(business_type)
            if tone_prompt:
                # å°‡å®Œæ•´çš„ tone prompt åŠ å…¥ï¼ˆç°¡æ½”ç‰ˆæœ¬ï¼Œç”¨æ–¼ç­”æ¡ˆåˆæˆï¼‰
                base_prompt += f"\n{rule_number}. **æ¥­ç¨®èªæ°£**ï¼š{vendor_name} çš„èªæ°£èª¿æ•´è¦ç¯„å¦‚ä¸‹ï¼š\n{tone_prompt}"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. **çŸ¥è­˜é¡å‹**ï¼šé€™æ˜¯çŸ¥è­˜æŸ¥è©¢ï¼Œè«‹æä¾›å®Œæ•´çš„èªªæ˜ã€æ­¥é©Ÿå’Œæ³¨æ„äº‹é …"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. **è³‡æ–™æŸ¥è©¢**ï¼šå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢å’Œæ‰€éœ€è³‡æ–™"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. **æ“ä½œæŒ‡å¼•**ï¼šè«‹æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æ“ä½œæ­¥é©Ÿ"

        base_prompt += "\n\né‡è¦ï¼šåªè¼¸å‡ºåˆæˆå¾Œçš„å®Œæ•´ç­”æ¡ˆï¼Œä¸è¦åŠ ä¸Šã€Œæ ¹æ“šä»¥ä¸Šè³‡è¨Šã€ç­‰å…ƒè³‡è¨Šã€‚"

        return base_prompt

    def _create_synthesis_user_prompt(self, question: str, formatted_answers: str, intent_info: Dict) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        # æª¢æ¸¬æ˜¯å¦ç‚ºæµç¨‹/æ­¥é©Ÿé¡å•é¡Œ
        process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
        is_process_question = any(kw in question for kw in process_keywords)

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

ä»¥ä¸‹æ˜¯å¤šå€‹ç›¸é—œç­”æ¡ˆï¼Œè«‹å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´çš„å›è¦†ï¼š

{formatted_answers}

è«‹ç¶œåˆä»¥ä¸Šç­”æ¡ˆï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚ç¢ºä¿ï¼š"""

        if is_process_question:
            # æµç¨‹é¡å•é¡Œï¼šå¼·èª¿å®Œæ•´æ€§å’Œé †åºæ€§
            prompt += """
- **å®Œæ•´æµç¨‹**ï¼šé€™æ˜¯æµç¨‹ç›¸é—œå•é¡Œï¼Œè«‹åŒ…å«æ‰€æœ‰æª¢ç´¢åˆ°çš„æ­¥é©Ÿï¼ŒæŒ‰æ™‚é–“é †åºæ•´ç†ï¼ˆå¾é–‹å§‹åˆ°çµæŸï¼‰
- **ä¸è¦éºæ¼æ­¥é©Ÿ**ï¼šå³ä½¿æŸå€‹æ­¥é©Ÿåœ¨å•é¡Œä¸­æ²’æœ‰ç›´æ¥æåˆ°ï¼Œåªè¦å®ƒæ˜¯æµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå°±å¿…é ˆåŒ…å«
- **æ™‚åºé‚è¼¯**ï¼šè«‹æŒ‰ç…§å¯¦éš›åŸ·è¡Œçš„å…ˆå¾Œé †åºçµ„ç¹”ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šç”³è«‹â†’å¯©æ ¸â†’æ‰¹å‡†â†’ç°½ç´„ï¼‰
- ä½¿ç”¨æ¸…æ™°çš„ç·¨è™Ÿæˆ–æ¨™é¡Œæ¨™ç¤ºæ¯å€‹æ­¥é©Ÿ
- æ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Š
- é¿å…é‡è¤‡
- ä¿æŒæº–ç¢ºæ€§
- **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œç­”æ¡ˆ1ã€ã€ã€Œç­”æ¡ˆ2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""
        else:
            # ä¸€èˆ¬å•é¡Œï¼šä¿æŒåŸæœ‰é‚è¼¯
            prompt += """
- **å„ªå…ˆä½¿ç”¨èˆ‡å•é¡Œä¸»é¡Œæœ€ç›¸é—œçš„ç­”æ¡ˆ**
- å¦‚æœæŸå€‹ç­”æ¡ˆèˆ‡ç•¶å‰å•é¡Œç„¡é—œï¼Œè«‹å¿½ç•¥å®ƒ
- åªæ•´åˆèƒ½å›ç­”ç•¶å‰å•é¡Œçš„å…§å®¹
- æ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Š
- ä½¿ç”¨æ¸…æ™°çš„çµæ§‹ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿï¼‰
- é¿å…é‡è¤‡
- ä¿æŒæº–ç¢ºæ€§
- **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œç­”æ¡ˆ1ã€ã€ã€Œç­”æ¡ˆ2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""

        return prompt

    def _call_llm(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        å‘¼å« LLM å„ªåŒ–ç­”æ¡ˆ

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (å„ªåŒ–å¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # 1. æº–å‚™æª¢ç´¢çµæœä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):  # æœ€å¤šä½¿ç”¨å‰ 3 å€‹çµæœ
            content = result['content']

            # Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šå¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name, vendor_info)

            context_parts.append(
                f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘\n"
                f"æ¨™é¡Œï¼š{self._get_result_title(result)}\n"
                f"å…§å®¹ï¼š{content}\n"
                f"ç›¸ä¼¼åº¦ï¼š{result['similarity']:.2f}"
            )

        context = "\n\n".join(context_parts)

        # 2. å»ºç«‹å„ªåŒ– Promptï¼ˆåŠ å…¥æ¥­è€…åƒæ•¸ï¼‰
        system_prompt = self._create_system_prompt(intent_info, vendor_name, vendor_info, vendor_params)
        user_prompt = self._create_user_prompt(question, context, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # 3. å‘¼å« OpenAI API
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=self.config["temperature"],
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        optimized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        return optimized_answer, tokens_used

    def _create_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None,
        vendor_params: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„å®¢æœåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šçŸ¥è­˜åº«çš„è³‡è¨Šï¼Œç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„èªè¨€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œå…§å®¹
2. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£è¦ªåˆ‡å°ˆæ¥­
3. è³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„åƒè€ƒè³‡æ–™ï¼Œä¸è¦ç·¨é€ 
4. å¦‚æœ‰æ­¥é©Ÿæˆ–æµç¨‹ï¼Œè«‹æ¸…æ¥šåˆ—å‡º
5. é©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ï¼‰ä½¿ç­”æ¡ˆæ›´æ˜“è®€
6. ä¿æŒç°¡æ½”ï¼Œé¿å…å†—é•·
7. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè«‹èª å¯¦èªªæ˜"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. ä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # ã€æ–°å¢ã€‘å¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œæ˜ç¢ºåˆ—å‡ºæ‰€æœ‰åƒæ•¸ä¾› AI åƒè€ƒ
        if vendor_params:
            base_prompt += f"\n{rule_number}. **é‡è¦ï¼šæ¥­è€…ç‰¹å®šåƒæ•¸** - ç•¶åƒè€ƒè³‡æ–™ä¸å¤ å…·é«”æ™‚ï¼Œè«‹å„ªå…ˆä½¿ç”¨ä»¥ä¸‹è³‡è¨Šè£œå……å›ç­”ï¼š\n"

            # å°‡åƒæ•¸æŒ‰é¡åˆ¥çµ„ç¹”ï¼Œæ›´æ˜“è®€
            payment_params = {}
            service_params = {}
            contract_params = {}
            other_params = {}

            for key, value in vendor_params.items():
                if 'payment' in key or 'fee' in key or 'late' in key or 'grace' in key:
                    payment_params[key] = value
                elif 'service' in key or 'hotline' in key or 'hours' in key or 'repair' in key or 'line' in key or 'address' in key:
                    service_params[key] = value
                elif 'lease' in key or 'deposit' in key or 'termination' in key or 'notice' in key:
                    contract_params[key] = value
                else:
                    other_params[key] = value

            # ç¹³è²»ç›¸é—œåƒæ•¸
            if payment_params:
                base_prompt += "   ã€ç¹³è²»ç›¸é—œã€‘\n"
                for key, value in payment_params.items():
                    base_prompt += f"   - {key}: {value}\n"

            # æœå‹™ç›¸é—œåƒæ•¸
            if service_params:
                base_prompt += "   ã€å®¢æœè¯çµ¡ã€‘\n"
                for key, value in service_params.items():
                    base_prompt += f"   - {key}: {value}\n"

            # åˆç´„ç›¸é—œåƒæ•¸
            if contract_params:
                base_prompt += "   ã€åˆç´„æ¢æ¬¾ã€‘\n"
                for key, value in contract_params.items():
                    base_prompt += f"   - {key}: {value}\n"

            # å…¶ä»–åƒæ•¸
            if other_params:
                base_prompt += "   ã€å…¶ä»–è³‡è¨Šã€‘\n"
                for key, value in other_params.items():
                    base_prompt += f"   - {key}: {value}\n"

            base_prompt += "   **æ³¨æ„**ï¼šå¦‚æœåƒè€ƒè³‡æ–™ä¸­çš„è³‡è¨Šç± çµ±æˆ–ç¼ºå¤±ï¼Œè«‹ä¸»å‹•ä½¿ç”¨ä¸Šè¿°åƒæ•¸æä¾›å…·é«”è³‡è¨Šã€‚\n"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 4 æ“´å±•ï¼šå¾è³‡æ–™åº«è¼‰å…¥ - ç°¡åŒ–ç‰ˆï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            tone_prompt = self._get_tone_config(business_type)
            if tone_prompt:
                # å°‡å®Œæ•´çš„ tone prompt åŠ å…¥ï¼ˆç°¡æ½”ç‰ˆæœ¬ï¼Œç”¨æ–¼ç­”æ¡ˆå„ªåŒ–ï¼‰
                base_prompt += f"\n{rule_number}. ã€æ¥­ç¨®èªæ°£ã€‘{vendor_name} çš„èªæ°£èª¿æ•´è¦ç¯„å¦‚ä¸‹ï¼š\n{tone_prompt}"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. é€™æ˜¯çŸ¥è­˜æŸ¥è©¢å•é¡Œï¼Œè«‹æä¾›æ¸…æ¥šçš„èªªæ˜å’Œæ­¥é©Ÿ"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. é€™æ˜¯è³‡æ–™æŸ¥è©¢å•é¡Œï¼Œå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. é€™æ˜¯æ“ä½œåŸ·è¡Œå•é¡Œï¼Œè«‹èªªæ˜å…·é«”æ“ä½œæ­¥é©Ÿ"

        return base_prompt

    def _create_user_prompt(self, question: str, context: str, intent_info: Dict) -> str:
        """å»ºç«‹ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        # æª¢æ¸¬æ˜¯å¦ç‚ºæµç¨‹/æ­¥é©Ÿé¡å•é¡Œ
        process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
        is_process_question = any(kw in question for kw in process_keywords)

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™ï¼Œç”¨è‡ªç„¶ã€å‹å–„çš„èªæ°£å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

âš ï¸ é‡è¦æé†’ï¼š"""

        if is_process_question:
            # æµç¨‹é¡å•é¡Œï¼šåŒ…å«æ‰€æœ‰æ­¥é©Ÿ
            prompt += """
1. **é€™æ˜¯æµç¨‹ç›¸é—œå•é¡Œ**ï¼šè«‹åŒ…å«æ‰€æœ‰åƒè€ƒè³‡æ–™ä¸­çš„æ­¥é©Ÿï¼ŒæŒ‰æ™‚é–“é †åºæ•´ç†
2. å³ä½¿æŸå€‹æ­¥é©Ÿåœ¨å•é¡Œä¸­æ²’æœ‰ç›´æ¥æåˆ°ï¼Œåªè¦å®ƒæ˜¯å®Œæ•´æµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå°±æ‡‰è©²åŒ…å«
3. è«‹æŒ‰ç…§å¯¦éš›åŸ·è¡Œçš„å…ˆå¾Œé †åºçµ„ç¹”ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šç”³è«‹â†’å¯©æ ¸â†’æ‰¹å‡†â†’ç°½ç´„ï¼‰
4. ç›¸ä¼¼åº¦åˆ†æ•¸åƒ…ä¾›åƒè€ƒï¼Œè«‹ä»¥æµç¨‹çš„å®Œæ•´æ€§ç‚ºä¸»
5. **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œåƒè€ƒè³‡æ–™1ã€ã€ã€Œåƒè€ƒè³‡æ–™2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""
        else:
            # ä¸€èˆ¬å•é¡Œï¼šä¿æŒåŸæœ‰é‚è¼¯
            prompt += """
1. **è«‹ä»”ç´°é–±è®€æ¯å€‹åƒè€ƒè³‡æ–™ï¼Œé¸æ“‡æœ€èƒ½å›ç­”ç•¶å‰å•é¡Œçš„é‚£å€‹**
2. å¦‚æœæŸå€‹åƒè€ƒè³‡æ–™çš„æ¨™é¡Œ/å…§å®¹èˆ‡å•é¡Œä¸ç›´æ¥ç›¸é—œï¼Œè«‹ä¸è¦ä½¿ç”¨å®ƒ
3. å„ªå…ˆä½¿ç”¨èˆ‡å•é¡Œä¸»é¡Œæœ€åŒ¹é…çš„åƒè€ƒè³‡æ–™
4. ç›¸ä¼¼åº¦åˆ†æ•¸åƒ…ä¾›åƒè€ƒï¼Œè«‹ä»¥å…§å®¹ç›¸é—œæ€§ç‚ºä¸»
5. **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œåƒè€ƒè³‡æ–™1ã€ã€ã€Œåƒè€ƒè³‡æ–™2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""

        return prompt

    def get_optimization_stats(self, optimizations: List[Dict]) -> Dict:
        """
        è¨ˆç®—å„ªåŒ–çµ±è¨ˆè³‡è¨Š

        Args:
            optimizations: å„ªåŒ–çµæœåˆ—è¡¨

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if not optimizations:
            return {
                "total_optimizations": 0,
                "successful_optimizations": 0,
                "total_tokens_used": 0,
                "avg_tokens_per_optimization": 0,
                "avg_processing_time_ms": 0
            }

        total = len(optimizations)
        successful = sum(1 for o in optimizations if o.get('optimization_applied', False))
        total_tokens = sum(o.get('tokens_used', 0) for o in optimizations)
        total_time = sum(o.get('processing_time_ms', 0) for o in optimizations)

        return {
            "total_optimizations": total,
            "successful_optimizations": successful,
            "success_rate": round(successful / total, 3) if total > 0 else 0,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_optimization": round(total_tokens / successful, 1) if successful > 0 else 0,
            "avg_processing_time_ms": round(total_time / total, 1) if total > 0 else 0
        }


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    import asyncio

    async def test_optimizer():
        """æ¸¬è©¦ LLM ç­”æ¡ˆå„ªåŒ–å™¨"""
        optimizer = LLMAnswerOptimizer()

        # æ¨¡æ“¬æª¢ç´¢çµæœ
        search_results = [
            {
                "id": 1,
                "title": "é€€ç§Ÿæµç¨‹èªªæ˜",
                "category": "åˆç´„å•é¡Œ",
                "content": """# é€€ç§Ÿæµç¨‹

## æ­¥é©Ÿèªªæ˜

1. **æå‰é€šçŸ¥**ï¼šè«‹æ–¼é€€ç§Ÿæ—¥å‰30å¤©ä»¥æ›¸é¢æ–¹å¼é€šçŸ¥æˆ¿æ±
2. **ç¹³æ¸…è²»ç”¨**ï¼šç¢ºèªæ‰€æœ‰ç§Ÿé‡‘ã€æ°´é›»è²»å·²ç¹³æ¸…
3. **æˆ¿å±‹æª¢æŸ¥**ï¼šèˆ‡æˆ¿æ±ç´„å®šæ™‚é–“é€²è¡Œæˆ¿å±‹æª¢æŸ¥
4. **æŠ¼é‡‘é€€é‚„**ï¼šç¢ºèªæˆ¿å±‹ç‹€æ³è‰¯å¥½å¾Œï¼Œ7å€‹å·¥ä½œå¤©å…§é€€é‚„æŠ¼é‡‘

## æ³¨æ„äº‹é …
- éœ€æå‰30å¤©é€šçŸ¥
- éœ€ç¹³æ¸…æ‰€æœ‰è²»ç”¨
- æˆ¿å±‹éœ€æ¢å¾©åŸç‹€""",
                "similarity": 0.89
            }
        ]

        intent_info = {
            "intent_name": "é€€ç§Ÿæµç¨‹",
            "intent_type": "knowledge",
            "keywords": ["é€€ç§Ÿ", "è¾¦ç†"]
        }

        # æ¸¬è©¦å„ªåŒ–
        print("ğŸ”„ é–‹å§‹å„ªåŒ–ç­”æ¡ˆ...")
        result = optimizer.optimize_answer(
            question="è«‹å•å¦‚ä½•è¾¦ç†é€€ç§Ÿæ‰‹çºŒï¼Ÿ",
            search_results=search_results,
            confidence_level="high",
            intent_info=intent_info
        )

        print(f"\nâœ… å„ªåŒ–å®Œæˆ")
        print(f"ä½¿ç”¨å„ªåŒ–ï¼š{result['optimization_applied']}")
        print(f"Token ä½¿ç”¨ï¼š{result['tokens_used']}")
        print(f"è™•ç†æ™‚é–“ï¼š{result['processing_time_ms']}ms")
        print(f"\n{'='*60}")
        print("åŸå§‹ç­”æ¡ˆï¼š")
        print(result['original_answer'][:200] + "...")
        print(f"\n{'='*60}")
        print("å„ªåŒ–å¾Œç­”æ¡ˆï¼š")
        print(result['optimized_answer'])
        print(f"{'='*60}")

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_optimizer())
