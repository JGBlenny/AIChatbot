"""
LLM ç­”æ¡ˆå„ªåŒ–æœå‹™
ä½¿ç”¨ GPT æ¨¡å‹å„ªåŒ– RAG æª¢ç´¢çµæœï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´ç²¾æº–çš„ç­”æ¡ˆ
Phase 1 æ“´å±•ï¼šæ”¯æ´æ¥­è€…åƒæ•¸å‹•æ…‹æ³¨å…¥
Phase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–ï¼ˆå¿«é€Ÿè·¯å¾‘ + æ¨¡æ¿æ ¼å¼åŒ–ï¼‰
"""
import os
import re
from typing import List, Dict, Optional
from openai import OpenAI
import time
from .answer_formatter import AnswerFormatter


class LLMAnswerOptimizer:
    """LLM ç­”æ¡ˆå„ªåŒ–å™¨"""

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ– LLM ç­”æ¡ˆå„ªåŒ–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        # å»¶é²åˆå§‹åŒ–ï¼šåªæœ‰åœ¨éœ€è¦æ™‚æ‰æª¢æŸ¥ API key
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key) if api_key else None

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨¡å‹é…ç½®ï¼ˆç”¨æ–¼é™ä½æ¸¬è©¦æˆæœ¬ï¼‰
        # é è¨­ä½¿ç”¨ gpt-3.5-turboï¼ˆé€Ÿåº¦å¿« 2-3å€ï¼Œæˆæœ¬ä½ 70%ï¼‰
        default_model = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
        synthesis_threshold = float(os.getenv("SYNTHESIS_THRESHOLD", "0.80"))
        fast_path_threshold = float(os.getenv("FAST_PATH_THRESHOLD", "0.75"))
        template_min_score = float(os.getenv("TEMPLATE_MIN_SCORE", "0.55"))
        template_max_score = float(os.getenv("TEMPLATE_MAX_SCORE", "0.75"))

        # é è¨­é…ç½®
        default_config = {
            "model": default_model,
            "temperature": 0.7,
            "max_tokens": 800,
            "enable_optimization": True,
            "optimize_for_confidence": ["high", "medium"],  # åªå„ªåŒ–é«˜/ä¸­ä¿¡å¿ƒåº¦
            "fallback_on_error": True,  # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
            # Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆåŠŸèƒ½
            "enable_synthesis": True,  # æ˜¯å¦å•Ÿç”¨ç­”æ¡ˆåˆæˆï¼ˆå·²å•Ÿç”¨ï¼Œæ•´åˆå¤šå€‹ SOP é …ç›®ï¼‰
            "synthesis_min_results": 2,  # æœ€å°‘éœ€è¦å¹¾å€‹çµæœæ‰è€ƒæ…®åˆæˆ
            "synthesis_max_results": 5,  # æœ€å¤šåˆæˆå¹¾å€‹ç­”æ¡ˆï¼ˆèª¿æ•´ç‚º 5ï¼Œæ”¯æ´æ›´å¤š SOP é …ç›®ï¼‰
            "synthesis_threshold": synthesis_threshold,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.80ï¼‰
            # Phase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–
            "enable_fast_path": True,  # æ˜¯å¦å•Ÿç”¨å¿«é€Ÿè·¯å¾‘
            "fast_path_threshold": fast_path_threshold,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.75ï¼‰
            "enable_template": True,  # æ˜¯å¦å•Ÿç”¨æ¨¡æ¿æ ¼å¼åŒ–
            "template_min_score": template_min_score,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.55ï¼‰
            "template_max_score": template_max_score  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆé è¨­ 0.75ï¼‰
        }

        # åˆå§‹åŒ–ç­”æ¡ˆæ ¼å¼åŒ–å™¨
        self.formatter = AnswerFormatter()

        # åˆä½µç”¨æˆ¶é…ç½®èˆ‡é è¨­é…ç½®
        if config:
            self.config = {**default_config, **config}
        else:
            self.config = default_config

    def optimize_answer(
        self,
        question: str,
        search_results: List[Dict],
        confidence_level: str,
        intent_info: Dict,
        confidence_score: Optional[float] = None,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None,
        enable_synthesis_override: Optional[bool] = None
    ) -> Dict:
        """
        å„ªåŒ–ç­”æ¡ˆï¼ˆPhase 3 æ“´å±•ï¼šæ¢ä»¶å¼å„ªåŒ–ï¼‰

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: RAG æª¢ç´¢çµæœåˆ—è¡¨
            confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š
            intent_info: æ„åœ–è³‡è¨Š
            confidence_score: ä¿¡å¿ƒåº¦åˆ†æ•¸ (0-1)ï¼Œç”¨æ–¼æ¢ä»¶å¼å„ªåŒ–åˆ¤æ–·
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_name: æ¥­è€…åç¨±ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼ŒPhase 1 SOP æ“´å±•ï¼‰
            enable_synthesis_override: è¦†è“‹ç­”æ¡ˆåˆæˆé…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            å„ªåŒ–çµæœå­—å…¸ï¼ŒåŒ…å«:
            - optimized_answer: å„ªåŒ–å¾Œçš„ç­”æ¡ˆ
            - original_answer: åŸå§‹ç­”æ¡ˆ
            - optimization_applied: æ˜¯å¦ä½¿ç”¨äº†å„ªåŒ–
            - optimization_method: å„ªåŒ–æ–¹æ³• (fast_path/template/llm/none)
            - synthesis_applied: æ˜¯å¦ä½¿ç”¨äº†ç­”æ¡ˆåˆæˆ
            - tokens_used: ä½¿ç”¨çš„ token æ•¸
            - processing_time_ms: è™•ç†æ™‚é–“
        """
        start_time = time.time()

        # 1. æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ–
        if not self._should_optimize(confidence_level, search_results):
            return self._create_fallback_response(search_results, start_time)

        # DEBUG: è¨˜éŒ„æ¢ä»¶å¼å„ªåŒ–åƒæ•¸
        print(f"ğŸ”§ æ¢ä»¶å¼å„ªåŒ–æª¢æŸ¥: confidence_score={confidence_score}, confidence_level={confidence_level}")

        # 2. å„ªå…ˆæª¢æŸ¥ï¼šå¤šå€‹é«˜å“è³ªçµæœ â†’ ç­”æ¡ˆåˆæˆï¼ˆç²¾ç…‰ç‰ˆé‚è¼¯ï¼‰
        synthesis_threshold = self.config.get("synthesis_threshold", 0.80)
        high_quality_results = [r for r in search_results if r.get('similarity', 0) > synthesis_threshold]
        force_synthesis = False

        if len(high_quality_results) >= 2 and self.config.get("enable_synthesis", True):
            # æª¢æ¸¬æµç¨‹å•é¡Œ
            process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
            is_process_question = any(kw in question for kw in process_keywords)

            # æª¢æŸ¥æ˜¯å¦æœ‰å®Œç¾åŒ¹é…ï¼ˆsimilarity >= 0.95ï¼‰
            max_similarity = max(r['similarity'] for r in high_quality_results)
            perfect_match_threshold = self.config.get("perfect_match_threshold", 0.95)
            has_perfect_match = max_similarity >= perfect_match_threshold

            # åªåœ¨ä»¥ä¸‹æƒ…æ³å¼·åˆ¶åˆæˆï¼š
            # 1. æµç¨‹å•é¡Œï¼ˆéœ€è¦å®Œæ•´æ­¥é©Ÿï¼‰ OR
            # 2. æ²’æœ‰å®Œç¾åŒ¹é…ï¼ˆéœ€è¦çµ„åˆå¤šå€‹ç­”æ¡ˆï¼‰
            if is_process_question:
                force_synthesis = True
                print(f"ğŸ”„ æª¢æ¸¬åˆ° {len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼ˆæµç¨‹å•é¡Œï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨ç­”æ¡ˆåˆæˆ")
                print(f"   ç›¸ä¼¼åº¦ç¯„åœ: {min(r['similarity'] for r in high_quality_results):.3f}-{max_similarity:.3f}")
            elif not has_perfect_match:
                force_synthesis = True
                print(f"ğŸ”„ æª¢æ¸¬åˆ° {len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼ˆç„¡å®Œç¾åŒ¹é…: max={max_similarity:.3f} < {perfect_match_threshold}ï¼‰ï¼Œå¼·åˆ¶ä½¿ç”¨ç­”æ¡ˆåˆæˆ")
            else:
                print(f"â„¹ï¸  æª¢æ¸¬åˆ°å®Œç¾åŒ¹é…ï¼ˆ{max_similarity:.3f} >= {perfect_match_threshold}ï¼‰ï¼Œä½¿ç”¨å–®ä¸€ç­”æ¡ˆå„ªåŒ–")
                force_synthesis = False
        # 3. Phase 3 æ¢ä»¶å¼å„ªåŒ–ï¼šæª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨å¿«é€Ÿè·¯å¾‘ï¼ˆå–®ä¸€çµæœæˆ–ä½å“è³ªå¤šçµæœï¼‰
        elif confidence_score is not None and self.config.get("enable_fast_path", True):
            if self.formatter.should_use_fast_path(
                confidence_score,
                search_results,
                threshold=self.config.get("fast_path_threshold", 0.85)
            ):
                # å¿«é€Ÿè·¯å¾‘ï¼šç›´æ¥è¿”å›æ ¼å¼åŒ–ç­”æ¡ˆï¼Œç„¡éœ€ LLM
                print(f"âš¡ å¿«é€Ÿè·¯å¾‘è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")
                result = self.formatter.format_simple_answer(search_results)
                answer = result["answer"]

                # âœ… Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šå¿«é€Ÿè·¯å¾‘é€²è¡Œåƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
                if vendor_params and vendor_name:
                    print(f"   ğŸ’‰ å¿«é€Ÿè·¯å¾‘ - æ³¨å…¥æ¥­è€…åƒæ•¸ + èªæ°£èª¿æ•´ ({len(vendor_params)} å€‹)")
                    answer = self.inject_vendor_params(answer, vendor_params, vendor_name, vendor_info)

                processing_time = int((time.time() - start_time) * 1000)

                return {
                    "optimized_answer": answer,
                    "original_answer": search_results[0].get('content', ''),
                    "optimization_applied": True,
                    "optimization_method": "fast_path",
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "model": "none"
                }

        # 4. Phase 3 æ¢ä»¶å¼å„ªåŒ–ï¼šæª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–ï¼ˆéé«˜å“è³ªå¤šçµæœæ™‚ï¼‰
        elif confidence_score is not None and self.config.get("enable_template", True):
            if self.formatter.should_use_template(
                confidence_score,
                confidence_level,
                search_results
            ):
                # æ¨¡æ¿æ ¼å¼åŒ–ï¼šä½¿ç”¨é å®šç¾©æ¨¡æ¿ï¼Œç„¡éœ€ LLM
                print(f"ğŸ“‹ æ¨¡æ¿æ ¼å¼åŒ–è§¸ç™¼ (ä¿¡å¿ƒåº¦: {confidence_score:.3f})")
                result = self.formatter.format_with_template(
                    question,
                    search_results,
                    intent_type=intent_info.get('intent_type')
                )
                answer = result["answer"]

                # âœ… Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šæ¨¡æ¿æ ¼å¼åŒ–é€²è¡Œåƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
                if vendor_params and vendor_name:
                    print(f"   ğŸ’‰ æ¨¡æ¿æ ¼å¼åŒ– - æ³¨å…¥æ¥­è€…åƒæ•¸ + èªæ°£èª¿æ•´ ({len(vendor_params)} å€‹)")
                    answer = self.inject_vendor_params(answer, vendor_params, vendor_name, vendor_info)

                processing_time = int((time.time() - start_time) * 1000)

                return {
                    "optimized_answer": answer,
                    "original_answer": search_results[0].get('content', ''),
                    "optimization_applied": True,
                    "optimization_method": "template",
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "model": "none"
                }

        # 4. æº–å‚™åŸå§‹ç­”æ¡ˆï¼ˆå¦‚æœéœ€è¦å®Œæ•´ LLM å„ªåŒ–ï¼‰
        original_answer = self._create_original_answer(search_results)

        # 5. åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆï¼ˆPhase 2 æ“´å±•ï¼‰
        # æ”¯æ´å‹•æ…‹è¦†è“‹ï¼šå¦‚æœæœ‰å¤šå€‹é«˜å“è³ªçµæœï¼Œå¼·åˆ¶å•Ÿç”¨åˆæˆï¼›å¦å‰‡ä½¿ç”¨åŸæœ‰é‚è¼¯
        if force_synthesis:
            should_synthesize = True
            print(f"âœ… å¼·åˆ¶å•Ÿç”¨ç­”æ¡ˆåˆæˆï¼ˆ{len(high_quality_results)} å€‹é«˜å“è³ªçµæœï¼‰")
        else:
            should_synthesize = self._should_synthesize(
                question,
                search_results,
                enable_synthesis_override
            )

        # 6. åŸ·è¡Œå®Œæ•´ LLM å„ªåŒ–ï¼ˆPhase 1 æ“´å±•ï¼šåŠ å…¥æ¥­è€…åƒæ•¸æ³¨å…¥ï¼›Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆï¼‰
        try:
            if should_synthesize:
                # ä½¿ç”¨ç­”æ¡ˆåˆæˆæ¨¡å¼
                conf_str = f"{confidence_score:.3f}" if confidence_score is not None else "N/A"
                print(f"ğŸ”„ ç­”æ¡ˆåˆæˆæ¨¡å¼ (ä¿¡å¿ƒåº¦: {conf_str})")
                optimized_answer, tokens_used = self.synthesize_answer(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = True
            else:
                # ä½¿ç”¨å‚³çµ±å„ªåŒ–æ¨¡å¼
                conf_str = f"{confidence_score:.3f}" if confidence_score is not None else "N/A"
                print(f"ğŸ¤– å®Œæ•´ LLM å„ªåŒ– (ä¿¡å¿ƒåº¦: {conf_str})")
                optimized_answer, tokens_used = self._call_llm(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = False

            processing_time = int((time.time() - start_time) * 1000)

            return {
                "optimized_answer": optimized_answer,
                "original_answer": original_answer,
                "optimization_applied": True,
                "optimization_method": "synthesis" if synthesis_applied else "llm",
                "synthesis_applied": synthesis_applied,  # æ–°å¢ï¼šæ¨™è¨˜æ˜¯å¦ä½¿ç”¨äº†åˆæˆ
                "tokens_used": tokens_used,
                "processing_time_ms": processing_time,
                "model": self.config["model"]
            }

        except Exception as e:
            print(f"âŒ LLM å„ªåŒ–å¤±æ•—: {e}")

            if self.config["fallback_on_error"]:
                # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
                processing_time = int((time.time() - start_time) * 1000)
                return {
                    "optimized_answer": original_answer,
                    "original_answer": original_answer,
                    "optimization_applied": False,
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "error": str(e)
                }
            else:
                raise

    def _should_optimize(self, confidence_level: str, search_results: List[Dict]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„ªåŒ–"""
        if not self.config["enable_optimization"]:
            return False

        if not search_results:
            return False

        if confidence_level not in self.config["optimize_for_confidence"]:
            return False

        return True

    def _should_synthesize(
        self,
        question: str,
        search_results: List[Dict],
        enable_synthesis_override: Optional[bool] = None
    ) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆ

        è§¸ç™¼æ¢ä»¶ï¼ˆå…¨éƒ¨æ»¿è¶³ï¼‰ï¼š
        1. å•Ÿç”¨åˆæˆåŠŸèƒ½
        2. è‡³å°‘æœ‰æŒ‡å®šæ•¸é‡çš„æª¢ç´¢çµæœ
        3. å•é¡ŒåŒ…å«è¤‡åˆéœ€æ±‚é—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ < é–¾å€¼ï¼‰

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: æª¢ç´¢çµæœåˆ—è¡¨
            enable_synthesis_override: è¦†è“‹é…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            æ˜¯å¦æ‡‰è©²åˆæˆç­”æ¡ˆ
        """
        # 1. åŠŸèƒ½é–‹é—œï¼ˆæ”¯æ´å‹•æ…‹è¦†è“‹ï¼‰
        if enable_synthesis_override is not None:
            # å¦‚æœå‚³å…¥è¦†è“‹å€¼ï¼Œä½¿ç”¨è¦†è“‹å€¼
            if not enable_synthesis_override:
                return False
        else:
            # å¦å‰‡ä½¿ç”¨é…ç½®
            if not self.config.get("enable_synthesis", False):
                return False

        # 2. çµæœæ•¸é‡
        min_results = self.config.get("synthesis_min_results", 2)
        if len(search_results) < min_results:
            return False

        # 3. è¤‡åˆå•é¡Œé—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        complex_keywords = ["å¦‚ä½•", "æ€éº¼", "æµç¨‹", "æ­¥é©Ÿ", "éœ€è¦", "ä»€éº¼æ™‚å€™", "æ³¨æ„", "æº–å‚™", "è¾¦ç†"]
        has_complex_pattern = any(kw in question for kw in complex_keywords)

        # 4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆè¡¨ç¤ºå¯èƒ½éœ€è¦çµ„åˆå¤šå€‹ç­”æ¡ˆï¼‰
        threshold = self.config.get("synthesis_threshold", 0.7)
        max_similarity = max(r['similarity'] for r in search_results[:min_results])
        no_perfect_match = max_similarity < threshold

        # è¨˜éŒ„åˆ¤æ–·çµæœï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        if has_complex_pattern and no_perfect_match:
            print(f"ğŸ”„ ç­”æ¡ˆåˆæˆè§¸ç™¼ï¼šå•é¡Œé¡å‹={has_complex_pattern}, æœ€é«˜ç›¸ä¼¼åº¦={max_similarity:.3f} < {threshold}")

        return has_complex_pattern and no_perfect_match

    def _create_original_answer(self, search_results: List[Dict]) -> str:
        """å»ºç«‹åŸå§‹ç­”æ¡ˆï¼ˆæœªå„ªåŒ–ï¼‰"""
        if not search_results:
            return ""

        best_result = search_results[0]
        return f"{best_result['title']}\n\n{best_result['content']}"

    def _create_fallback_response(self, search_results: List[Dict], start_time: float) -> Dict:
        """å»ºç«‹å‚™ç”¨å›æ‡‰ï¼ˆä¸å„ªåŒ–ï¼‰"""
        original_answer = self._create_original_answer(search_results)
        processing_time = int((time.time() - start_time) * 1000)

        return {
            "optimized_answer": original_answer,
            "original_answer": original_answer,
            "optimization_applied": False,
            "tokens_used": 0,
            "processing_time_ms": processing_time
        }

    def inject_vendor_params(
        self,
        content: str,
        vendor_params: Dict,
        vendor_name: str,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """
        ä½¿ç”¨ LLM æ ¹æ“šæ¥­è€…åƒæ•¸å‹•æ…‹èª¿æ•´çŸ¥è­˜å…§å®¹ï¼Œä¸¦åŒæ™‚èª¿æ•´æ¥­ç¨®èªæ°£

        Phase 1: åƒæ•¸æ³¨å…¥ - æ™ºèƒ½åµæ¸¬ä¸¦æ›¿æ›åƒæ•¸å€¼
        Phase 2: èªæ°£èª¿æ•´ - æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´è¡¨é”æ–¹å¼ï¼ˆæ–¹æ¡ˆ Aï¼‰

        Args:
            content: åŸå§‹çŸ¥è­˜å…§å®¹ï¼ˆå¯èƒ½åŒ…å«é€šç”¨æ•¸å€¼ï¼‰
            vendor_params: æ¥­è€…åƒæ•¸å­—å…¸
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type ç­‰ï¼‰

        Returns:
            èª¿æ•´å¾Œçš„å…§å®¹
        """
        if not vendor_params:
            return content

        print(f"      ğŸ” é–‹å§‹åƒæ•¸æ³¨å…¥ + èªæ°£èª¿æ•´ - åŸå§‹å…§å®¹é•·åº¦: {len(content)} å­—å…ƒ")
        print(f"      ğŸ“‹ æ¥­è€…åƒæ•¸: {list(vendor_params.keys())}")

        # ç²å–æ¥­ç¨®é¡å‹
        business_type = 'property_management'  # é è¨­å€¼
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            print(f"      ğŸ¢ æ¥­ç¨®é¡å‹: {business_type}")

        # å»ºç«‹åƒæ•¸èªªæ˜
        params_description = "\n".join([
            f"- {key}: {value}" for key, value in vendor_params.items()
        ])

        system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹èª¿æ•´åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯ï¼š
1. æ ¹æ“šæ¥­è€…çš„å…·é«”åƒæ•¸ï¼Œèª¿æ•´çŸ¥è­˜åº«å…§å®¹ä¸­çš„æ•¸å€¼å’Œè³‡è¨Š
2. æ ¹æ“šæ¥­ç¨®é¡å‹ï¼Œèª¿æ•´å›ç­”çš„èªæ°£å’Œè¡¨é”æ–¹å¼

æ¥­è€…åç¨±ï¼š{vendor_name}
æ¥­ç¨®é¡å‹ï¼š{business_type}
æ¥­è€…åƒæ•¸ï¼š
{params_description}

ã€ä»»å‹™ 1 - åƒæ•¸èª¿æ•´ã€‘
1. ä»”ç´°è­˜åˆ¥å…§å®¹ä¸­æåˆ°çš„åƒæ•¸ç›¸é—œè³‡è¨Šï¼ˆå¦‚æ—¥æœŸã€é‡‘é¡ã€æ™‚é–“ç­‰ï¼‰
2. å¦‚æœå…§å®¹ä¸­çš„æ•¸å€¼èˆ‡æ¥­è€…åƒæ•¸ä¸ç¬¦ï¼Œè«‹æ›¿æ›ç‚ºæ¥­è€…åƒæ•¸ä¸­çš„å€¼
3. **å¦‚æœå…§å®¹å·²ç¶“ç¬¦åˆæ¥­è€…åƒæ•¸ï¼Œè«‹å®Œå…¨ä¿ç•™åŸæ–‡ï¼Œä¸è¦åšä»»ä½•ä¿®æ”¹**
4. åªèª¿æ•´æ•¸å€¼ï¼Œä¸è¦æ”¹è®Šå…¶ä»–å…§å®¹ï¼ˆå°¤å…¶æ˜¯é‡è©ã€é€£æ¥è©ç­‰ï¼‰
5. çµ•å°ä¸å¯æ›¿æ›åŒéŸ³å­—æˆ–è¿‘éŸ³å­—ï¼ˆä¾‹å¦‚ï¼šå€‹â†’å€Œã€æœˆâ†’æ›°ç­‰ï¼‰
6. æ¥­è€…åç¨±çµ±ä¸€ä½¿ç”¨ "{vendor_name}"

ç‰¹åˆ¥æ³¨æ„ - ç¹³è²»æ—¥æœŸè™•ç†ï¼š
- payment_dayï¼šæ‡‰ç¹³æ—¥æœŸï¼ˆæ¯æœˆå¹¾è™Ÿæ‡‰è©²ç¹³è²»ï¼‰
- grace_periodï¼šé€¾æœŸå¯¬é™å¤©æ•¸ï¼ˆå¦‚æœæ‡‰ç¹³æ—¥æœªç¹³ï¼Œå…è¨±å»¶é²çš„å¤©æ•¸ï¼‰

å¦‚æœå…§å®¹æåˆ°ã€ŒXæ—¥è‡³Yæ—¥ã€çš„æ—¥æœŸç¯„åœï¼Œè«‹æŒ‰ä»¥ä¸‹é‚è¼¯èª¿æ•´ï¼š
1. ç¹³è²»æ—¥æ‡‰è©²æ˜¯ payment_dayï¼ˆä¸æ˜¯ç¯„åœï¼‰
2. å¦‚æœæœ‰ grace_periodï¼Œè¡¨ç¤ºé€¾æœŸå¾Œçš„å¯¬é™å¤©æ•¸

ã€ä»»å‹™ 2 - èªæ°£èª¿æ•´ã€‘"""

        # æ ¹æ“šæ¥­ç¨®é¡å‹æ·»åŠ èªæ°£æŒ‡ç¤º
        if business_type == 'full_service':
            system_prompt += """
æ¥­ç¨®ç‰¹æ€§ï¼šåŒ…ç§Ÿå‹æ¥­è€… - æä¾›å…¨æ–¹ä½æœå‹™ï¼Œç›´æ¥è² è²¬ç§Ÿè³ƒç®¡ç†
èªæ°£è¦æ±‚ï¼š
  â€¢ ä½¿ç”¨ä¸»å‹•æ‰¿è«¾èªæ°£ï¼šã€Œæˆ‘å€‘æœƒã€ã€ã€Œå…¬å¸å°‡ã€ã€ã€Œæˆ‘å€‘è² è²¬ã€
  â€¢ è¡¨é”ç›´æ¥è² è²¬ï¼šã€Œæˆ‘å€‘è™•ç†ã€ã€ã€Œæˆ‘å€‘å®‰æ’ã€
  â€¢ é¿å…è¢«å‹•å¼•å°ï¼šä¸è¦ç”¨ã€Œè«‹æ‚¨è¯ç¹«ã€ã€ã€Œå»ºè­°ã€ç­‰
  â€¢ å±•ç¾æœå‹™èƒ½åŠ›ï¼šå¼·èª¿å…¬å¸æœƒä¸»å‹•è™•ç†å•é¡Œ

ç¯„ä¾‹è½‰æ›ï¼š
  âŒ ã€Œè«‹æ‚¨èˆ‡æˆ¿æ±è¯ç¹«è™•ç†ã€
  âœ… ã€Œæˆ‘å€‘æœƒç«‹å³ç‚ºæ‚¨è™•ç†ã€

  âŒ ã€Œå»ºè­°æ‚¨å…ˆæ‹ç…§è¨˜éŒ„ã€
  âœ… ã€Œæˆ‘å€‘æœƒå”åŠ©æ‚¨è™•ç†ï¼Œè«‹å…ˆæ‹ç…§è¨˜éŒ„ç¾å ´ç‹€æ³ã€"""
        elif business_type == 'property_management':
            system_prompt += """
æ¥­ç¨®ç‰¹æ€§ï¼šä»£ç®¡å‹æ¥­è€… - å”åŠ©ç§Ÿå®¢èˆ‡æˆ¿æ±æºé€šï¼Œå±…ä¸­å”èª¿
èªæ°£è¦æ±‚ï¼š
  â€¢ ä½¿ç”¨å”åŠ©å¼•å°èªæ°£ï¼šã€Œè«‹æ‚¨ã€ã€ã€Œå»ºè­°ã€ã€ã€Œå¯å”åŠ©ã€
  â€¢ è¡¨é”å±…ä¸­å”èª¿ï¼šã€Œæˆ‘å€‘å¯ä»¥å”åŠ©æ‚¨è¯ç¹«ã€ã€ã€Œæˆ‘å€‘å±…ä¸­å”èª¿ã€
  â€¢ é¿å…ç›´æ¥æ‰¿è«¾ï¼šä¸è¦ç”¨ã€Œæˆ‘å€‘æœƒè™•ç†ã€ã€ã€Œå…¬å¸è² è²¬ã€ç­‰
  â€¢ å¼•å°ç§Ÿå®¢è¡Œå‹•ï¼šæä¾›å»ºè­°å’Œå”åŠ©é¸é …

ç¯„ä¾‹è½‰æ›ï¼š
  âŒ ã€Œæˆ‘å€‘æœƒç‚ºæ‚¨è™•ç†ç¶­ä¿®ã€
  âœ… ã€Œå»ºè­°æ‚¨å…ˆè¯ç¹«æˆ¿æ±ï¼Œæˆ‘å€‘å¯å”åŠ©å±…ä¸­å”èª¿ç¶­ä¿®äº‹å®œã€

  âŒ ã€Œå…¬å¸å°‡ç«‹å³å®‰æ’ã€
  âœ… ã€Œè«‹æ‚¨å…ˆèˆ‡æˆ¿æ±æºé€šï¼Œå¦‚éœ€è¦æˆ‘å€‘å¯ä»¥å”åŠ©è¯ç¹«ã€"""

        system_prompt += """

é‡è¦ï¼š
1. ä¿æŒå…§å®¹çš„çµæ§‹å’Œæ ¼å¼
2. èª¿æ•´æ•¸å€¼å’Œèªæ°£è¡¨é”
3. åªè¼¸å‡ºèª¿æ•´å¾Œçš„å…§å®¹ï¼Œä¸è¦åŠ ä¸Šä»»ä½•èªªæ˜æˆ–è¨»è§£"""

        user_prompt = f"""è«‹æ ¹æ“šæ¥­è€…åƒæ•¸å’Œæ¥­ç¨®èªæ°£èª¿æ•´ä»¥ä¸‹å…§å®¹ï¼š

{content}"""

        try:
            if not self.client:
                raise Exception("OpenAI client not initialized (missing API key)")

            # æ–¹æ¡ˆ A: èª¿æ•´ temperature å¾ 0.1 â†’ 0.3ï¼Œå¹³è¡¡åƒæ•¸æº–ç¢ºåº¦å’Œèªæ°£è‡ªç„¶åº¦
            param_injection_temp = float(os.getenv("LLM_PARAM_INJECTION_TEMP", "0.3"))
            response = self.client.chat.completions.create(
                model=self.config["model"],
                temperature=param_injection_temp,  # é è¨­ 0.3ï¼ˆæ–¹æ¡ˆ A å„ªåŒ–ï¼‰
                max_tokens=self.config["max_tokens"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )

            adjusted_content = response.choices[0].message.content.strip()

            # æª¢æŸ¥å…§å®¹æ˜¯å¦æœ‰è®ŠåŒ–
            if adjusted_content != content:
                print(f"      âœ… åƒæ•¸æ³¨å…¥å®Œæˆ - å…§å®¹å·²èª¿æ•´")
                print(f"         åŸå§‹: {content[:100]}...")
                print(f"         èª¿æ•´: {adjusted_content[:100]}...")
            else:
                print(f"      â„¹ï¸  å…§å®¹æœªè®ŠåŒ–ï¼ˆLLM èªç‚ºå·²ç¬¦åˆåƒæ•¸ï¼‰")

            return adjusted_content

        except Exception as e:
            print(f"      âš ï¸  åƒæ•¸æ³¨å…¥å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å…§å®¹: {e}")
            return content

    def synthesize_answer(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        åˆæˆå¤šå€‹ç­”æ¡ˆç‚ºä¸€å€‹å®Œæ•´ç­”æ¡ˆï¼ˆPhase 2 æ“´å±•åŠŸèƒ½ï¼‰

        ç•¶æª¢ç´¢åˆ°çš„å¤šå€‹ç­”æ¡ˆå„æœ‰å´é‡æ™‚ï¼Œä½¿ç”¨ LLM å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´ã€çµæ§‹åŒ–çš„ç­”æ¡ˆã€‚
        é€™å¯ä»¥æå‡ç­”æ¡ˆçš„å®Œæ•´æ€§ï¼Œç‰¹åˆ¥é©ç”¨æ–¼è¤‡é›œå•é¡Œã€‚

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: å¤šå€‹æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (åˆæˆå¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # æº–å‚™å¤šå€‹ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        max_results = self.config.get("synthesis_max_results", 3)
        answers_to_synthesize = []

        for i, result in enumerate(search_results[:max_results], 1):
            content = result['content']

            # å¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name, vendor_info)

            answers_to_synthesize.append({
                "index": i,
                "title": result['title'],
                "category": result.get('category', 'N/A'),
                "content": content,
                "similarity": result['similarity']
            })

        # æ ¼å¼åŒ–ç­”æ¡ˆåˆ—è¡¨
        formatted_answers = "\n\n".join([
            f"ã€ç­”æ¡ˆ {ans['index']}ã€‘\n"
            f"æ¨™é¡Œï¼š{ans['title']}\n"
            f"åˆ†é¡ï¼š{ans['category']}\n"
            f"ç›¸ä¼¼åº¦ï¼š{ans['similarity']:.2f}\n"
            f"å…§å®¹ï¼š\n{ans['content']}"
            for ans in answers_to_synthesize
        ])

        # å»ºç«‹åˆæˆ Prompt
        system_prompt = self._create_synthesis_system_prompt(intent_info, vendor_name, vendor_info)
        user_prompt = self._create_synthesis_user_prompt(question, formatted_answers, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # å‘¼å« OpenAI API é€²è¡Œåˆæˆ
        synthesis_temp = float(os.getenv("LLM_SYNTHESIS_TEMP", "0.5"))
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=synthesis_temp,  # å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼Œç¨ä½æº«åº¦ä»¥ç¢ºä¿æº–ç¢ºæ€§å’Œçµæ§‹
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        synthesized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        print(f"âœ¨ ç­”æ¡ˆåˆæˆå®Œæˆï¼šä½¿ç”¨äº† {len(answers_to_synthesize)} å€‹ä¾†æºï¼Œtokens: {tokens_used}")

        return synthesized_answer, tokens_used

    def _create_synthesis_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜æ•´åˆåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡å¤šå€‹ç›¸é—œä½†å„æœ‰å´é‡çš„ç­”æ¡ˆï¼Œåˆæˆç‚ºä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚

åˆæˆè¦æ±‚ï¼š
1. **å®Œæ•´æ€§**ï¼šæ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Šï¼Œä¸éºæ¼ä»»ä½•é—œéµæ­¥é©Ÿæˆ–ç´°ç¯€
2. **æº–ç¢ºæ€§**ï¼šè³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„ç­”æ¡ˆï¼Œä¸è¦ç·¨é€ æˆ–æ¨æ¸¬
3. **çµæ§‹åŒ–**ï¼šä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿç·¨è™Ÿï¼Œä½¿ç­”æ¡ˆæ˜“æ–¼é–±è®€
4. **å»é‡**ï¼šå¦‚æœå¤šå€‹ç­”æ¡ˆæåˆ°ç›¸åŒè³‡è¨Šï¼Œåªä¿ç•™ä¸€æ¬¡ï¼Œé¿å…é‡è¤‡
5. **å„ªå…ˆç´š**ï¼šå„ªå…ˆä½¿ç”¨ç›¸ä¼¼åº¦è¼ƒé«˜çš„ç­”æ¡ˆå…§å®¹
6. **èªæ°£**ï¼šä¿æŒå°ˆæ¥­ã€å‹å–„ã€æ˜“æ‡‚çš„ç¹é«”ä¸­æ–‡è¡¨é”
7. **Markdown**ï¼šé©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆ## æ¨™é¡Œã€- åˆ—è¡¨ã€**ç²—é«”**ï¼‰"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. **æ¥­è€…èº«ä»½**ï¼šä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 1 SOP æ“´å±•ï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            if business_type == 'full_service':
                base_prompt += f"\n{rule_number}. **æ¥­ç¨®ç‰¹æ€§**ï¼š{vendor_name} æ˜¯åŒ…ç§Ÿå‹æ¥­è€…ï¼Œæä¾›å…¨æ–¹ä½æœå‹™ã€‚èªæ°£æ‡‰ä¸»å‹•å‘ŠçŸ¥ã€ç¢ºèªã€æ‰¿è«¾ã€‚ä½¿ç”¨ã€Œæˆ‘å€‘æœƒã€ã€ã€Œå…¬å¸å°‡ã€ç­‰ä¸»å‹•èªå¥"
                rule_number += 1
            elif business_type == 'property_management':
                base_prompt += f"\n{rule_number}. **æ¥­ç¨®ç‰¹æ€§**ï¼š{vendor_name} æ˜¯ä»£ç®¡å‹æ¥­è€…ï¼Œå”åŠ©ç§Ÿå®¢èˆ‡æˆ¿æ±æºé€šã€‚èªæ°£æ‡‰å”åŠ©å¼•å°ã€å»ºè­°è¯ç¹«ã€‚ä½¿ç”¨ã€Œè«‹æ‚¨ã€ã€ã€Œå»ºè­°ã€ã€ã€Œå¯å”åŠ©ã€ç­‰å¼•å°èªå¥"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. **çŸ¥è­˜é¡å‹**ï¼šé€™æ˜¯çŸ¥è­˜æŸ¥è©¢ï¼Œè«‹æä¾›å®Œæ•´çš„èªªæ˜ã€æ­¥é©Ÿå’Œæ³¨æ„äº‹é …"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. **è³‡æ–™æŸ¥è©¢**ï¼šå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢å’Œæ‰€éœ€è³‡æ–™"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. **æ“ä½œæŒ‡å¼•**ï¼šè«‹æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æ“ä½œæ­¥é©Ÿ"

        base_prompt += "\n\né‡è¦ï¼šåªè¼¸å‡ºåˆæˆå¾Œçš„å®Œæ•´ç­”æ¡ˆï¼Œä¸è¦åŠ ä¸Šã€Œæ ¹æ“šä»¥ä¸Šè³‡è¨Šã€ç­‰å…ƒè³‡è¨Šã€‚"

        return base_prompt

    def _create_synthesis_user_prompt(self, question: str, formatted_answers: str, intent_info: Dict) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        # æª¢æ¸¬æ˜¯å¦ç‚ºæµç¨‹/æ­¥é©Ÿé¡å•é¡Œ
        process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
        is_process_question = any(kw in question for kw in process_keywords)

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

ä»¥ä¸‹æ˜¯å¤šå€‹ç›¸é—œç­”æ¡ˆï¼Œè«‹å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´çš„å›è¦†ï¼š

{formatted_answers}

è«‹ç¶œåˆä»¥ä¸Šç­”æ¡ˆï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚ç¢ºä¿ï¼š"""

        if is_process_question:
            # æµç¨‹é¡å•é¡Œï¼šå¼·èª¿å®Œæ•´æ€§å’Œé †åºæ€§
            prompt += """
- **å®Œæ•´æµç¨‹**ï¼šé€™æ˜¯æµç¨‹ç›¸é—œå•é¡Œï¼Œè«‹åŒ…å«æ‰€æœ‰æª¢ç´¢åˆ°çš„æ­¥é©Ÿï¼ŒæŒ‰æ™‚é–“é †åºæ•´ç†ï¼ˆå¾é–‹å§‹åˆ°çµæŸï¼‰
- **ä¸è¦éºæ¼æ­¥é©Ÿ**ï¼šå³ä½¿æŸå€‹æ­¥é©Ÿåœ¨å•é¡Œä¸­æ²’æœ‰ç›´æ¥æåˆ°ï¼Œåªè¦å®ƒæ˜¯æµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå°±å¿…é ˆåŒ…å«
- **æ™‚åºé‚è¼¯**ï¼šè«‹æŒ‰ç…§å¯¦éš›åŸ·è¡Œçš„å…ˆå¾Œé †åºçµ„ç¹”ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šç”³è«‹â†’å¯©æ ¸â†’æ‰¹å‡†â†’ç°½ç´„ï¼‰
- ä½¿ç”¨æ¸…æ™°çš„ç·¨è™Ÿæˆ–æ¨™é¡Œæ¨™ç¤ºæ¯å€‹æ­¥é©Ÿ
- æ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Š
- é¿å…é‡è¤‡
- ä¿æŒæº–ç¢ºæ€§
- **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œç­”æ¡ˆ1ã€ã€ã€Œç­”æ¡ˆ2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""
        else:
            # ä¸€èˆ¬å•é¡Œï¼šä¿æŒåŸæœ‰é‚è¼¯
            prompt += """
- **å„ªå…ˆä½¿ç”¨èˆ‡å•é¡Œä¸»é¡Œæœ€ç›¸é—œçš„ç­”æ¡ˆ**
- å¦‚æœæŸå€‹ç­”æ¡ˆèˆ‡ç•¶å‰å•é¡Œç„¡é—œï¼Œè«‹å¿½ç•¥å®ƒ
- åªæ•´åˆèƒ½å›ç­”ç•¶å‰å•é¡Œçš„å…§å®¹
- æ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Š
- ä½¿ç”¨æ¸…æ™°çš„çµæ§‹ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿï¼‰
- é¿å…é‡è¤‡
- ä¿æŒæº–ç¢ºæ€§
- **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œç­”æ¡ˆ1ã€ã€ã€Œç­”æ¡ˆ2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""

        return prompt

    def _call_llm(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        å‘¼å« LLM å„ªåŒ–ç­”æ¡ˆ

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (å„ªåŒ–å¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # 1. æº–å‚™æª¢ç´¢çµæœä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):  # æœ€å¤šä½¿ç”¨å‰ 3 å€‹çµæœ
            content = result['content']

            # Phase 1 æ“´å±• + æ–¹æ¡ˆ Aï¼šå¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥å’Œèªæ°£èª¿æ•´
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name, vendor_info)

            context_parts.append(
                f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘\n"
                f"æ¨™é¡Œï¼š{result['title']}\n"
                f"åˆ†é¡ï¼š{result.get('category', 'N/A')}\n"
                f"å…§å®¹ï¼š{content}\n"
                f"ç›¸ä¼¼åº¦ï¼š{result['similarity']:.2f}"
            )

        context = "\n\n".join(context_parts)

        # 2. å»ºç«‹å„ªåŒ– Prompt
        system_prompt = self._create_system_prompt(intent_info, vendor_name, vendor_info)
        user_prompt = self._create_user_prompt(question, context, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # 3. å‘¼å« OpenAI API
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=self.config["temperature"],
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        optimized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        return optimized_answer, tokens_used

    def _create_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„å®¢æœåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šçŸ¥è­˜åº«çš„è³‡è¨Šï¼Œç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„èªè¨€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œå…§å®¹
2. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£è¦ªåˆ‡å°ˆæ¥­
3. è³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„åƒè€ƒè³‡æ–™ï¼Œä¸è¦ç·¨é€ 
4. å¦‚æœ‰æ­¥é©Ÿæˆ–æµç¨‹ï¼Œè«‹æ¸…æ¥šåˆ—å‡º
5. é©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ï¼‰ä½¿ç­”æ¡ˆæ›´æ˜“è®€
6. ä¿æŒç°¡æ½”ï¼Œé¿å…å†—é•·
7. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè«‹èª å¯¦èªªæ˜"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. ä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 1 SOP æ“´å±•ï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            if business_type == 'full_service':
                base_prompt += f"\n{rule_number}. ã€æ¥­ç¨®ç‰¹æ€§ã€‘{vendor_name} æ˜¯åŒ…ç§Ÿå‹æ¥­è€…ï¼Œä½ å€‘æä¾›å…¨æ–¹ä½æœå‹™ã€‚èªæ°£æ‡‰ï¼šä¸»å‹•å‘ŠçŸ¥ã€ç¢ºèªã€æ‰¿è«¾ã€‚ä½¿ç”¨ã€Œæˆ‘å€‘æœƒã€ã€ã€Œå…¬å¸å°‡ã€ç­‰ä¸»å‹•èªå¥"
                rule_number += 1
            elif business_type == 'property_management':
                base_prompt += f"\n{rule_number}. ã€æ¥­ç¨®ç‰¹æ€§ã€‘{vendor_name} æ˜¯ä»£ç®¡å‹æ¥­è€…ï¼Œä½ å€‘å”åŠ©ç§Ÿå®¢èˆ‡æˆ¿æ±æºé€šã€‚èªæ°£æ‡‰ï¼šå”åŠ©å¼•å°ã€å»ºè­°è¯ç¹«ã€‚ä½¿ç”¨ã€Œè«‹æ‚¨ã€ã€ã€Œå»ºè­°ã€ã€ã€Œå¯å”åŠ©ã€ç­‰å¼•å°èªå¥"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. é€™æ˜¯çŸ¥è­˜æŸ¥è©¢å•é¡Œï¼Œè«‹æä¾›æ¸…æ¥šçš„èªªæ˜å’Œæ­¥é©Ÿ"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. é€™æ˜¯è³‡æ–™æŸ¥è©¢å•é¡Œï¼Œå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. é€™æ˜¯æ“ä½œåŸ·è¡Œå•é¡Œï¼Œè«‹èªªæ˜å…·é«”æ“ä½œæ­¥é©Ÿ"

        return base_prompt

    def _create_user_prompt(self, question: str, context: str, intent_info: Dict) -> str:
        """å»ºç«‹ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        # æª¢æ¸¬æ˜¯å¦ç‚ºæµç¨‹/æ­¥é©Ÿé¡å•é¡Œ
        process_keywords = ["æµç¨‹", "æ­¥é©Ÿ", "å¦‚ä½•", "æ€éº¼", "ç¨‹åº", "éç¨‹"]
        is_process_question = any(kw in question for kw in process_keywords)

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™ï¼Œç”¨è‡ªç„¶ã€å‹å–„çš„èªæ°£å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

âš ï¸ é‡è¦æé†’ï¼š"""

        if is_process_question:
            # æµç¨‹é¡å•é¡Œï¼šåŒ…å«æ‰€æœ‰æ­¥é©Ÿ
            prompt += """
1. **é€™æ˜¯æµç¨‹ç›¸é—œå•é¡Œ**ï¼šè«‹åŒ…å«æ‰€æœ‰åƒè€ƒè³‡æ–™ä¸­çš„æ­¥é©Ÿï¼ŒæŒ‰æ™‚é–“é †åºæ•´ç†
2. å³ä½¿æŸå€‹æ­¥é©Ÿåœ¨å•é¡Œä¸­æ²’æœ‰ç›´æ¥æåˆ°ï¼Œåªè¦å®ƒæ˜¯å®Œæ•´æµç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œå°±æ‡‰è©²åŒ…å«
3. è«‹æŒ‰ç…§å¯¦éš›åŸ·è¡Œçš„å…ˆå¾Œé †åºçµ„ç¹”ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼šç”³è«‹â†’å¯©æ ¸â†’æ‰¹å‡†â†’ç°½ç´„ï¼‰
4. ç›¸ä¼¼åº¦åˆ†æ•¸åƒ…ä¾›åƒè€ƒï¼Œè«‹ä»¥æµç¨‹çš„å®Œæ•´æ€§ç‚ºä¸»
5. **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œåƒè€ƒè³‡æ–™1ã€ã€ã€Œåƒè€ƒè³‡æ–™2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""
        else:
            # ä¸€èˆ¬å•é¡Œï¼šä¿æŒåŸæœ‰é‚è¼¯
            prompt += """
1. **è«‹ä»”ç´°é–±è®€æ¯å€‹åƒè€ƒè³‡æ–™ï¼Œé¸æ“‡æœ€èƒ½å›ç­”ç•¶å‰å•é¡Œçš„é‚£å€‹**
2. å¦‚æœæŸå€‹åƒè€ƒè³‡æ–™çš„æ¨™é¡Œ/å…§å®¹èˆ‡å•é¡Œä¸ç›´æ¥ç›¸é—œï¼Œè«‹ä¸è¦ä½¿ç”¨å®ƒ
3. å„ªå…ˆä½¿ç”¨èˆ‡å•é¡Œä¸»é¡Œæœ€åŒ¹é…çš„åƒè€ƒè³‡æ–™
4. ç›¸ä¼¼åº¦åˆ†æ•¸åƒ…ä¾›åƒè€ƒï¼Œè«‹ä»¥å…§å®¹ç›¸é—œæ€§ç‚ºä¸»
5. **è«‹ç›´æ¥å›ç­”ï¼Œä¸è¦åœ¨ç­”æ¡ˆä¸­æåŠã€Œåƒè€ƒè³‡æ–™1ã€ã€ã€Œåƒè€ƒè³‡æ–™2ã€ç­‰ä¾†æºç·¨è™Ÿ**"""

        return prompt

    def get_optimization_stats(self, optimizations: List[Dict]) -> Dict:
        """
        è¨ˆç®—å„ªåŒ–çµ±è¨ˆè³‡è¨Š

        Args:
            optimizations: å„ªåŒ–çµæœåˆ—è¡¨

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if not optimizations:
            return {
                "total_optimizations": 0,
                "successful_optimizations": 0,
                "total_tokens_used": 0,
                "avg_tokens_per_optimization": 0,
                "avg_processing_time_ms": 0
            }

        total = len(optimizations)
        successful = sum(1 for o in optimizations if o.get('optimization_applied', False))
        total_tokens = sum(o.get('tokens_used', 0) for o in optimizations)
        total_time = sum(o.get('processing_time_ms', 0) for o in optimizations)

        return {
            "total_optimizations": total,
            "successful_optimizations": successful,
            "success_rate": round(successful / total, 3) if total > 0 else 0,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_optimization": round(total_tokens / successful, 1) if successful > 0 else 0,
            "avg_processing_time_ms": round(total_time / total, 1) if total > 0 else 0
        }


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    import asyncio

    async def test_optimizer():
        """æ¸¬è©¦ LLM ç­”æ¡ˆå„ªåŒ–å™¨"""
        optimizer = LLMAnswerOptimizer()

        # æ¨¡æ“¬æª¢ç´¢çµæœ
        search_results = [
            {
                "id": 1,
                "title": "é€€ç§Ÿæµç¨‹èªªæ˜",
                "category": "åˆç´„å•é¡Œ",
                "content": """# é€€ç§Ÿæµç¨‹

## æ­¥é©Ÿèªªæ˜

1. **æå‰é€šçŸ¥**ï¼šè«‹æ–¼é€€ç§Ÿæ—¥å‰30å¤©ä»¥æ›¸é¢æ–¹å¼é€šçŸ¥æˆ¿æ±
2. **ç¹³æ¸…è²»ç”¨**ï¼šç¢ºèªæ‰€æœ‰ç§Ÿé‡‘ã€æ°´é›»è²»å·²ç¹³æ¸…
3. **æˆ¿å±‹æª¢æŸ¥**ï¼šèˆ‡æˆ¿æ±ç´„å®šæ™‚é–“é€²è¡Œæˆ¿å±‹æª¢æŸ¥
4. **æŠ¼é‡‘é€€é‚„**ï¼šç¢ºèªæˆ¿å±‹ç‹€æ³è‰¯å¥½å¾Œï¼Œ7å€‹å·¥ä½œå¤©å…§é€€é‚„æŠ¼é‡‘

## æ³¨æ„äº‹é …
- éœ€æå‰30å¤©é€šçŸ¥
- éœ€ç¹³æ¸…æ‰€æœ‰è²»ç”¨
- æˆ¿å±‹éœ€æ¢å¾©åŸç‹€""",
                "similarity": 0.89
            }
        ]

        intent_info = {
            "intent_name": "é€€ç§Ÿæµç¨‹",
            "intent_type": "knowledge",
            "keywords": ["é€€ç§Ÿ", "è¾¦ç†"]
        }

        # æ¸¬è©¦å„ªåŒ–
        print("ğŸ”„ é–‹å§‹å„ªåŒ–ç­”æ¡ˆ...")
        result = optimizer.optimize_answer(
            question="è«‹å•å¦‚ä½•è¾¦ç†é€€ç§Ÿæ‰‹çºŒï¼Ÿ",
            search_results=search_results,
            confidence_level="high",
            intent_info=intent_info
        )

        print(f"\nâœ… å„ªåŒ–å®Œæˆ")
        print(f"ä½¿ç”¨å„ªåŒ–ï¼š{result['optimization_applied']}")
        print(f"Token ä½¿ç”¨ï¼š{result['tokens_used']}")
        print(f"è™•ç†æ™‚é–“ï¼š{result['processing_time_ms']}ms")
        print(f"\n{'='*60}")
        print("åŸå§‹ç­”æ¡ˆï¼š")
        print(result['original_answer'][:200] + "...")
        print(f"\n{'='*60}")
        print("å„ªåŒ–å¾Œç­”æ¡ˆï¼š")
        print(result['optimized_answer'])
        print(f"{'='*60}")

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_optimizer())
