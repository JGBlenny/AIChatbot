"""
LLM ç­”æ¡ˆå„ªåŒ–æœå‹™
ä½¿ç”¨ GPT æ¨¡å‹å„ªåŒ– RAG æª¢ç´¢çµæœï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´ç²¾æº–çš„ç­”æ¡ˆ
Phase 1 æ“´å±•ï¼šæ”¯æ´æ¥­è€…åƒæ•¸å‹•æ…‹æ³¨å…¥
"""
import os
import re
from typing import List, Dict, Optional
from openai import OpenAI
import time


class LLMAnswerOptimizer:
    """LLM ç­”æ¡ˆå„ªåŒ–å™¨"""

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ– LLM ç­”æ¡ˆå„ªåŒ–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        # å»¶é²åˆå§‹åŒ–ï¼šåªæœ‰åœ¨éœ€è¦æ™‚æ‰æª¢æŸ¥ API key
        api_key = os.getenv("OPENAI_API_KEY")
        self.client = OpenAI(api_key=api_key) if api_key else None

        # å¾ç’°å¢ƒè®Šæ•¸è®€å–æ¨¡å‹é…ç½®ï¼ˆç”¨æ–¼é™ä½æ¸¬è©¦æˆæœ¬ï¼‰
        default_model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        # é è¨­é…ç½®
        default_config = {
            "model": default_model,
            "temperature": 0.7,
            "max_tokens": 800,
            "enable_optimization": True,
            "optimize_for_confidence": ["high", "medium"],  # åªå„ªåŒ–é«˜/ä¸­ä¿¡å¿ƒåº¦
            "fallback_on_error": True,  # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
            # Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆåŠŸèƒ½
            "enable_synthesis": False,  # æ˜¯å¦å•Ÿç”¨ç­”æ¡ˆåˆæˆï¼ˆé è¨­é—œé–‰ï¼Œéœ€æ¸¬è©¦å¾Œå•Ÿç”¨ï¼‰
            "synthesis_min_results": 2,  # æœ€å°‘éœ€è¦å¹¾å€‹çµæœæ‰è€ƒæ…®åˆæˆ
            "synthesis_max_results": 3,  # æœ€å¤šåˆæˆå¹¾å€‹ç­”æ¡ˆ
            "synthesis_threshold": 0.7   # ç•¶æœ€é«˜ç›¸ä¼¼åº¦ä½æ–¼æ­¤å€¼æ™‚ï¼Œè€ƒæ…®åˆæˆ
        }

        # åˆä½µç”¨æˆ¶é…ç½®èˆ‡é è¨­é…ç½®
        if config:
            self.config = {**default_config, **config}
        else:
            self.config = default_config

    def optimize_answer(
        self,
        question: str,
        search_results: List[Dict],
        confidence_level: str,
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None,
        enable_synthesis_override: Optional[bool] = None
    ) -> Dict:
        """
        å„ªåŒ–ç­”æ¡ˆ

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: RAG æª¢ç´¢çµæœåˆ—è¡¨
            confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_name: æ¥­è€…åç¨±ï¼ˆPhase 1 æ“´å±•ï¼‰
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼ŒPhase 1 SOP æ“´å±•ï¼‰
            enable_synthesis_override: è¦†è“‹ç­”æ¡ˆåˆæˆé…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            å„ªåŒ–çµæœå­—å…¸ï¼ŒåŒ…å«:
            - optimized_answer: å„ªåŒ–å¾Œçš„ç­”æ¡ˆ
            - original_answer: åŸå§‹ç­”æ¡ˆ
            - optimization_applied: æ˜¯å¦ä½¿ç”¨äº†å„ªåŒ–
            - synthesis_applied: æ˜¯å¦ä½¿ç”¨äº†ç­”æ¡ˆåˆæˆ
            - tokens_used: ä½¿ç”¨çš„ token æ•¸
            - processing_time_ms: è™•ç†æ™‚é–“
        """
        start_time = time.time()

        # 1. æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ–
        if not self._should_optimize(confidence_level, search_results):
            return self._create_fallback_response(search_results, start_time)

        # 2. æº–å‚™åŸå§‹ç­”æ¡ˆ
        original_answer = self._create_original_answer(search_results)

        # 3. åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆï¼ˆPhase 2 æ“´å±•ï¼‰
        # æ”¯æ´å‹•æ…‹è¦†è“‹ï¼šå¦‚æœå‚³å…¥ enable_synthesis_overrideï¼Œå‰‡ä½¿ç”¨è©²å€¼
        should_synthesize = self._should_synthesize(
            question,
            search_results,
            enable_synthesis_override
        )

        # 4. å˜—è©¦ LLM å„ªåŒ–ï¼ˆPhase 1 æ“´å±•ï¼šåŠ å…¥æ¥­è€…åƒæ•¸æ³¨å…¥ï¼›Phase 2 æ“´å±•ï¼šç­”æ¡ˆåˆæˆï¼‰
        try:
            if should_synthesize:
                # ä½¿ç”¨ç­”æ¡ˆåˆæˆæ¨¡å¼
                optimized_answer, tokens_used = self.synthesize_answer(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = True
            else:
                # ä½¿ç”¨å‚³çµ±å„ªåŒ–æ¨¡å¼
                optimized_answer, tokens_used = self._call_llm(
                    question=question,
                    search_results=search_results,
                    intent_info=intent_info,
                    vendor_params=vendor_params,
                    vendor_name=vendor_name,
                    vendor_info=vendor_info
                )
                synthesis_applied = False

            processing_time = int((time.time() - start_time) * 1000)

            return {
                "optimized_answer": optimized_answer,
                "original_answer": original_answer,
                "optimization_applied": True,
                "synthesis_applied": synthesis_applied,  # æ–°å¢ï¼šæ¨™è¨˜æ˜¯å¦ä½¿ç”¨äº†åˆæˆ
                "tokens_used": tokens_used,
                "processing_time_ms": processing_time,
                "model": self.config["model"]
            }

        except Exception as e:
            print(f"âŒ LLM å„ªåŒ–å¤±æ•—: {e}")

            if self.config["fallback_on_error"]:
                # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
                processing_time = int((time.time() - start_time) * 1000)
                return {
                    "optimized_answer": original_answer,
                    "original_answer": original_answer,
                    "optimization_applied": False,
                    "synthesis_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "error": str(e)
                }
            else:
                raise

    def _should_optimize(self, confidence_level: str, search_results: List[Dict]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„ªåŒ–"""
        if not self.config["enable_optimization"]:
            return False

        if not search_results:
            return False

        if confidence_level not in self.config["optimize_for_confidence"]:
            return False

        return True

    def _should_synthesize(
        self,
        question: str,
        search_results: List[Dict],
        enable_synthesis_override: Optional[bool] = None
    ) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦ç­”æ¡ˆåˆæˆ

        è§¸ç™¼æ¢ä»¶ï¼ˆå…¨éƒ¨æ»¿è¶³ï¼‰ï¼š
        1. å•Ÿç”¨åˆæˆåŠŸèƒ½
        2. è‡³å°‘æœ‰æŒ‡å®šæ•¸é‡çš„æª¢ç´¢çµæœ
        3. å•é¡ŒåŒ…å«è¤‡åˆéœ€æ±‚é—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆæœ€é«˜ç›¸ä¼¼åº¦ < é–¾å€¼ï¼‰

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: æª¢ç´¢çµæœåˆ—è¡¨
            enable_synthesis_override: è¦†è“‹é…ç½®ï¼ˆNone=ä½¿ç”¨é…ç½®ï¼ŒTrue=å¼·åˆ¶å•Ÿç”¨ï¼ŒFalse=å¼·åˆ¶ç¦ç”¨ï¼‰

        Returns:
            æ˜¯å¦æ‡‰è©²åˆæˆç­”æ¡ˆ
        """
        # 1. åŠŸèƒ½é–‹é—œï¼ˆæ”¯æ´å‹•æ…‹è¦†è“‹ï¼‰
        if enable_synthesis_override is not None:
            # å¦‚æœå‚³å…¥è¦†è“‹å€¼ï¼Œä½¿ç”¨è¦†è“‹å€¼
            if not enable_synthesis_override:
                return False
        else:
            # å¦å‰‡ä½¿ç”¨é…ç½®
            if not self.config.get("enable_synthesis", False):
                return False

        # 2. çµæœæ•¸é‡
        min_results = self.config.get("synthesis_min_results", 2)
        if len(search_results) < min_results:
            return False

        # 3. è¤‡åˆå•é¡Œé—œéµå­—ï¼ˆé€™é¡å•é¡Œé€šå¸¸éœ€è¦å¤šæ–¹é¢è³‡è¨Šï¼‰
        complex_keywords = ["å¦‚ä½•", "æ€éº¼", "æµç¨‹", "æ­¥é©Ÿ", "éœ€è¦", "ä»€éº¼æ™‚å€™", "æ³¨æ„", "æº–å‚™", "è¾¦ç†"]
        has_complex_pattern = any(kw in question for kw in complex_keywords)

        # 4. æ²’æœ‰å–®ä¸€é«˜åˆ†ç­”æ¡ˆï¼ˆè¡¨ç¤ºå¯èƒ½éœ€è¦çµ„åˆå¤šå€‹ç­”æ¡ˆï¼‰
        threshold = self.config.get("synthesis_threshold", 0.7)
        max_similarity = max(r['similarity'] for r in search_results[:min_results])
        no_perfect_match = max_similarity < threshold

        # è¨˜éŒ„åˆ¤æ–·çµæœï¼ˆç”¨æ–¼èª¿è©¦ï¼‰
        if has_complex_pattern and no_perfect_match:
            print(f"ğŸ”„ ç­”æ¡ˆåˆæˆè§¸ç™¼ï¼šå•é¡Œé¡å‹={has_complex_pattern}, æœ€é«˜ç›¸ä¼¼åº¦={max_similarity:.3f} < {threshold}")

        return has_complex_pattern and no_perfect_match

    def _create_original_answer(self, search_results: List[Dict]) -> str:
        """å»ºç«‹åŸå§‹ç­”æ¡ˆï¼ˆæœªå„ªåŒ–ï¼‰"""
        if not search_results:
            return ""

        best_result = search_results[0]
        return f"{best_result['title']}\n\n{best_result['content']}"

    def _create_fallback_response(self, search_results: List[Dict], start_time: float) -> Dict:
        """å»ºç«‹å‚™ç”¨å›æ‡‰ï¼ˆä¸å„ªåŒ–ï¼‰"""
        original_answer = self._create_original_answer(search_results)
        processing_time = int((time.time() - start_time) * 1000)

        return {
            "optimized_answer": original_answer,
            "original_answer": original_answer,
            "optimization_applied": False,
            "tokens_used": 0,
            "processing_time_ms": processing_time
        }

    def inject_vendor_params(
        self,
        content: str,
        vendor_params: Dict,
        vendor_name: str
    ) -> str:
        """
        ä½¿ç”¨ LLM æ ¹æ“šæ¥­è€…åƒæ•¸å‹•æ…‹èª¿æ•´çŸ¥è­˜å…§å®¹
        ä¸ä½¿ç”¨æ¨¡æ¿è®Šæ•¸ï¼Œè€Œæ˜¯æ™ºèƒ½åµæ¸¬ä¸¦æ›¿æ›åƒæ•¸

        Args:
            content: åŸå§‹çŸ¥è­˜å…§å®¹ï¼ˆå¯èƒ½åŒ…å«é€šç”¨æ•¸å€¼ï¼‰
            vendor_params: æ¥­è€…åƒæ•¸å­—å…¸
            vendor_name: æ¥­è€…åç¨±

        Returns:
            èª¿æ•´å¾Œçš„å…§å®¹
        """
        if not vendor_params:
            return content

        # å»ºç«‹åƒæ•¸èªªæ˜
        params_description = "\n".join([
            f"- {key}: {value}" for key, value in vendor_params.items()
        ])

        system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å…§å®¹èª¿æ•´åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæ¥­è€…çš„å…·é«”åƒæ•¸ï¼Œèª¿æ•´çŸ¥è­˜åº«å…§å®¹ä¸­çš„æ•¸å€¼å’Œè³‡è¨Šã€‚

æ¥­è€…åç¨±ï¼š{vendor_name}
æ¥­è€…åƒæ•¸ï¼š
{params_description}

èª¿æ•´è¦å‰‡ï¼š
1. ä»”ç´°è­˜åˆ¥å…§å®¹ä¸­æåˆ°çš„åƒæ•¸ç›¸é—œè³‡è¨Šï¼ˆå¦‚æ—¥æœŸã€é‡‘é¡ã€æ™‚é–“ç­‰ï¼‰
2. å¦‚æœå…§å®¹ä¸­çš„æ•¸å€¼èˆ‡æ¥­è€…åƒæ•¸ä¸ç¬¦ï¼Œè«‹æ›¿æ›ç‚ºæ¥­è€…åƒæ•¸ä¸­çš„å€¼
3. ä¿æŒå…§å®¹çš„èªæ°£ã€çµæ§‹å’Œæ ¼å¼ä¸è®Š
4. åªèª¿æ•´æ•¸å€¼ï¼Œä¸è¦æ”¹è®Šå…¶ä»–å…§å®¹
5. å¦‚æœå…§å®¹å·²ç¶“ç¬¦åˆæ¥­è€…åƒæ•¸ï¼Œå‰‡ä¸éœ€è¦ä¿®æ”¹
6. æ¥­è€…åç¨±çµ±ä¸€ä½¿ç”¨ "{vendor_name}"

é‡è¦ï¼šåªè¼¸å‡ºèª¿æ•´å¾Œçš„å…§å®¹ï¼Œä¸è¦åŠ ä¸Šä»»ä½•èªªæ˜æˆ–è¨»è§£ã€‚"""

        user_prompt = f"""è«‹æ ¹æ“šæ¥­è€…åƒæ•¸èª¿æ•´ä»¥ä¸‹å…§å®¹ï¼š

{content}"""

        try:
            if not self.client:
                raise Exception("OpenAI client not initialized (missing API key)")

            response = self.client.chat.completions.create(
                model=self.config["model"],
                temperature=0.3,  # ä½¿ç”¨è¼ƒä½æº«åº¦ç¢ºä¿æº–ç¢ºæ€§
                max_tokens=self.config["max_tokens"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
            )

            adjusted_content = response.choices[0].message.content.strip()
            return adjusted_content

        except Exception as e:
            print(f"âš ï¸  åƒæ•¸æ³¨å…¥å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å…§å®¹: {e}")
            return content

    def synthesize_answer(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        åˆæˆå¤šå€‹ç­”æ¡ˆç‚ºä¸€å€‹å®Œæ•´ç­”æ¡ˆï¼ˆPhase 2 æ“´å±•åŠŸèƒ½ï¼‰

        ç•¶æª¢ç´¢åˆ°çš„å¤šå€‹ç­”æ¡ˆå„æœ‰å´é‡æ™‚ï¼Œä½¿ç”¨ LLM å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´ã€çµæ§‹åŒ–çš„ç­”æ¡ˆã€‚
        é€™å¯ä»¥æå‡ç­”æ¡ˆçš„å®Œæ•´æ€§ï¼Œç‰¹åˆ¥é©ç”¨æ–¼è¤‡é›œå•é¡Œã€‚

        Args:
            question: ç”¨æˆ¶å•é¡Œ
            search_results: å¤šå€‹æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (åˆæˆå¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # æº–å‚™å¤šå€‹ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        max_results = self.config.get("synthesis_max_results", 3)
        answers_to_synthesize = []

        for i, result in enumerate(search_results[:max_results], 1):
            content = result['content']

            # å¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name)

            answers_to_synthesize.append({
                "index": i,
                "title": result['title'],
                "category": result.get('category', 'N/A'),
                "content": content,
                "similarity": result['similarity']
            })

        # æ ¼å¼åŒ–ç­”æ¡ˆåˆ—è¡¨
        formatted_answers = "\n\n".join([
            f"ã€ç­”æ¡ˆ {ans['index']}ã€‘\n"
            f"æ¨™é¡Œï¼š{ans['title']}\n"
            f"åˆ†é¡ï¼š{ans['category']}\n"
            f"ç›¸ä¼¼åº¦ï¼š{ans['similarity']:.2f}\n"
            f"å…§å®¹ï¼š\n{ans['content']}"
            for ans in answers_to_synthesize
        ])

        # å»ºç«‹åˆæˆ Prompt
        system_prompt = self._create_synthesis_system_prompt(intent_info, vendor_name, vendor_info)
        user_prompt = self._create_synthesis_user_prompt(question, formatted_answers, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # å‘¼å« OpenAI API é€²è¡Œåˆæˆ
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=0.5,  # ç¨ä½æº«åº¦ä»¥ç¢ºä¿æº–ç¢ºæ€§å’Œçµæ§‹
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        synthesized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        print(f"âœ¨ ç­”æ¡ˆåˆæˆå®Œæˆï¼šä½¿ç”¨äº† {len(answers_to_synthesize)} å€‹ä¾†æºï¼Œtokens: {tokens_used}")

        return synthesized_answer, tokens_used

    def _create_synthesis_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜æ•´åˆåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å°‡å¤šå€‹ç›¸é—œä½†å„æœ‰å´é‡çš„ç­”æ¡ˆï¼Œåˆæˆç‚ºä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚

åˆæˆè¦æ±‚ï¼š
1. **å®Œæ•´æ€§**ï¼šæ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Šï¼Œä¸éºæ¼ä»»ä½•é—œéµæ­¥é©Ÿæˆ–ç´°ç¯€
2. **æº–ç¢ºæ€§**ï¼šè³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„ç­”æ¡ˆï¼Œä¸è¦ç·¨é€ æˆ–æ¨æ¸¬
3. **çµæ§‹åŒ–**ï¼šä½¿ç”¨æ¸…æ™°çš„æ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿç·¨è™Ÿï¼Œä½¿ç­”æ¡ˆæ˜“æ–¼é–±è®€
4. **å»é‡**ï¼šå¦‚æœå¤šå€‹ç­”æ¡ˆæåˆ°ç›¸åŒè³‡è¨Šï¼Œåªä¿ç•™ä¸€æ¬¡ï¼Œé¿å…é‡è¤‡
5. **å„ªå…ˆç´š**ï¼šå„ªå…ˆä½¿ç”¨ç›¸ä¼¼åº¦è¼ƒé«˜çš„ç­”æ¡ˆå…§å®¹
6. **èªæ°£**ï¼šä¿æŒå°ˆæ¥­ã€å‹å–„ã€æ˜“æ‡‚çš„ç¹é«”ä¸­æ–‡è¡¨é”
7. **Markdown**ï¼šé©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆ## æ¨™é¡Œã€- åˆ—è¡¨ã€**ç²—é«”**ï¼‰"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. **æ¥­è€…èº«ä»½**ï¼šä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 1 SOP æ“´å±•ï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            if business_type == 'full_service':
                base_prompt += f"\n{rule_number}. **æ¥­ç¨®ç‰¹æ€§**ï¼š{vendor_name} æ˜¯åŒ…ç§Ÿå‹æ¥­è€…ï¼Œæä¾›å…¨æ–¹ä½æœå‹™ã€‚èªæ°£æ‡‰ä¸»å‹•å‘ŠçŸ¥ã€ç¢ºèªã€æ‰¿è«¾ã€‚ä½¿ç”¨ã€Œæˆ‘å€‘æœƒã€ã€ã€Œå…¬å¸å°‡ã€ç­‰ä¸»å‹•èªå¥"
                rule_number += 1
            elif business_type == 'property_management':
                base_prompt += f"\n{rule_number}. **æ¥­ç¨®ç‰¹æ€§**ï¼š{vendor_name} æ˜¯ä»£ç®¡å‹æ¥­è€…ï¼Œå”åŠ©ç§Ÿå®¢èˆ‡æˆ¿æ±æºé€šã€‚èªæ°£æ‡‰å”åŠ©å¼•å°ã€å»ºè­°è¯ç¹«ã€‚ä½¿ç”¨ã€Œè«‹æ‚¨ã€ã€ã€Œå»ºè­°ã€ã€ã€Œå¯å”åŠ©ã€ç­‰å¼•å°èªå¥"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. **çŸ¥è­˜é¡å‹**ï¼šé€™æ˜¯çŸ¥è­˜æŸ¥è©¢ï¼Œè«‹æä¾›å®Œæ•´çš„èªªæ˜ã€æ­¥é©Ÿå’Œæ³¨æ„äº‹é …"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. **è³‡æ–™æŸ¥è©¢**ï¼šå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢å’Œæ‰€éœ€è³‡æ–™"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. **æ“ä½œæŒ‡å¼•**ï¼šè«‹æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æ“ä½œæ­¥é©Ÿ"

        base_prompt += "\n\né‡è¦ï¼šåªè¼¸å‡ºåˆæˆå¾Œçš„å®Œæ•´ç­”æ¡ˆï¼Œä¸è¦åŠ ä¸Šã€Œæ ¹æ“šä»¥ä¸Šè³‡è¨Šã€ç­‰å…ƒè³‡è¨Šã€‚"

        return base_prompt

    def _create_synthesis_user_prompt(self, question: str, formatted_answers: str, intent_info: Dict) -> str:
        """å»ºç«‹ç­”æ¡ˆåˆæˆçš„ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

ä»¥ä¸‹æ˜¯å¤šå€‹ç›¸é—œç­”æ¡ˆï¼Œè«‹å°‡å®ƒå€‘åˆæˆç‚ºä¸€å€‹å®Œæ•´çš„å›è¦†ï¼š

{formatted_answers}

è«‹ç¶œåˆä»¥ä¸Šç­”æ¡ˆï¼Œç”Ÿæˆä¸€å€‹å®Œæ•´ã€æº–ç¢ºã€çµæ§‹åŒ–çš„å›è¦†ã€‚ç¢ºä¿ï¼š
- æ¶µè“‹æ‰€æœ‰é‡è¦è³‡è¨Š
- ä½¿ç”¨æ¸…æ™°çš„çµæ§‹ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ã€æ­¥é©Ÿï¼‰
- é¿å…é‡è¤‡
- ä¿æŒæº–ç¢ºæ€§"""

        return prompt

    def _call_llm(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict,
        vendor_params: Optional[Dict] = None,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> tuple[str, int]:
        """
        å‘¼å« LLM å„ªåŒ–ç­”æ¡ˆ

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: æª¢ç´¢çµæœ
            intent_info: æ„åœ–è³‡è¨Š
            vendor_params: æ¥­è€…åƒæ•¸ï¼ˆç”¨æ–¼å‹•æ…‹æ³¨å…¥ï¼‰
            vendor_name: æ¥­è€…åç¨±
            vendor_info: å®Œæ•´æ¥­è€…è³‡è¨Šï¼ˆåŒ…å« business_type, cashflow_model ç­‰ï¼‰

        Returns:
            (å„ªåŒ–å¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # 1. æº–å‚™æª¢ç´¢çµæœä¸Šä¸‹æ–‡ï¼ˆå…ˆé€²è¡Œåƒæ•¸æ³¨å…¥ï¼‰
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):  # æœ€å¤šä½¿ç”¨å‰ 3 å€‹çµæœ
            content = result['content']

            # Phase 1 æ“´å±•ï¼šå¦‚æœæœ‰æ¥­è€…åƒæ•¸ï¼Œå…ˆé€²è¡Œæ™ºèƒ½åƒæ•¸æ³¨å…¥
            if vendor_params and vendor_name:
                content = self.inject_vendor_params(content, vendor_params, vendor_name)

            context_parts.append(
                f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘\n"
                f"æ¨™é¡Œï¼š{result['title']}\n"
                f"åˆ†é¡ï¼š{result.get('category', 'N/A')}\n"
                f"å…§å®¹ï¼š{content}\n"
                f"ç›¸ä¼¼åº¦ï¼š{result['similarity']:.2f}"
            )

        context = "\n\n".join(context_parts)

        # 2. å»ºç«‹å„ªåŒ– Prompt
        system_prompt = self._create_system_prompt(intent_info, vendor_name, vendor_info)
        user_prompt = self._create_user_prompt(question, context, intent_info)

        # æª¢æŸ¥ API key
        if not self.client:
            raise Exception("OpenAI client not initialized (missing API key)")

        # 3. å‘¼å« OpenAI API
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=self.config["temperature"],
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        optimized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        return optimized_answer, tokens_used

    def _create_system_prompt(
        self,
        intent_info: Dict,
        vendor_name: Optional[str] = None,
        vendor_info: Optional[Dict] = None
    ) -> str:
        """å»ºç«‹ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„å®¢æœåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šçŸ¥è­˜åº«çš„è³‡è¨Šï¼Œç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„èªè¨€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œå…§å®¹
2. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£è¦ªåˆ‡å°ˆæ¥­
3. è³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„åƒè€ƒè³‡æ–™ï¼Œä¸è¦ç·¨é€ 
4. å¦‚æœ‰æ­¥é©Ÿæˆ–æµç¨‹ï¼Œè«‹æ¸…æ¥šåˆ—å‡º
5. é©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ï¼‰ä½¿ç­”æ¡ˆæ›´æ˜“è®€
6. ä¿æŒç°¡æ½”ï¼Œé¿å…å†—é•·
7. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè«‹èª å¯¦èªªæ˜"""

        rule_number = 8

        # å¦‚æœæœ‰æ¥­è€…åç¨±ï¼ŒåŠ å…¥æ¥­è€…è³‡è¨Š
        if vendor_name:
            base_prompt += f"\n{rule_number}. ä½ ä»£è¡¨ {vendor_name}ï¼Œè«‹ä½¿ç”¨è©²æ¥­è€…çš„è³‡è¨Šå›ç­”"
            rule_number += 1

        # æ ¹æ“šæ¥­ç¨®é¡å‹èª¿æ•´èªæ°£ï¼ˆPhase 1 SOP æ“´å±•ï¼‰
        if vendor_info:
            business_type = vendor_info.get('business_type', 'property_management')
            if business_type == 'full_service':
                base_prompt += f"\n{rule_number}. ã€æ¥­ç¨®ç‰¹æ€§ã€‘{vendor_name} æ˜¯åŒ…ç§Ÿå‹æ¥­è€…ï¼Œä½ å€‘æä¾›å…¨æ–¹ä½æœå‹™ã€‚èªæ°£æ‡‰ï¼šä¸»å‹•å‘ŠçŸ¥ã€ç¢ºèªã€æ‰¿è«¾ã€‚ä½¿ç”¨ã€Œæˆ‘å€‘æœƒã€ã€ã€Œå…¬å¸å°‡ã€ç­‰ä¸»å‹•èªå¥"
                rule_number += 1
            elif business_type == 'property_management':
                base_prompt += f"\n{rule_number}. ã€æ¥­ç¨®ç‰¹æ€§ã€‘{vendor_name} æ˜¯ä»£ç®¡å‹æ¥­è€…ï¼Œä½ å€‘å”åŠ©ç§Ÿå®¢èˆ‡æˆ¿æ±æºé€šã€‚èªæ°£æ‡‰ï¼šå”åŠ©å¼•å°ã€å»ºè­°è¯ç¹«ã€‚ä½¿ç”¨ã€Œè«‹æ‚¨ã€ã€ã€Œå»ºè­°ã€ã€ã€Œå¯å”åŠ©ã€ç­‰å¼•å°èªå¥"
                rule_number += 1

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += f"\n{rule_number}. é€™æ˜¯çŸ¥è­˜æŸ¥è©¢å•é¡Œï¼Œè«‹æä¾›æ¸…æ¥šçš„èªªæ˜å’Œæ­¥é©Ÿ"
        elif intent_type == "data_query":
            base_prompt += f"\n{rule_number}. é€™æ˜¯è³‡æ–™æŸ¥è©¢å•é¡Œï¼Œå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢"
        elif intent_type == "action":
            base_prompt += f"\n{rule_number}. é€™æ˜¯æ“ä½œåŸ·è¡Œå•é¡Œï¼Œè«‹èªªæ˜å…·é«”æ“ä½œæ­¥é©Ÿ"

        return base_prompt

    def _create_user_prompt(self, question: str, context: str, intent_info: Dict) -> str:
        """å»ºç«‹ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™ï¼Œç”¨è‡ªç„¶ã€å‹å–„çš„èªæ°£å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚"""

        return prompt

    def get_optimization_stats(self, optimizations: List[Dict]) -> Dict:
        """
        è¨ˆç®—å„ªåŒ–çµ±è¨ˆè³‡è¨Š

        Args:
            optimizations: å„ªåŒ–çµæœåˆ—è¡¨

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if not optimizations:
            return {
                "total_optimizations": 0,
                "successful_optimizations": 0,
                "total_tokens_used": 0,
                "avg_tokens_per_optimization": 0,
                "avg_processing_time_ms": 0
            }

        total = len(optimizations)
        successful = sum(1 for o in optimizations if o.get('optimization_applied', False))
        total_tokens = sum(o.get('tokens_used', 0) for o in optimizations)
        total_time = sum(o.get('processing_time_ms', 0) for o in optimizations)

        return {
            "total_optimizations": total,
            "successful_optimizations": successful,
            "success_rate": round(successful / total, 3) if total > 0 else 0,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_optimization": round(total_tokens / successful, 1) if successful > 0 else 0,
            "avg_processing_time_ms": round(total_time / total, 1) if total > 0 else 0
        }


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    import asyncio

    async def test_optimizer():
        """æ¸¬è©¦ LLM ç­”æ¡ˆå„ªåŒ–å™¨"""
        optimizer = LLMAnswerOptimizer()

        # æ¨¡æ“¬æª¢ç´¢çµæœ
        search_results = [
            {
                "id": 1,
                "title": "é€€ç§Ÿæµç¨‹èªªæ˜",
                "category": "åˆç´„å•é¡Œ",
                "content": """# é€€ç§Ÿæµç¨‹

## æ­¥é©Ÿèªªæ˜

1. **æå‰é€šçŸ¥**ï¼šè«‹æ–¼é€€ç§Ÿæ—¥å‰30å¤©ä»¥æ›¸é¢æ–¹å¼é€šçŸ¥æˆ¿æ±
2. **ç¹³æ¸…è²»ç”¨**ï¼šç¢ºèªæ‰€æœ‰ç§Ÿé‡‘ã€æ°´é›»è²»å·²ç¹³æ¸…
3. **æˆ¿å±‹æª¢æŸ¥**ï¼šèˆ‡æˆ¿æ±ç´„å®šæ™‚é–“é€²è¡Œæˆ¿å±‹æª¢æŸ¥
4. **æŠ¼é‡‘é€€é‚„**ï¼šç¢ºèªæˆ¿å±‹ç‹€æ³è‰¯å¥½å¾Œï¼Œ7å€‹å·¥ä½œå¤©å…§é€€é‚„æŠ¼é‡‘

## æ³¨æ„äº‹é …
- éœ€æå‰30å¤©é€šçŸ¥
- éœ€ç¹³æ¸…æ‰€æœ‰è²»ç”¨
- æˆ¿å±‹éœ€æ¢å¾©åŸç‹€""",
                "similarity": 0.89
            }
        ]

        intent_info = {
            "intent_name": "é€€ç§Ÿæµç¨‹",
            "intent_type": "knowledge",
            "keywords": ["é€€ç§Ÿ", "è¾¦ç†"]
        }

        # æ¸¬è©¦å„ªåŒ–
        print("ğŸ”„ é–‹å§‹å„ªåŒ–ç­”æ¡ˆ...")
        result = optimizer.optimize_answer(
            question="è«‹å•å¦‚ä½•è¾¦ç†é€€ç§Ÿæ‰‹çºŒï¼Ÿ",
            search_results=search_results,
            confidence_level="high",
            intent_info=intent_info
        )

        print(f"\nâœ… å„ªåŒ–å®Œæˆ")
        print(f"ä½¿ç”¨å„ªåŒ–ï¼š{result['optimization_applied']}")
        print(f"Token ä½¿ç”¨ï¼š{result['tokens_used']}")
        print(f"è™•ç†æ™‚é–“ï¼š{result['processing_time_ms']}ms")
        print(f"\n{'='*60}")
        print("åŸå§‹ç­”æ¡ˆï¼š")
        print(result['original_answer'][:200] + "...")
        print(f"\n{'='*60}")
        print("å„ªåŒ–å¾Œç­”æ¡ˆï¼š")
        print(result['optimized_answer'])
        print(f"{'='*60}")

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_optimizer())
