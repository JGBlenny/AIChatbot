"""
LLM ç­”æ¡ˆå„ªåŒ–æœå‹™
ä½¿ç”¨ GPT æ¨¡å‹å„ªåŒ– RAG æª¢ç´¢çµæœï¼Œç”Ÿæˆæ›´è‡ªç„¶ã€æ›´ç²¾æº–çš„ç­”æ¡ˆ
"""
import os
from typing import List, Dict, Optional
from openai import OpenAI
import time


class LLMAnswerOptimizer:
    """LLM ç­”æ¡ˆå„ªåŒ–å™¨"""

    def __init__(self, config: Dict = None):
        """
        åˆå§‹åŒ– LLM ç­”æ¡ˆå„ªåŒ–å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        self.config = config or {
            "model": "gpt-4o-mini",
            "temperature": 0.7,
            "max_tokens": 800,
            "enable_optimization": True,
            "optimize_for_confidence": ["high", "medium"],  # åªå„ªåŒ–é«˜/ä¸­ä¿¡å¿ƒåº¦
            "fallback_on_error": True  # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
        }

    def optimize_answer(
        self,
        question: str,
        search_results: List[Dict],
        confidence_level: str,
        intent_info: Dict
    ) -> Dict:
        """
        å„ªåŒ–ç­”æ¡ˆ

        Args:
            question: ä½¿ç”¨è€…å•é¡Œ
            search_results: RAG æª¢ç´¢çµæœåˆ—è¡¨
            confidence_level: ä¿¡å¿ƒåº¦ç­‰ç´š
            intent_info: æ„åœ–è³‡è¨Š

        Returns:
            å„ªåŒ–çµæœå­—å…¸ï¼ŒåŒ…å«:
            - optimized_answer: å„ªåŒ–å¾Œçš„ç­”æ¡ˆ
            - original_answer: åŸå§‹ç­”æ¡ˆ
            - optimization_applied: æ˜¯å¦ä½¿ç”¨äº†å„ªåŒ–
            - tokens_used: ä½¿ç”¨çš„ token æ•¸
            - processing_time_ms: è™•ç†æ™‚é–“
        """
        start_time = time.time()

        # 1. æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ–
        if not self._should_optimize(confidence_level, search_results):
            return self._create_fallback_response(search_results, start_time)

        # 2. æº–å‚™åŸå§‹ç­”æ¡ˆ
        original_answer = self._create_original_answer(search_results)

        # 3. å˜—è©¦ LLM å„ªåŒ–
        try:
            optimized_answer, tokens_used = self._call_llm(
                question=question,
                search_results=search_results,
                intent_info=intent_info
            )

            processing_time = int((time.time() - start_time) * 1000)

            return {
                "optimized_answer": optimized_answer,
                "original_answer": original_answer,
                "optimization_applied": True,
                "tokens_used": tokens_used,
                "processing_time_ms": processing_time,
                "model": self.config["model"]
            }

        except Exception as e:
            print(f"âŒ LLM å„ªåŒ–å¤±æ•—: {e}")

            if self.config["fallback_on_error"]:
                # éŒ¯èª¤æ™‚ä½¿ç”¨åŸå§‹ç­”æ¡ˆ
                processing_time = int((time.time() - start_time) * 1000)
                return {
                    "optimized_answer": original_answer,
                    "original_answer": original_answer,
                    "optimization_applied": False,
                    "tokens_used": 0,
                    "processing_time_ms": processing_time,
                    "error": str(e)
                }
            else:
                raise

    def _should_optimize(self, confidence_level: str, search_results: List[Dict]) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²å„ªåŒ–"""
        if not self.config["enable_optimization"]:
            return False

        if not search_results:
            return False

        if confidence_level not in self.config["optimize_for_confidence"]:
            return False

        return True

    def _create_original_answer(self, search_results: List[Dict]) -> str:
        """å»ºç«‹åŸå§‹ç­”æ¡ˆï¼ˆæœªå„ªåŒ–ï¼‰"""
        if not search_results:
            return ""

        best_result = search_results[0]
        return f"{best_result['title']}\n\n{best_result['content']}"

    def _create_fallback_response(self, search_results: List[Dict], start_time: float) -> Dict:
        """å»ºç«‹å‚™ç”¨å›æ‡‰ï¼ˆä¸å„ªåŒ–ï¼‰"""
        original_answer = self._create_original_answer(search_results)
        processing_time = int((time.time() - start_time) * 1000)

        return {
            "optimized_answer": original_answer,
            "original_answer": original_answer,
            "optimization_applied": False,
            "tokens_used": 0,
            "processing_time_ms": processing_time
        }

    def _call_llm(
        self,
        question: str,
        search_results: List[Dict],
        intent_info: Dict
    ) -> tuple[str, int]:
        """
        å‘¼å« LLM å„ªåŒ–ç­”æ¡ˆ

        Returns:
            (å„ªåŒ–å¾Œçš„ç­”æ¡ˆ, ä½¿ç”¨çš„ tokens æ•¸)
        """
        # 1. æº–å‚™æª¢ç´¢çµæœä¸Šä¸‹æ–‡
        context_parts = []
        for i, result in enumerate(search_results[:3], 1):  # æœ€å¤šä½¿ç”¨å‰ 3 å€‹çµæœ
            context_parts.append(
                f"ã€åƒè€ƒè³‡æ–™ {i}ã€‘\n"
                f"æ¨™é¡Œï¼š{result['title']}\n"
                f"åˆ†é¡ï¼š{result.get('category', 'N/A')}\n"
                f"å…§å®¹ï¼š{result['content']}\n"
                f"ç›¸ä¼¼åº¦ï¼š{result['similarity']:.2f}"
            )

        context = "\n\n".join(context_parts)

        # 2. å»ºç«‹å„ªåŒ– Prompt
        system_prompt = self._create_system_prompt(intent_info)
        user_prompt = self._create_user_prompt(question, context, intent_info)

        # 3. å‘¼å« OpenAI API
        response = self.client.chat.completions.create(
            model=self.config["model"],
            temperature=self.config["temperature"],
            max_tokens=self.config["max_tokens"],
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        )

        optimized_answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        return optimized_answer, tokens_used

    def _create_system_prompt(self, intent_info: Dict) -> str:
        """å»ºç«‹ç³»çµ±æç¤ºè©"""
        intent_type = intent_info.get('intent_type', 'knowledge')

        base_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ã€å‹å–„çš„å®¢æœåŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šçŸ¥è­˜åº«çš„è³‡è¨Šï¼Œç”¨è‡ªç„¶ã€æ˜“æ‡‚çš„èªè¨€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚

å›ç­”è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œå…§å®¹
2. ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£è¦ªåˆ‡å°ˆæ¥­
3. è³‡è¨Šå¿…é ˆä¾†è‡ªæä¾›çš„åƒè€ƒè³‡æ–™ï¼Œä¸è¦ç·¨é€ 
4. å¦‚æœ‰æ­¥é©Ÿæˆ–æµç¨‹ï¼Œè«‹æ¸…æ¥šåˆ—å‡º
5. é©ç•¶ä½¿ç”¨ Markdown æ ¼å¼ï¼ˆæ¨™é¡Œã€åˆ—è¡¨ï¼‰ä½¿ç­”æ¡ˆæ›´æ˜“è®€
6. ä¿æŒç°¡æ½”ï¼Œé¿å…å†—é•·
7. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè«‹èª å¯¦èªªæ˜"""

        # æ ¹æ“šæ„åœ–é¡å‹èª¿æ•´æç¤º
        if intent_type == "knowledge":
            base_prompt += "\n8. é€™æ˜¯çŸ¥è­˜æŸ¥è©¢å•é¡Œï¼Œè«‹æä¾›æ¸…æ¥šçš„èªªæ˜å’Œæ­¥é©Ÿ"
        elif intent_type == "data_query":
            base_prompt += "\n8. é€™æ˜¯è³‡æ–™æŸ¥è©¢å•é¡Œï¼Œå¦‚éœ€æŸ¥è©¢å…·é«”è³‡æ–™ï¼Œè«‹èªªæ˜å¦‚ä½•æŸ¥è©¢"
        elif intent_type == "action":
            base_prompt += "\n8. é€™æ˜¯æ“ä½œåŸ·è¡Œå•é¡Œï¼Œè«‹èªªæ˜å…·é«”æ“ä½œæ­¥é©Ÿ"

        return base_prompt

    def _create_user_prompt(self, question: str, context: str, intent_info: Dict) -> str:
        """å»ºç«‹ä½¿ç”¨è€…æç¤ºè©"""
        keywords = intent_info.get('keywords', [])
        keywords_str = "ã€".join(keywords) if keywords else "ç„¡"

        prompt = f"""ä½¿ç”¨è€…å•é¡Œï¼š{question}

æ„åœ–é¡å‹ï¼š{intent_info.get('intent_name', 'æœªçŸ¥')}
é—œéµå­—ï¼š{keywords_str}

åƒè€ƒè³‡æ–™ï¼š
{context}

è«‹æ ¹æ“šä»¥ä¸Šåƒè€ƒè³‡æ–™ï¼Œç”¨è‡ªç„¶ã€å‹å–„çš„èªæ°£å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚"""

        return prompt

    def get_optimization_stats(self, optimizations: List[Dict]) -> Dict:
        """
        è¨ˆç®—å„ªåŒ–çµ±è¨ˆè³‡è¨Š

        Args:
            optimizations: å„ªåŒ–çµæœåˆ—è¡¨

        Returns:
            çµ±è¨ˆè³‡è¨Šå­—å…¸
        """
        if not optimizations:
            return {
                "total_optimizations": 0,
                "successful_optimizations": 0,
                "total_tokens_used": 0,
                "avg_tokens_per_optimization": 0,
                "avg_processing_time_ms": 0
            }

        total = len(optimizations)
        successful = sum(1 for o in optimizations if o.get('optimization_applied', False))
        total_tokens = sum(o.get('tokens_used', 0) for o in optimizations)
        total_time = sum(o.get('processing_time_ms', 0) for o in optimizations)

        return {
            "total_optimizations": total,
            "successful_optimizations": successful,
            "success_rate": round(successful / total, 3) if total > 0 else 0,
            "total_tokens_used": total_tokens,
            "avg_tokens_per_optimization": round(total_tokens / successful, 1) if successful > 0 else 0,
            "avg_processing_time_ms": round(total_time / total, 1) if total > 0 else 0
        }


# ä½¿ç”¨ç¯„ä¾‹
if __name__ == "__main__":
    import asyncio

    async def test_optimizer():
        """æ¸¬è©¦ LLM ç­”æ¡ˆå„ªåŒ–å™¨"""
        optimizer = LLMAnswerOptimizer()

        # æ¨¡æ“¬æª¢ç´¢çµæœ
        search_results = [
            {
                "id": 1,
                "title": "é€€ç§Ÿæµç¨‹èªªæ˜",
                "category": "åˆç´„å•é¡Œ",
                "content": """# é€€ç§Ÿæµç¨‹

## æ­¥é©Ÿèªªæ˜

1. **æå‰é€šçŸ¥**ï¼šè«‹æ–¼é€€ç§Ÿæ—¥å‰30å¤©ä»¥æ›¸é¢æ–¹å¼é€šçŸ¥æˆ¿æ±
2. **ç¹³æ¸…è²»ç”¨**ï¼šç¢ºèªæ‰€æœ‰ç§Ÿé‡‘ã€æ°´é›»è²»å·²ç¹³æ¸…
3. **æˆ¿å±‹æª¢æŸ¥**ï¼šèˆ‡æˆ¿æ±ç´„å®šæ™‚é–“é€²è¡Œæˆ¿å±‹æª¢æŸ¥
4. **æŠ¼é‡‘é€€é‚„**ï¼šç¢ºèªæˆ¿å±‹ç‹€æ³è‰¯å¥½å¾Œï¼Œ7å€‹å·¥ä½œå¤©å…§é€€é‚„æŠ¼é‡‘

## æ³¨æ„äº‹é …
- éœ€æå‰30å¤©é€šçŸ¥
- éœ€ç¹³æ¸…æ‰€æœ‰è²»ç”¨
- æˆ¿å±‹éœ€æ¢å¾©åŸç‹€""",
                "similarity": 0.89
            }
        ]

        intent_info = {
            "intent_name": "é€€ç§Ÿæµç¨‹",
            "intent_type": "knowledge",
            "keywords": ["é€€ç§Ÿ", "è¾¦ç†"]
        }

        # æ¸¬è©¦å„ªåŒ–
        print("ğŸ”„ é–‹å§‹å„ªåŒ–ç­”æ¡ˆ...")
        result = optimizer.optimize_answer(
            question="è«‹å•å¦‚ä½•è¾¦ç†é€€ç§Ÿæ‰‹çºŒï¼Ÿ",
            search_results=search_results,
            confidence_level="high",
            intent_info=intent_info
        )

        print(f"\nâœ… å„ªåŒ–å®Œæˆ")
        print(f"ä½¿ç”¨å„ªåŒ–ï¼š{result['optimization_applied']}")
        print(f"Token ä½¿ç”¨ï¼š{result['tokens_used']}")
        print(f"è™•ç†æ™‚é–“ï¼š{result['processing_time_ms']}ms")
        print(f"\n{'='*60}")
        print("åŸå§‹ç­”æ¡ˆï¼š")
        print(result['original_answer'][:200] + "...")
        print(f"\n{'='*60}")
        print("å„ªåŒ–å¾Œç­”æ¡ˆï¼š")
        print(result['optimized_answer'])
        print(f"{'='*60}")

    # åŸ·è¡Œæ¸¬è©¦
    asyncio.run(test_optimizer())
