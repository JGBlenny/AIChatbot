"""
æ–‡ä»¶è½‰æ›æœå‹™ - å°‡ Word/PDF è¦æ ¼æ›¸è½‰æ›ç‚ºçŸ¥è­˜åº« Q&A

æ”¯æ´æ ¼å¼:
- .docx (Microsoft Word)
- .pdf (å°‡ä¾†æ“´å±•)

å·¥ä½œæµç¨‹:
1. ä¸Šå‚³æ–‡ä»¶
2. è§£ææ–‡ä»¶å…§å®¹
3. ä½¿ç”¨ AI æå– Q&A
4. äººå·¥å¯©æ ¸ç·¨è¼¯
5. åŒ¯å…¥çŸ¥è­˜åº«
"""

import os
import uuid
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
from docx import Document
import openai
import asyncpg
from asyncpg.pool import Pool


class DocumentConverterService:
    # OpenAI æ¨¡å‹çš„ context é™åˆ¶ï¼ˆtokensï¼‰
    MODEL_CONTEXT_LIMITS = {
        'gpt-4o': 128000,
        'gpt-4o-mini': 128000,
        'gpt-4-turbo': 128000,
        'gpt-4': 8192,
        'gpt-3.5-turbo': 16385
    }

    def __init__(self, db_pool: Optional[Pool] = None):
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        # è¦æ ¼æ›¸è½‰æ›å°ˆç”¨æ¨¡å‹ï¼ˆéœ€è¦æ›´å¼·çš„ç†è§£èƒ½åŠ›å’Œå¤§ contextï¼‰
        self.model = os.getenv('DOCUMENT_CONVERTER_MODEL', os.getenv('KNOWLEDGE_GEN_MODEL', 'gpt-4o'))
        self.temp_dir = Path('/tmp/document_converter')
        self.temp_dir.mkdir(exist_ok=True)
        self.db_pool = db_pool

        # è½‰æ›ä»»å‹™ç·©å­˜ (ç”Ÿç”¢ç’°å¢ƒæ‡‰ä½¿ç”¨ Redis)
        self.jobs = {}

        # æ„åœ–å¿«å–ï¼ˆæ¸›å°‘è³‡æ–™åº«æŸ¥è©¢ï¼‰
        self._cached_intents = None

    async def upload_document(self, file_path: str, original_filename: str) -> Dict:
        """
        ä¸Šå‚³ä¸¦é©—è­‰æ–‡ä»¶

        Args:
            file_path: è‡¨æ™‚æ–‡ä»¶è·¯å¾‘
            original_filename: åŸå§‹æª”å

        Returns:
            åŒ…å« job_id å’Œæ–‡ä»¶è³‡è¨Šçš„å­—å…¸
        """
        job_id = str(uuid.uuid4())
        file_size = Path(file_path).stat().st_size
        file_ext = Path(original_filename).suffix.lower()

        # é©—è­‰æ–‡ä»¶æ ¼å¼
        if file_ext not in ['.docx', '.pdf']:
            raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_ext}ã€‚ç›®å‰åªæ”¯æ´ .docx å’Œ .pdf")

        # é©—è­‰æ–‡ä»¶å¤§å° (æœ€å¤§ 50MB)
        max_size = 50 * 1024 * 1024
        if file_size > max_size:
            raise ValueError(f"æª”æ¡ˆéå¤§: {file_size / 1024 / 1024:.1f}MBã€‚æœ€å¤§é™åˆ¶: 50MB")

        # ä¿å­˜æ–‡ä»¶
        saved_path = self.temp_dir / f"{job_id}_{original_filename}"
        Path(file_path).rename(saved_path)

        # å‰µå»ºä»»å‹™è¨˜éŒ„
        self.jobs[job_id] = {
            'job_id': job_id,
            'status': 'uploaded',  # uploaded, parsing, converting, completed, failed
            'file_path': str(saved_path),
            'file_name': original_filename,
            'file_size': file_size,
            'file_type': file_ext[1:],  # å»æ‰é»
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'content': None,  # è§£æå¾Œçš„ç´”æ–‡å­—
            'qa_list': None,  # AI æå–çš„ Q&A
            'error': None
        }

        print(f"âœ… æ–‡ä»¶ä¸Šå‚³æˆåŠŸ (job_id: {job_id})")
        print(f"   æª”å: {original_filename}")
        print(f"   å¤§å°: {file_size / 1024:.1f} KB")
        print(f"   æ ¼å¼: {file_ext}")

        return self.jobs[job_id]

    async def parse_document(self, job_id: str) -> Dict:
        """
        è§£ææ–‡ä»¶å…§å®¹ç‚ºç´”æ–‡å­—

        Args:
            job_id: ä»»å‹™ ID

        Returns:
            åŒ…å«è§£æå…§å®¹çš„ä»»å‹™è³‡è¨Š
        """
        if job_id not in self.jobs:
            raise ValueError(f"ä»»å‹™ä¸å­˜åœ¨: {job_id}")

        job = self.jobs[job_id]
        job['status'] = 'parsing'
        job['updated_at'] = datetime.now().isoformat()

        try:
            file_type = job['file_type']
            file_path = job['file_path']

            if file_type == 'docx':
                content = await self._parse_docx(file_path)
            elif file_type == 'pdf':
                content = await self._parse_pdf(file_path)
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {file_type}")

            job['content'] = content
            job['status'] = 'parsed'
            job['updated_at'] = datetime.now().isoformat()

            print(f"âœ… æ–‡ä»¶è§£æå®Œæˆ (job_id: {job_id})")
            print(f"   å…§å®¹é•·åº¦: {len(content)} å­—å…ƒ")

            return job

        except Exception as e:
            job['status'] = 'failed'
            job['error'] = str(e)
            job['updated_at'] = datetime.now().isoformat()
            print(f"âŒ æ–‡ä»¶è§£æå¤±æ•—: {e}")
            raise

    async def _parse_docx(self, file_path: str) -> str:
        """
        è§£æ Word æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾‘

        Returns:
            ç´”æ–‡å­—å…§å®¹
        """
        doc = Document(file_path)

        # æå–æ‰€æœ‰æ®µè½
        paragraphs = []
        for para in doc.paragraphs:
            text = para.text.strip()
            if text:  # åªä¿ç•™éç©ºæ®µè½
                paragraphs.append(text)

        # æå–è¡¨æ ¼å…§å®¹
        tables_content = []
        for table in doc.tables:
            for row in table.rows:
                row_data = []
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        row_data.append(cell_text)
                if row_data:
                    tables_content.append(' | '.join(row_data))

        # åˆä½µæ®µè½å’Œè¡¨æ ¼
        all_content = paragraphs
        if tables_content:
            all_content.append("\n=== è¡¨æ ¼å…§å®¹ ===")
            all_content.extend(tables_content)

        content = '\n'.join(all_content)

        print(f"ğŸ“„ Word æ–‡ä»¶è§£æ:")
        print(f"   æ®µè½æ•¸: {len(paragraphs)}")
        print(f"   è¡¨æ ¼æ•¸: {len(doc.tables)}")
        print(f"   ç¸½å­—å…ƒæ•¸: {len(content)}")

        return content

    async def _parse_pdf(self, file_path: str) -> str:
        """
        è§£æ PDF æ–‡ä»¶ (æœªä¾†å¯¦ä½œ)

        Args:
            file_path: æ–‡ä»¶è·¯å¾‘

        Returns:
            ç´”æ–‡å­—å…§å®¹
        """
        raise NotImplementedError("PDF è§£æåŠŸèƒ½å°šæœªå¯¦ä½œ")

    async def convert_to_qa(self, job_id: str, custom_prompt: Optional[str] = None) -> Dict:
        """
        ä½¿ç”¨ AI å°‡æ–‡ä»¶å…§å®¹è½‰æ›ç‚º Q&A

        Args:
            job_id: ä»»å‹™ ID
            custom_prompt: è‡ªè¨‚æç¤ºè©ï¼ˆå¯é¸ï¼‰

        Returns:
            åŒ…å« Q&A åˆ—è¡¨çš„ä»»å‹™è³‡è¨Š
        """
        if job_id not in self.jobs:
            raise ValueError(f"ä»»å‹™ä¸å­˜åœ¨: {job_id}")

        job = self.jobs[job_id]

        if job['status'] != 'parsed':
            raise ValueError(f"ä»»å‹™ç‹€æ…‹éŒ¯èª¤: {job['status']}ã€‚è«‹å…ˆè§£ææ–‡ä»¶")

        job['status'] = 'converting'
        job['updated_at'] = datetime.now().isoformat()

        try:
            content = job['content']

            # ä¼°ç®— token ä¸¦åˆ†æ®µè™•ç†ï¼ˆè€ƒæ…® context + TPM é™åˆ¶ï¼‰
            # æ ¹æ“šæ¨¡å‹å‹•æ…‹èª¿æ•´åˆ†æ®µå¤§å°
            max_context = self.MODEL_CONTEXT_LIMITS.get(self.model, 16385)

            # TPM (Tokens Per Minute) è€ƒé‡
            # gpt-4o çµ„ç¹” TPM é™åˆ¶é€šå¸¸ç‚º 30K-90Kï¼Œä¿å®ˆä¼°è¨ˆä½¿ç”¨ 30K
            # ç‚ºäº†é¿å… rate limitï¼Œå–®æ¬¡è«‹æ±‚æ‡‰è©²å°æ–¼ TPM é™åˆ¶çš„ 70%
            tpm_limit = 30000 if self.model == 'gpt-4o' else 90000  # gpt-3.5-turbo é€šå¸¸æ›´é«˜
            safe_request_tokens = int(tpm_limit * 0.7)  # å–®æ¬¡è«‹æ±‚å®‰å…¨ä¸Šé™

            # æ ¹æ“šæ¨¡å‹å®¹é‡å’Œ TPM é™åˆ¶è¨ˆç®—å®‰å…¨çš„åˆ†æ®µå¤§å°
            # é ç•™ 1000 tokens çµ¦ promptï¼Œ4000 tokens çµ¦è¼¸å‡º
            safe_input_tokens = min(max_context - 5000, safe_request_tokens - 4000)
            max_chars = int(safe_input_tokens / 2)  # ä¸­æ–‡ç´„ 1 å­— = 2 tokens

            # é™åˆ¶ç¯„åœï¼šæœ€å°‘ 3000 å­—ï¼Œæœ€å¤š 10000 å­—ï¼ˆé¿å…å–®æ®µå¤ªå¤§ï¼‰
            max_chars = max(3000, min(10000, max_chars))

            print(f"   ğŸ“ æ¨¡å‹: {self.model} (Context: {max_context}, TPM: ~{tpm_limit})")
            print(f"   ğŸ“ åˆ†æ®µå¤§å°: {max_chars} å­—å…ƒ (ç´„ {max_chars * 2} tokens)")

            content_chunks = self._split_content(content, max_chars)

            print(f"ğŸ¤– é–‹å§‹ AI è½‰æ› (job_id: {job_id})")
            print(f"   å…§å®¹åˆ†ç‚º {len(content_chunks)} æ®µè™•ç†")
            print(f"   ä½¿ç”¨æ¨¡å‹: {self.model}")

            # è¨ˆç®— TPM é™åˆ¶ä¸‹çš„å®‰å…¨å»¶é²
            # gpt-4o: 30K TPMï¼Œæ¯æ®µç´„ 20K tokensï¼Œéœ€è¦ç­‰å¾… 40 ç§’é¿å…è¶…é™
            if len(content_chunks) > 1:
                estimated_tokens_per_chunk = max_chars * 2 + 4000  # è¼¸å…¥ + è¼¸å‡º
                delay_seconds = int((estimated_tokens_per_chunk / tpm_limit) * 60 * 1.2)  # åŠ  20% ç·©è¡
                delay_seconds = max(20, min(60, delay_seconds))  # é™åˆ¶åœ¨ 20-60 ç§’ä¹‹é–“
                print(f"   â±ï¸  æ¯æ®µé–“éš”: {delay_seconds} ç§’ (é¿å… TPM è¶…é™)")

            all_qa = []
            for i, chunk in enumerate(content_chunks, 1):
                print(f"   è™•ç†ç¬¬ {i}/{len(content_chunks)} æ®µ...")
                qa_list = await self._call_openai_extract_qa(chunk, custom_prompt)

                # ç‚ºæ¯å€‹ Q&A æ¨è–¦æ„åœ–
                if self.db_pool and qa_list:
                    print(f"   ğŸ“Œ ç‚º {len(qa_list)} å€‹ Q&A æ¨è–¦æ„åœ–...")
                    for qa in qa_list:
                        recommended_intent = await self._recommend_intent_for_qa(qa)
                        qa['recommended_intent'] = recommended_intent

                        if recommended_intent['intent_id']:
                            print(f"      âœ… {qa['question_summary'][:30]}... â†’ {recommended_intent['intent_name']} ({recommended_intent['confidence']:.2f})")
                        else:
                            print(f"      âš ï¸  {qa['question_summary'][:30]}... â†’ æœªåˆ†é¡")

                all_qa.extend(qa_list)

                # åœ¨åˆ†æ®µä¹‹é–“æ·»åŠ å»¶é²ä»¥é¿å…è¶…é TPM é™åˆ¶
                if i < len(content_chunks) and len(content_chunks) > 1:
                    print(f"   â³ ç­‰å¾… {delay_seconds} ç§’å¾Œè™•ç†ä¸‹ä¸€æ®µ...")
                    await asyncio.sleep(delay_seconds)

            job['qa_list'] = all_qa
            job['status'] = 'completed'
            job['updated_at'] = datetime.now().isoformat()

            print(f"âœ… AI è½‰æ›å®Œæˆ")
            print(f"   æå–åˆ° {len(all_qa)} å€‹ Q&A")
            if self.db_pool:
                intent_recommended = sum(1 for qa in all_qa if qa.get('recommended_intent', {}).get('intent_id'))
                print(f"   å·²æ¨è–¦æ„åœ–: {intent_recommended}/{len(all_qa)} å€‹ Q&A")

            return job

        except Exception as e:
            job['status'] = 'failed'
            job['error'] = str(e)
            job['updated_at'] = datetime.now().isoformat()
            print(f"âŒ AI è½‰æ›å¤±æ•—: {e}")
            raise

    def _split_content(self, content: str, max_chars: int) -> List[str]:
        """
        å°‡é•·æ–‡æœ¬åˆ†æ®µ

        Args:
            content: åŸå§‹æ–‡æœ¬
            max_chars: æ¯æ®µæœ€å¤§å­—å…ƒæ•¸

        Returns:
            åˆ†æ®µå¾Œçš„æ–‡æœ¬åˆ—è¡¨
        """
        if len(content) <= max_chars:
            return [content]

        chunks = []
        paragraphs = content.split('\n')
        current_chunk = []
        current_length = 0

        for para in paragraphs:
            para_length = len(para) + 1  # +1 for newline

            if current_length + para_length > max_chars and current_chunk:
                # ç•¶å‰æ®µè½æœƒè¶…éé™åˆ¶ï¼Œå…ˆä¿å­˜ç•¶å‰chunk
                chunks.append('\n'.join(current_chunk))
                current_chunk = [para]
                current_length = para_length
            else:
                current_chunk.append(para)
                current_length += para_length

        # ä¿å­˜æœ€å¾Œä¸€æ®µ
        if current_chunk:
            chunks.append('\n'.join(current_chunk))

        return chunks

    async def _call_openai_extract_qa(self, content: str, custom_prompt: Optional[str] = None) -> List[Dict]:
        """
        å‘¼å« OpenAI API æå– Q&A

        Args:
            content: æ–‡ä»¶å…§å®¹
            custom_prompt: è‡ªè¨‚æç¤ºè©

        Returns:
            Q&A åˆ—è¡¨
        """
        if custom_prompt:
            prompt = custom_prompt.format(content=content)
        else:
            # è©³ç´° prompt æä¾›æ›´å¥½çš„æŒ‡å°ï¼ˆGPT-4o å¯ä»¥è™•ç†ï¼‰
            prompt = f"""è«‹å¾ä»¥ä¸‹æŠ€è¡“è¦æ ¼æ›¸ä¸­æå–å¯¦ç”¨çš„ä½¿ç”¨è€…å•ç­”å°ï¼ˆQ&Aï¼‰ã€‚

**é‡è¦ï¼šè«‹ç›¡å¯èƒ½æå–æ‰€æœ‰æœ‰åƒ¹å€¼çš„ Q&Aï¼Œä¸è¦éºæ¼ä»»ä½•é‡è¦åŠŸèƒ½æˆ–æ“ä½œèªªæ˜ã€‚**

## æå–è¦æ±‚ï¼š

1. **å•é¡Œé¡å‹**ï¼š
   - æ“ä½œæ­¥é©Ÿé¡ï¼ˆå¦‚ä½•ä½¿ç”¨æŸåŠŸèƒ½ï¼Ÿï¼‰
   - è¦å®šèªªæ˜é¡ï¼ˆä»€éº¼æƒ…æ³ä¸‹éœ€è¦...ï¼Ÿï¼‰
   - åŠŸèƒ½ä»‹ç´¹é¡ï¼ˆé€™å€‹åŠŸèƒ½æ˜¯åšä»€éº¼çš„ï¼Ÿï¼‰
   - ç–‘é›£æ’è§£é¡ï¼ˆé‡åˆ°XXå•é¡Œæ€éº¼è¾¦ï¼Ÿï¼‰

2. **ç­”æ¡ˆå“è³ª**ï¼š
   - æ¸…æ™°ã€å…·é«”ã€å®Œæ•´
   - åŒ…å«å¿…è¦çš„æ­¥é©Ÿèªªæ˜
   - ä½¿ç”¨æ¢åˆ—å¼èªªæ˜ï¼ˆå¦‚æœé©ç”¨ï¼‰
   - ä¿ç•™é‡è¦çš„ç´°ç¯€å’Œæ³¨æ„äº‹é …

3. **é—œéµå­—æå–**ï¼š
   - å¾å•ç­”ä¸­æå– 3-5 å€‹é—œéµè©
   - åŒ…å«å°ˆæœ‰åè©ã€åŠŸèƒ½åç¨±ã€æ“ä½œå‹•ä½œç­‰

## è¼¸å‡ºæ ¼å¼ï¼š

è«‹ä»¥ JSON é™£åˆ—æ ¼å¼è¼¸å‡ºï¼Œæ¯å€‹ Q&A åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
- question_summary: å•é¡Œæ‘˜è¦ï¼ˆ10-30å­—ï¼‰
- content: å®Œæ•´ç­”æ¡ˆï¼ˆåŒ…å«æ‰€æœ‰å¿…è¦è³‡è¨Šï¼‰
- keywords: é—œéµå­—é™£åˆ—ï¼ˆ3-5å€‹è©ï¼‰

ç¯„ä¾‹ï¼š
[
  {{
    "question_summary": "å¦‚ä½•ç”³è«‹åœè»Šä½ï¼Ÿ",
    "content": "ç”³è«‹åœè»Šä½çš„æ­¥é©Ÿå¦‚ä¸‹ï¼š\\n1. å¡«å¯«åœè»Šä½ç”³è«‹è¡¨\\n2. æä¾›è»Šè¼›è¡Œç…§å½±æœ¬\\n3. ç¹³äº¤ä¿è­‰é‡‘ 5,000 å…ƒ\\n4. ç­‰å¾…ç®¡ç†ä¸­å¿ƒå¯©æ ¸é€šçŸ¥",
    "keywords": ["åœè»Šä½", "ç”³è«‹", "è¡Œç…§", "ä¿è­‰é‡‘"]
  }}
]

## è¦æ ¼æ›¸å…§å®¹ï¼š

{content}

è«‹åªè¿”å› JSON æ ¼å¼çš„è¼¸å‡ºï¼Œä¸è¦åŒ…å«å…¶ä»–èªªæ˜æ–‡å­—ã€‚"""

        try:
            client = openai.OpenAI(api_key=self.openai_api_key)

            # è¨ˆç®—å®‰å…¨çš„ max_tokens
            # ä¼°ç®—è¼¸å…¥ tokensï¼ˆä¸­æ–‡ç´„ 1 å­— = 2 tokensï¼ŒåŒ…å« system + prompt + contentï¼‰
            estimated_input_tokens = len(content) * 2 + 1000  # +1000 for system and prompt

            # æ ¹æ“šæ¨¡å‹å‹•æ…‹è¨ˆç®—å¯ç”¨çš„è¼¸å‡º tokens
            # gpt-4o: 128K context, gpt-4: 8K context, gpt-4-turbo: 128K context
            max_context = self.MODEL_CONTEXT_LIMITS.get(self.model, 16385)  # é è¨­ 16K

            # è¨ˆç®—å¯ç”¨çš„è¼¸å‡º tokensï¼ˆä¿ç•™ 10% ç·©è¡ï¼‰
            available_output_tokens = int((max_context - estimated_input_tokens) * 0.9)

            # é™åˆ¶è¼¸å‡ºç¯„åœï¼šæœ€å°‘ 1000ï¼Œæœ€å¤š 4000
            safe_max_tokens = max(1000, min(4000, available_output_tokens))

            print(f"   ğŸ“Š Token ä¼°ç®—: è¼¸å…¥ ~{estimated_input_tokens}, è¼¸å‡ºä¸Šé™ {safe_max_tokens}")

            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åº«ç®¡ç†å°ˆå®¶ï¼Œæ“…é•·å¾æŠ€è¡“è¦æ ¼æ›¸ä¸­æå–å¯¦ç”¨çš„Q&Aã€‚è«‹ä»”ç´°åˆ†ææ–‡ä»¶å…§å®¹ï¼Œæå–å°ä½¿ç”¨è€…æœ‰å¯¦éš›å¹«åŠ©çš„å•ç­”å°ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=safe_max_tokens  # è¨­ç½®å‹•æ…‹è¨ˆç®—çš„å®‰å…¨ä¸Šé™
            )

            result_text = response.choices[0].message.content.strip()

            # å˜—è©¦è§£æ JSON
            # ç§»é™¤å¯èƒ½çš„ markdown code block æ¨™è¨˜
            if result_text.startswith('```'):
                result_text = result_text.split('```')[1]
                if result_text.startswith('json'):
                    result_text = result_text[4:]

            qa_list = json.loads(result_text)

            # é©—è­‰æ ¼å¼
            for qa in qa_list:
                if not all(k in qa for k in ['question_summary', 'content', 'keywords']):
                    raise ValueError(f"Q&A æ ¼å¼éŒ¯èª¤: {qa}")

                # ç¢ºä¿ keywords æ˜¯åˆ—è¡¨
                if isinstance(qa['keywords'], str):
                    qa['keywords'] = [k.strip() for k in qa['keywords'].split(',')]

            return qa_list

        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON è§£æå¤±æ•—: {e}")
            print(f"åŸå§‹å›æ‡‰: {result_text[:500]}")
            # è¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯æ‹‹å‡ºéŒ¯èª¤
            return []
        except Exception as e:
            print(f"âŒ OpenAI API å‘¼å«å¤±æ•—: {e}")
            raise

    async def update_qa_list(self, job_id: str, qa_list: List[Dict]) -> Dict:
        """
        æ›´æ–° Q&A åˆ—è¡¨ï¼ˆäººå·¥ç·¨è¼¯å¾Œï¼‰

        Args:
            job_id: ä»»å‹™ ID
            qa_list: æ›´æ–°å¾Œçš„ Q&A åˆ—è¡¨

        Returns:
            æ›´æ–°å¾Œçš„ä»»å‹™è³‡è¨Š
        """
        if job_id not in self.jobs:
            raise ValueError(f"ä»»å‹™ä¸å­˜åœ¨: {job_id}")

        job = self.jobs[job_id]
        job['qa_list'] = qa_list
        job['updated_at'] = datetime.now().isoformat()

        print(f"âœ… Q&A åˆ—è¡¨å·²æ›´æ–° (job_id: {job_id})")
        print(f"   Q&A æ•¸é‡: {len(qa_list)}")

        return job

    async def get_job(self, job_id: str) -> Optional[Dict]:
        """
        ç²å–ä»»å‹™è³‡è¨Š

        Args:
            job_id: ä»»å‹™ ID

        Returns:
            ä»»å‹™è³‡è¨Šï¼Œä¸å­˜åœ¨å‰‡è¿”å› None
        """
        return self.jobs.get(job_id)

    async def estimate_cost(self, content_length: int) -> Dict:
        """
        ä¼°ç®—è½‰æ›æˆæœ¬

        Args:
            content_length: å…§å®¹é•·åº¦ï¼ˆå­—å…ƒæ•¸ï¼‰

        Returns:
            æˆæœ¬ä¼°ç®—è³‡è¨Š
        """
        # GPT-4 pricing (approximate)
        # Input: $0.03 / 1K tokens
        # Output: $0.06 / 1K tokens

        # ä¼°ç®— tokens (ä¸­æ–‡ç´„ 1 å­— = 1.5 tokens, è‹±æ–‡ç´„ 1 å­— = 0.25 tokens)
        # ä¿å®ˆä¼°è¨ˆä½¿ç”¨ 1.5
        estimated_tokens = int(content_length * 1.5)

        # å‡è¨­è¼¸å‡ºæ˜¯è¼¸å…¥çš„ 50%
        output_tokens = int(estimated_tokens * 0.5)

        # è¨ˆç®—æˆæœ¬
        input_cost = (estimated_tokens / 1000) * 0.03
        output_cost = (output_tokens / 1000) * 0.06
        total_cost = input_cost + output_cost

        return {
            'content_length': content_length,
            'estimated_input_tokens': estimated_tokens,
            'estimated_output_tokens': output_tokens,
            'estimated_cost_usd': round(total_cost, 2),
            'model': self.model
        }

    async def cleanup_job(self, job_id: str):
        """
        æ¸…ç†ä»»å‹™æ–‡ä»¶

        Args:
            job_id: ä»»å‹™ ID
        """
        if job_id in self.jobs:
            job = self.jobs[job_id]
            file_path = Path(job['file_path'])

            if file_path.exists():
                file_path.unlink()
                print(f"ğŸ—‘ï¸  å·²åˆªé™¤æ–‡ä»¶: {file_path}")

            del self.jobs[job_id]
            print(f"âœ… ä»»å‹™å·²æ¸…ç† (job_id: {job_id})")

    async def _get_all_intents(self) -> List[Dict]:
        """
        å–å¾—æ‰€æœ‰å¯ç”¨çš„æ„åœ–

        Returns:
            æ„åœ–åˆ—è¡¨ï¼ŒåŒ…å« id, name, description
        """
        if not self.db_pool:
            print("   âš ï¸  æœªè¨­å®šè³‡æ–™åº«é€£æ¥æ± ï¼Œè·³éæ„åœ–è¼‰å…¥")
            return []

        try:
            # ä½¿ç”¨å¿«å–é¿å…é‡è¤‡æŸ¥è©¢
            if self._cached_intents is not None:
                return self._cached_intents

            async with self.db_pool.acquire() as conn:
                rows = await conn.fetch("""
                    SELECT id, name, description
                    FROM intents
                    ORDER BY id
                """)

                self._cached_intents = [dict(row) for row in rows]
                print(f"   âœ… è¼‰å…¥ {len(self._cached_intents)} å€‹æ„åœ–")
                return self._cached_intents

        except Exception as e:
            print(f"   âš ï¸  è¼‰å…¥æ„åœ–å¤±æ•—: {e}")
            return []

    async def _recommend_intent_for_qa(self, qa: Dict) -> Dict:
        """
        ç‚ºå–®å€‹ Q&A æ¨è–¦æ„åœ–

        è¤‡è£½è‡ª knowledge_import_service._recommend_intents() çš„é‚è¼¯

        Args:
            qa: Q&A è³‡æ–™ï¼ŒåŒ…å« question_summary, content, keywords

        Returns:
            æ¨è–¦çµæœï¼ŒåŒ…å« intent_id, intent_name, confidence, reasoning
        """
        try:
            # 1. å–å¾—æ‰€æœ‰æ„åœ–ï¼ˆä½¿ç”¨å¿«å–ï¼‰
            intents = await self._get_all_intents()

            if not intents:
                return {
                    'intent_id': None,
                    'intent_name': 'æœªåˆ†é¡',
                    'confidence': 0.0,
                    'reasoning': 'ç³»çµ±ä¸­æ²’æœ‰å¯ç”¨æ„åœ–'
                }

            # 2. å»ºç«‹æ„åœ–æ¸…å–®æ–‡å­—
            intent_list = "\n".join([
                f"- {i['id']}: {i['name']} ({i['description']})"
                for i in intents
            ])

            # 3. å‘¼å« LLM æ¨è–¦
            keywords_str = ', '.join(qa.get('keywords', []))
            prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹å•ç­”å…§å®¹ï¼Œå¾æ„åœ–æ¸…å–®ä¸­é¸æ“‡æœ€åˆé©çš„æ„åœ–ã€‚

å•é¡Œï¼š{qa['question_summary']}
ç­”æ¡ˆï¼š{qa['content'][:200]}
é—œéµå­—ï¼š{keywords_str}

å¯ç”¨çš„æ„åœ–æ¸…å–®ï¼š
{intent_list}

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "intent_id": æ¨è–¦çš„æ„åœ– IDï¼ˆæ•¸å­—ï¼‰,
  "intent_name": æ„åœ–åç¨±,
  "confidence": ä¿¡å¿ƒåº¦ï¼ˆ0.0-1.0ï¼‰,
  "reasoning": æ¨è–¦ç†ç”±ï¼ˆç°¡çŸ­èªªæ˜ï¼‰
}}

åªè¼¸å‡º JSONï¼Œä¸è¦åŠ å…¶ä»–èªªæ˜ã€‚"""

            client = openai.OpenAI(api_key=self.openai_api_key)
            response = client.chat.completions.create(
                model=self.model,
                temperature=0.3,
                max_tokens=500,  # æ„åœ–æ¨è–¦åªéœ€è¦å°é‡è¼¸å‡º
                response_format={"type": "json_object"},
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.choices[0].message.content)

            return {
                'intent_id': result.get('intent_id'),
                'intent_name': result.get('intent_name'),
                'confidence': result.get('confidence', 0.8),
                'reasoning': result.get('reasoning', '')
            }

        except Exception as e:
            print(f"   âš ï¸  æ„åœ–æ¨è–¦å¤±æ•—: {e}")
            return {
                'intent_id': None,
                'intent_name': 'æœªåˆ†é¡',
                'confidence': 0.0,
                'reasoning': f'æ¨è–¦å¤±æ•—: {str(e)}'
            }
