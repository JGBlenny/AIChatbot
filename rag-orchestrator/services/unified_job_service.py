"""
çµ±ä¸€ Job ç®¡ç†æœå‹™ - åŸºé¡

ç”¨é€”ï¼šç‚ºæ‰€æœ‰ç•°æ­¥ä½œæ¥­ï¼ˆåŒ¯å…¥ã€åŒ¯å‡ºã€è½‰æ›ç­‰ï¼‰æä¾›çµ±ä¸€çš„ CRUD ä»‹é¢
ç‰¹é»ï¼š
- çµ±ä¸€ç‹€æ…‹ç®¡ç†ï¼ˆpending â†’ processing â†’ completed/failedï¼‰
- çµ±ä¸€é€²åº¦è¿½è¹¤
- çµ±ä¸€éŒ¯èª¤è™•ç†
- å½ˆæ€§é…ç½®ï¼ˆJSONBï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    class KnowledgeImportService(UnifiedJobService):
        async def process_import_job(self, job_id, ...):
            await self.update_status(job_id, 'processing')
            # ... è™•ç†é‚è¼¯
            await self.update_status(job_id, 'completed', result={...})

å¯¦ä½œæ—¥æœŸï¼š2025-11-21
ç›¸é—œæ–‡ä»¶ï¼šdocs/planning/UNIFIED_JOB_SYSTEM_DESIGN.md
"""

import uuid
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from asyncpg.pool import Pool
import asyncpg


class UnifiedJobService:
    """çµ±ä¸€ Job ç®¡ç†æœå‹™ï¼ˆåŸºé¡ï¼‰"""

    def __init__(self, db_pool: Optional[Pool] = None):
        """
        åˆå§‹åŒ–çµ±ä¸€ Job æœå‹™

        Args:
            db_pool: è³‡æ–™åº«é€£æ¥æ± 
        """
        self.db_pool = db_pool

    # ==================== Job ç”Ÿå‘½é€±æœŸç®¡ç† ====================

    async def create_job(
        self,
        job_type: str,
        vendor_id: Optional[int],
        user_id: str,
        job_config: Dict,
        file_path: Optional[str] = None,
        file_name: Optional[str] = None,
        file_size_bytes: Optional[int] = None,
        expires_days: int = 7
    ) -> str:
        """
        å‰µå»ºæ–°ä½œæ¥­

        Args:
            job_type: ä½œæ¥­é¡å‹ï¼ˆknowledge_import, knowledge_export, document_convert ç­‰ï¼‰
            vendor_id: æ¥­è€… IDï¼ˆNone è¡¨ç¤ºé€šç”¨çŸ¥è­˜ï¼‰
            user_id: ä½¿ç”¨è€… ID
            job_config: ä½œæ¥­é…ç½®ï¼ˆJSONBï¼Œé¡å‹ç‰¹å®šåƒæ•¸ï¼‰
            file_path: æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
            file_name: æª”æ¡ˆåç¨±ï¼ˆå¯é¸ï¼‰
            file_size_bytes: æª”æ¡ˆå¤§å°ï¼ˆå¯é¸ï¼‰
            expires_days: æª”æ¡ˆéæœŸå¤©æ•¸ï¼ˆé è¨­ 7 å¤©ï¼‰

        Returns:
            str: Job ID
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        job_id = str(uuid.uuid4())
        expires_at = datetime.now() + timedelta(days=expires_days) if file_path else None

        async with self.db_pool.acquire() as conn:
            await conn.execute("""
                INSERT INTO unified_jobs (
                    job_id, job_type, vendor_id, user_id,
                    status, job_config,
                    file_path, file_name, file_size_bytes, expires_at,
                    created_at, updated_at
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
            """,
                uuid.UUID(job_id),
                job_type,
                vendor_id,
                user_id,
                'pending',
                json.dumps(job_config),
                file_path,
                file_name,
                file_size_bytes,
                expires_at
            )

        print(f"âœ… ä½œæ¥­å·²å‰µå»º")
        print(f"   Job ID: {job_id}")
        print(f"   é¡å‹: {job_type}")
        print(f"   æ¥­è€… ID: {vendor_id or 'é€šç”¨çŸ¥è­˜'}")

        return job_id

    async def update_status(
        self,
        job_id: str,
        status: str,
        progress: Optional[Dict] = None,
        result: Optional[Dict] = None,
        error_message: Optional[str] = None,
        error_details: Optional[Dict] = None,
        **record_updates
    ):
        """
        æ›´æ–°ä½œæ¥­ç‹€æ…‹

        Args:
            job_id: ä½œæ¥­ ID
            status: æ–°ç‹€æ…‹ï¼ˆpending, processing, completed, failed, cancelledï¼‰
            progress: é€²åº¦è³‡è¨Šï¼ˆå¯é¸ï¼‰
            result: çµæœè³‡è¨Šï¼ˆå¯é¸ï¼‰
            error_message: éŒ¯èª¤è¨Šæ¯ï¼ˆå¯é¸ï¼‰
            error_details: è©³ç´°éŒ¯èª¤è³‡è¨Šï¼ˆå¯é¸ï¼‰
            **record_updates: å…¶ä»–æ¬„ä½æ›´æ–°ï¼ˆå¦‚ total_records, processed_records ç­‰ï¼‰
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        # å»ºæ§‹æ›´æ–° SQL
        update_parts = ["status = $2", "updated_at = CURRENT_TIMESTAMP"]
        params = [uuid.UUID(job_id), status]
        param_index = 3

        if progress is not None:
            update_parts.append(f"progress = ${param_index}")
            params.append(json.dumps(progress))
            param_index += 1

        if result is not None:
            update_parts.append(f"job_result = ${param_index}")
            params.append(json.dumps(result))
            param_index += 1

        if error_message is not None:
            update_parts.append(f"error_message = ${param_index}")
            params.append(error_message)
            param_index += 1

        if error_details is not None:
            update_parts.append(f"error_details = ${param_index}")
            params.append(json.dumps(error_details))
            param_index += 1

        # è™•ç†å…¶ä»–æ¬„ä½æ›´æ–°
        for key, value in record_updates.items():
            update_parts.append(f"{key} = ${param_index}")
            params.append(value)
            param_index += 1

        sql = f"""
            UPDATE unified_jobs
            SET {', '.join(update_parts)}
            WHERE job_id = $1
        """

        async with self.db_pool.acquire() as conn:
            await conn.execute(sql, *params)

    async def get_job(self, job_id: str) -> Optional[Dict]:
        """
        ç²å–ä½œæ¥­è©³æƒ…

        Args:
            job_id: ä½œæ¥­ ID

        Returns:
            Optional[Dict]: ä½œæ¥­è³‡è¨Šï¼Œä¸å­˜åœ¨å‰‡è¿”å› None
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT
                    job_id, job_type, vendor_id, user_id,
                    status, progress, job_config, job_result,
                    error_message, error_details,
                    total_records, processed_records, success_records, failed_records, skipped_records,
                    file_path, file_name, file_size_bytes,
                    created_at, updated_at, started_at, completed_at,
                    processing_time_seconds, expires_at
                FROM unified_jobs
                WHERE job_id = $1
            """, uuid.UUID(job_id))

            if not row:
                return None

            # è§£æ JSON æ¬„ä½
            return {
                'job_id': str(row['job_id']),
                'job_type': row['job_type'],
                'vendor_id': row['vendor_id'],
                'user_id': row['user_id'],
                'status': row['status'],
                'progress': json.loads(row['progress']) if row['progress'] else None,
                'config': json.loads(row['job_config']) if row['job_config'] else None,
                'result': json.loads(row['job_result']) if row['job_result'] else None,
                'error_message': row['error_message'],
                'error_details': json.loads(row['error_details']) if row['error_details'] else None,
                'total_records': row['total_records'],
                'processed_records': row['processed_records'],
                'success_records': row['success_records'],
                'failed_records': row['failed_records'],
                'skipped_records': row['skipped_records'],
                'file_path': row['file_path'],
                'file_name': row['file_name'],
                'file_size_bytes': row['file_size_bytes'],
                'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                'updated_at': row['updated_at'].isoformat() if row['updated_at'] else None,
                'started_at': row['started_at'].isoformat() if row['started_at'] else None,
                'completed_at': row['completed_at'].isoformat() if row['completed_at'] else None,
                'processing_time_seconds': row['processing_time_seconds'],
                'expires_at': row['expires_at'].isoformat() if row['expires_at'] else None
            }

    async def list_jobs(
        self,
        job_type: Optional[str] = None,
        vendor_id: Optional[int] = None,
        user_id: Optional[str] = None,
        status: Optional[str] = None,
        limit: int = 20,
        offset: int = 0
    ) -> Dict:
        """
        åˆ—å‡ºä½œæ¥­ï¼ˆæ”¯æ´å¤šç¶­åº¦éæ¿¾ï¼‰

        Args:
            job_type: ä½œæ¥­é¡å‹ï¼ˆå¯é¸ï¼‰
            vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
            user_id: ä½¿ç”¨è€… IDï¼ˆå¯é¸ï¼‰
            status: ç‹€æ…‹ï¼ˆå¯é¸ï¼‰
            limit: è¿”å›æ•¸é‡é™åˆ¶
            offset: åç§»é‡

        Returns:
            Dict: {jobs: List[Dict], total: int, limit: int, offset: int}
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        # å»ºæ§‹ WHERE æ¢ä»¶
        where_parts = []
        params = []
        param_index = 1

        if job_type is not None:
            where_parts.append(f"job_type = ${param_index}")
            params.append(job_type)
            param_index += 1

        if vendor_id is not None:
            where_parts.append(f"vendor_id = ${param_index}")
            params.append(vendor_id)
            param_index += 1

        if user_id is not None:
            where_parts.append(f"user_id = ${param_index}")
            params.append(user_id)
            param_index += 1

        if status is not None:
            where_parts.append(f"status = ${param_index}")
            params.append(status)
            param_index += 1

        where_clause = f"WHERE {' AND '.join(where_parts)}" if where_parts else ""

        # æ·»åŠ  limit å’Œ offset
        params.extend([limit, offset])

        async with self.db_pool.acquire() as conn:
            # æŸ¥è©¢åˆ—è¡¨
            rows = await conn.fetch(f"""
                SELECT
                    job_id, job_type, vendor_id, status,
                    total_records, processed_records, success_records,
                    file_name, file_size_bytes,
                    created_at, completed_at, processing_time_seconds
                FROM unified_jobs
                {where_clause}
                ORDER BY created_at DESC
                LIMIT ${param_index} OFFSET ${param_index + 1}
            """, *params)

            # æŸ¥è©¢ç¸½æ•¸
            total = await conn.fetchval(f"""
                SELECT COUNT(*) FROM unified_jobs {where_clause}
            """, *params[:-2])  # æ’é™¤ limit å’Œ offset

            jobs = [
                {
                    'job_id': str(row['job_id']),
                    'job_type': row['job_type'],
                    'vendor_id': row['vendor_id'],
                    'status': row['status'],
                    'total_records': row['total_records'],
                    'processed_records': row['processed_records'],
                    'success_records': row['success_records'],
                    'file_name': row['file_name'],
                    'file_size_bytes': row['file_size_bytes'],
                    'created_at': row['created_at'].isoformat() if row['created_at'] else None,
                    'completed_at': row['completed_at'].isoformat() if row['completed_at'] else None,
                    'processing_time_seconds': row['processing_time_seconds']
                }
                for row in rows
            ]

            return {
                'jobs': jobs,
                'total': total,
                'limit': limit,
                'offset': offset
            }

    async def delete_job(self, job_id: str, delete_file: bool = True):
        """
        åˆªé™¤ä½œæ¥­ï¼ˆå«æª”æ¡ˆï¼‰

        Args:
            job_id: ä½œæ¥­ ID
            delete_file: æ˜¯å¦åˆªé™¤é—œè¯çš„æª”æ¡ˆï¼ˆé è¨­ Trueï¼‰
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        async with self.db_pool.acquire() as conn:
            # å–å¾—æª”æ¡ˆè·¯å¾‘
            job = await self.get_job(job_id)
            if not job:
                raise Exception(f"ä½œæ¥­ä¸å­˜åœ¨: {job_id}")

            # åˆªé™¤å¯¦é«”æª”æ¡ˆ
            if delete_file and job.get('file_path'):
                import os
                file_path = job['file_path']
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        print(f"âœ… å·²åˆªé™¤æª”æ¡ˆ: {file_path}")
                    except Exception as e:
                        print(f"âš ï¸ ç„¡æ³•åˆªé™¤æª”æ¡ˆ: {e}")

            # åˆªé™¤è³‡æ–™åº«è¨˜éŒ„
            await conn.execute("""
                DELETE FROM unified_jobs WHERE job_id = $1
            """, uuid.UUID(job_id))

            print(f"âœ… ä½œæ¥­å·²åˆªé™¤ (job_id: {job_id})")

    async def cancel_job(self, job_id: str, reason: str = "ä½¿ç”¨è€…å–æ¶ˆ"):
        """
        å–æ¶ˆä½œæ¥­

        Args:
            job_id: ä½œæ¥­ ID
            reason: å–æ¶ˆåŸå› 
        """
        await self.update_status(
            job_id,
            status='cancelled',
            error_message=reason
        )
        print(f"âš ï¸ ä½œæ¥­å·²å–æ¶ˆ (job_id: {job_id}): {reason}")

    # ==================== çµ±è¨ˆèˆ‡ç›£æ§ ====================

    async def get_statistics(
        self,
        job_type: Optional[str] = None,
        vendor_id: Optional[int] = None,
        days: int = 30
    ) -> Dict:
        """
        ç²å–çµ±è¨ˆè³‡è¨Š

        Args:
            job_type: ä½œæ¥­é¡å‹ï¼ˆå¯é¸ï¼ŒNone è¡¨ç¤ºæ‰€æœ‰é¡å‹ï¼‰
            vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
            days: çµ±è¨ˆå¤©æ•¸

        Returns:
            Dict: çµ±è¨ˆè³‡è¨Š
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        # å»ºæ§‹ WHERE æ¢ä»¶
        where_parts = [f"created_at > CURRENT_TIMESTAMP - INTERVAL '{days} days'"]
        params = []
        param_index = 1

        if job_type is not None:
            where_parts.append(f"job_type = ${param_index}")
            params.append(job_type)
            param_index += 1

        if vendor_id is not None:
            where_parts.append(f"vendor_id = ${param_index}")
            params.append(vendor_id)
            param_index += 1

        where_clause = ' AND '.join(where_parts)

        async with self.db_pool.acquire() as conn:
            # åŸºç¤çµ±è¨ˆ
            stats = await conn.fetchrow(f"""
                SELECT
                    COUNT(*) as total_jobs,
                    COUNT(*) FILTER (WHERE status = 'completed') as completed_jobs,
                    COUNT(*) FILTER (WHERE status = 'failed') as failed_jobs,
                    COUNT(*) FILTER (WHERE status = 'processing') as processing_jobs,
                    COUNT(*) FILTER (WHERE status = 'pending') as pending_jobs,
                    COUNT(*) FILTER (WHERE status = 'cancelled') as cancelled_jobs,
                    COALESCE(SUM(total_records), 0) as total_records_sum,
                    COALESCE(SUM(success_records), 0) as success_records_sum,
                    COALESCE(SUM(failed_records), 0) as failed_records_sum,
                    COALESCE(AVG(processing_time_seconds), 0) as avg_processing_time
                FROM unified_jobs
                WHERE {where_clause}
            """, *params)

            # æŒ‰é¡å‹çµ±è¨ˆï¼ˆå¦‚æœæ²’æœ‰æŒ‡å®š job_typeï¼‰
            by_type = None
            if job_type is None:
                type_stats = await conn.fetch(f"""
                    SELECT
                        job_type,
                        COUNT(*) as count,
                        COUNT(*) FILTER (WHERE status = 'completed') as completed,
                        AVG(processing_time_seconds) as avg_time
                    FROM unified_jobs
                    WHERE {where_clause}
                    GROUP BY job_type
                    ORDER BY count DESC
                """, *params)

                by_type = [
                    {
                        'type': row['job_type'],
                        'count': row['count'],
                        'completed': row['completed'],
                        'avg_time_seconds': round(float(row['avg_time']), 2) if row['avg_time'] else 0
                    }
                    for row in type_stats
                ]

            # è¨ˆç®—æˆåŠŸç‡
            total = stats['total_jobs'] or 0
            completed = stats['completed_jobs'] or 0
            success_rate = (completed / total * 100) if total > 0 else 0

            return {
                'total_jobs': total,
                'completed_jobs': completed,
                'failed_jobs': stats['failed_jobs'],
                'processing_jobs': stats['processing_jobs'],
                'pending_jobs': stats['pending_jobs'],
                'cancelled_jobs': stats['cancelled_jobs'],
                'total_records_processed': stats['total_records_sum'],
                'success_records': stats['success_records_sum'],
                'failed_records': stats['failed_records_sum'],
                'avg_processing_time_seconds': round(float(stats['avg_processing_time']), 2),
                'success_rate': round(success_rate, 2),
                'by_type': by_type,
                'days': days
            }

    # ==================== æ¸…ç†èˆ‡ç¶­è­· ====================

    async def cleanup_expired_jobs(self, dry_run: bool = True) -> Dict:
        """
        æ¸…ç†éæœŸçš„ä½œæ¥­æª”æ¡ˆ

        Args:
            dry_run: æ˜¯å¦ç‚ºæ¨¡æ“¬é‹è¡Œï¼ˆä¸å¯¦éš›åˆªé™¤ï¼‰

        Returns:
            Dict: æ¸…ç†çµ±è¨ˆ
        """
        if not self.db_pool:
            raise Exception("è³‡æ–™åº«é€£ç·šæ± æœªåˆå§‹åŒ–")

        async with self.db_pool.acquire() as conn:
            # æŸ¥è©¢éæœŸçš„ jobs
            expired_jobs = await conn.fetch("""
                SELECT job_id, file_path
                FROM unified_jobs
                WHERE expires_at IS NOT NULL
                  AND expires_at < CURRENT_TIMESTAMP
                  AND status = 'completed'
            """)

            deleted_count = 0
            deleted_size = 0

            for job in expired_jobs:
                job_id = str(job['job_id'])
                file_path = job['file_path']

                if not dry_run:
                    try:
                        await self.delete_job(job_id, delete_file=True)
                        deleted_count += 1
                    except Exception as e:
                        print(f"âš ï¸ æ¸…ç†å¤±æ•— (job_id: {job_id}): {e}")
                else:
                    deleted_count += 1
                    print(f"ğŸ” [æ¨¡æ“¬] å°‡åˆªé™¤: {job_id} ({file_path})")

            return {
                'deleted_count': deleted_count,
                'dry_run': dry_run,
                'message': f"{'[æ¨¡æ“¬é‹è¡Œ] ' if dry_run else ''}å·²æ¸…ç† {deleted_count} å€‹éæœŸä½œæ¥­"
            }
