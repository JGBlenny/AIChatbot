"""
AI çŸ¥è­˜ç”Ÿæˆæœå‹™
ä½¿ç”¨ OpenAI API ç‚ºæ¸¬è©¦æƒ…å¢ƒç”ŸæˆçŸ¥è­˜åº«å€™é¸ç­”æ¡ˆ
"""
import os
import json
import asyncio
from typing import List, Dict, Optional
import httpx
from difflib import SequenceMatcher


class KnowledgeGenerator:
    """ä½¿ç”¨ OpenAI ç”ŸæˆçŸ¥è­˜å€™é¸ç­”æ¡ˆ"""

    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY ç’°å¢ƒè®Šæ•¸æœªè¨­å®š")

        # çŸ¥è­˜ç”Ÿæˆå°ˆç”¨æ¨¡å‹ï¼ˆé è¨­ gpt-3.5-turboï¼Œæˆæœ¬è¼ƒä½ï¼‰
        # ç³»çµ±å…¶ä»–éƒ¨åˆ†ä½¿ç”¨ gpt-4o-mini
        self.model = os.getenv("KNOWLEDGE_GEN_MODEL", "gpt-3.5-turbo")
        self.api_url = "https://api.openai.com/v1/chat/completions"

        print(f"âœ… KnowledgeGenerator åˆå§‹åŒ–å®Œæˆ")
        print(f"   ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"   ï¼ˆç³»çµ±é è¨­: gpt-4o-miniï¼ŒçŸ¥è­˜ç”Ÿæˆ: {self.model}ï¼‰")

        # å®‰å…¨é¡åˆ¥ç™½åå–®ï¼ˆåªç‚ºé€™äº›é¡åˆ¥ç”ŸæˆçŸ¥è­˜ï¼‰
        self.safe_categories = [
            "è¨­æ–½ä½¿ç”¨",
            "ç¤¾å€è¦ç¯„",
            "å¸¸è¦‹å•é¡Œ",
            "å…¬å…±ç©ºé–“",
            "åƒåœ¾è™•ç†",
            "åœè»Šè¦å®š"
        ]

        # é™åˆ¶é¡åˆ¥ï¼ˆä¸è‡ªå‹•ç”Ÿæˆï¼Œéœ€äººå·¥å¯©æ ¸ï¼‰
        self.restricted_categories = [
            "ç§Ÿé‡‘è¨ˆç®—",
            "åˆç´„æ¢æ¬¾",
            "é€€ç§Ÿæµç¨‹",
            "æŠ¼é‡‘è™•ç†",
            "æ³•å¾‹è«®è©¢"
        ]

    def is_safe_category(self, category: str) -> bool:
        """æª¢æŸ¥é¡åˆ¥æ˜¯å¦å®‰å…¨ï¼ˆå¯è‡ªå‹•ç”Ÿæˆï¼‰"""
        return category in self.safe_categories

    def is_restricted_category(self, category: str) -> bool:
        """æª¢æŸ¥é¡åˆ¥æ˜¯å¦å—é™ï¼ˆä¸å»ºè­°è‡ªå‹•ç”Ÿæˆï¼‰"""
        return category in self.restricted_categories

    async def generate_knowledge_candidates(
        self,
        test_question: str,
        intent_category: str,
        num_candidates: int = 3,
        context: Optional[Dict] = None
    ) -> List[Dict]:
        """
        ç‚ºæ¸¬è©¦å•é¡Œç”ŸæˆçŸ¥è­˜å€™é¸

        Args:
            test_question: æ¸¬è©¦å•é¡Œ
            intent_category: æ„åœ–åˆ†é¡ï¼ˆå¦‚ï¼šç§Ÿé‡‘ç¹³ç´ã€å¯µç‰©é£¼é¤Šï¼‰
            num_candidates: ç”Ÿæˆå€™é¸æ•¸é‡ï¼ˆ1-3ï¼‰
            context: é¡å¤–ä¸Šä¸‹æ–‡ï¼ˆæ¥­å‹™ç¯„åœã€ç¾æœ‰çŸ¥è­˜æ‘˜è¦ç­‰ï¼‰

        Returns:
            List[Dict]: å€™é¸çŸ¥è­˜åˆ—è¡¨ï¼ˆå·²å»é‡ï¼‰
        """

        # æª¢æŸ¥é¡åˆ¥é™åˆ¶
        if self.is_restricted_category(intent_category):
            print(f"âš ï¸  é¡åˆ¥ã€Œ{intent_category}ã€ç‚ºé™åˆ¶é¡åˆ¥ï¼Œä¸å»ºè­°è‡ªå‹•ç”ŸæˆçŸ¥è­˜")
            return [{
                "question": test_question,
                "answer": "ï¼ˆæ­¤é¡åˆ¥ä¸å»ºè­°è‡ªå‹•ç”Ÿæˆï¼Œè«‹äººå·¥æ’°å¯«ï¼‰",
                "category": intent_category,
                "confidence_score": 0.0,
                "reasoning": "å—é™é¡åˆ¥ï¼Œéœ€è¦äººå·¥å¯©æ ¸",
                "sources_needed": ["åˆç´„æ¢æ¬¾", "å…¬å¸æ”¿ç­–", "æ³•å¾‹æ¢æ–‡"],
                "warnings": ["æ­¤é¡åˆ¥æ¶‰åŠæ³•å¾‹ã€é‡‘éŒ¢æˆ–åˆç´„ï¼Œä¸å»ºè­°ä½¿ç”¨ AI ç”Ÿæˆ"]
            }]

        # æ§‹å»º prompt
        prompt = self._build_generation_prompt(
            test_question,
            intent_category,
            context
        )

        # å‘¼å« OpenAI API
        try:
            candidates = await self._call_openai_api(
                prompt=prompt,
                num_candidates=num_candidates
            )

            print(f"ğŸ¤– AI ç”Ÿæˆäº† {len(candidates)} å€‹åŸå§‹å€™é¸ç­”æ¡ˆ")

            # å»é‡ï¼šéæ¿¾ç›¸ä¼¼åº¦éé«˜çš„å€™é¸
            deduplicated = self._deduplicate_candidates(candidates)

            if len(deduplicated) < len(candidates):
                print(f"ğŸ”„ å»é‡å¾Œä¿ç•™ {len(deduplicated)} å€‹ä¸åŒçš„å€™é¸ç­”æ¡ˆï¼ˆç§»é™¤äº† {len(candidates) - len(deduplicated)} å€‹é‡è¤‡ï¼‰")

            print(f"âœ… æœ€çµ‚ç‚ºã€Œ{test_question}ã€æä¾› {len(deduplicated)} å€‹å€™é¸ç­”æ¡ˆ")
            return deduplicated

        except Exception as e:
            print(f"âŒ AI çŸ¥è­˜ç”Ÿæˆå¤±æ•—: {str(e)}")
            raise

    def _get_system_prompt(self) -> str:
        """ç³»çµ± prompt - å®šç¾© AI çš„è§’è‰²å’Œé™åˆ¶"""
        return """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç§Ÿå±‹ç®¡ç†çŸ¥è­˜å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯ç‚ºç§Ÿå®¢å¸¸è¦‹å•é¡Œç”Ÿæˆæº–ç¢ºã€å°ˆæ¥­çš„ç­”æ¡ˆã€‚

ã€é‡è¦åŸå‰‡ã€‘
1. **æº–ç¢ºæ€§å„ªå…ˆ**ï¼šå¦‚æœä¸ç¢ºå®šç­”æ¡ˆï¼Œæ˜ç¢ºèªªæ˜éœ€è¦äººå·¥å¯©æ ¸æˆ–ç¢ºèªåˆç´„
2. **å¼•ç”¨ä¾†æº**ï¼šèªªæ˜ç­”æ¡ˆæ‡‰åŸºæ–¼ä»€éº¼ä¾†æºï¼ˆæ³•è¦ã€åˆç´„ã€å…¬å¸æ”¿ç­–ï¼‰
3. **é¢¨éšªæç¤º**ï¼šæ¨™è¨»å¯èƒ½å› å€‹æ¡ˆè€Œç•°çš„æƒ…æ³
4. **å…·é«”å¯¦ç”¨**ï¼šæä¾›å…·é«”æ­¥é©Ÿæˆ–æ˜ç¢ºæŒ‡å¼•ï¼Œé¿å…æ¨¡ç³Šå›ç­”
5. **çµæ§‹åŒ–è¼¸å‡º**ï¼šåš´æ ¼ä½¿ç”¨ JSON æ ¼å¼è¿”å›

ã€ç­”æ¡ˆæ’°å¯«è¦ç¯„ã€‘
- é•·åº¦ï¼š200-400 å­—
- èªæ°£ï¼šå°ˆæ¥­ã€å‹å–„ã€æ¸…æ™°
- çµæ§‹ï¼šç¸½è¿° + å…·é«”èªªæ˜ + è£œå……æé†’
- é¿å…ï¼šæ³•å¾‹è¡“èªã€æ¨¡ç³Šè¡¨é”ã€æœªç¶“è­‰å¯¦çš„è³‡è¨Š

ã€è¼¸å‡ºæ ¼å¼ã€‘
{
  "answer": "å®Œæ•´ç­”æ¡ˆï¼ˆ2-3 æ®µï¼Œæ¸…æ™°æ˜“æ‡‚ï¼‰",
  "confidence": 0.8,
  "reasoning": "ç”Ÿæˆæ­¤ç­”æ¡ˆçš„æ¨ç†éç¨‹",
  "sources": ["ç§Ÿè³ƒå¥‘ç´„ç¬¬5æ¢", "æ°‘æ³•ç¬¬423æ¢"],
  "warnings": ["æ­¤ç­”æ¡ˆå¯èƒ½å› ç‰©ä»¶è€Œç•°ï¼Œå»ºè­°ç¢ºèªåˆç´„"]
}

ã€ç¯„ä¾‹è¼¸å‡ºã€‘
{
  "answer": "ä¸€èˆ¬ä¾†èªªï¼Œç§Ÿå±‹è™•æ˜¯å¯ä»¥é£¼é¤Šå¯µç‰©çš„ï¼Œä½†éœ€è¦éµå®ˆä»¥ä¸‹è¦å®šï¼š\\n\\n1. **äº‹å‰å‘ŠçŸ¥**ï¼šåœ¨ç°½ç´„å‰æˆ–é£¼é¤Šå‰ï¼Œå‹™å¿…å‘ŠçŸ¥æˆ¿æ±ä¸¦å–å¾—æ›¸é¢åŒæ„ã€‚\\n2. **éµå®ˆç¤¾å€è¦ç¯„**ï¼šä¸å¾—å½±éŸ¿å…¶ä»–ä½æˆ¶å®‰å¯§ï¼Œéœ€åšå¥½æ¸…æ½”ç®¡ç†ã€‚\\n3. **æŠ¼é‡‘è¦å®š**ï¼šéƒ¨åˆ†æˆ¿æ±æœƒè¦æ±‚é¡å¤–å¯µç‰©æŠ¼é‡‘ï¼Œä»¥ä¿éšœæˆ¿å±‹ç¶­è­·ã€‚\\n\\nå»ºè­°æ‚¨æŸ¥çœ‹ç§Ÿè³ƒåˆç´„ä¸­çš„ã€Œå¯µç‰©é£¼é¤Šæ¢æ¬¾ã€ï¼Œæˆ–ç›´æ¥è¯ç¹«æˆ¿æ±ç¢ºèªã€‚å¦‚åˆç´„æœªæ˜ç¢ºè¦å®šï¼Œå¯èˆ‡æˆ¿æ±å”å•†ä¸¦ç°½ç½²è£œå……å”è­°ã€‚",
  "confidence": 0.75,
  "reasoning": "æ ¹æ“šä¸€èˆ¬ç§Ÿå±‹æ…£ä¾‹å’Œå¸¸è¦‹åˆç´„æ¢æ¬¾æ¨æ–·ã€‚å¯µç‰©é£¼é¤Šé€šå¸¸éœ€æˆ¿æ±åŒæ„ï¼Œä½†å…·é«”è¦å®šå› åˆç´„è€Œç•°ã€‚",
  "sources": ["ç§Ÿè³ƒå¥‘ç´„å¯µç‰©æ¢æ¬¾", "ç¤¾å€ç®¡ç†è¦ç¯„"],
  "warnings": ["å¯¦éš›è¦å®šè«‹ä»¥ç§Ÿè³ƒåˆç´„ç‚ºæº–", "ä¸åŒæˆ¿æ±æ”¿ç­–å¯èƒ½ä¸åŒ"]
}"""

    def _build_generation_prompt(
        self,
        question: str,
        category: str,
        context: Optional[Dict]
    ) -> str:
        """æ§‹å»ºç”Ÿæˆ prompt"""

        prompt = f"""è«‹ç‚ºä»¥ä¸‹ç§Ÿå±‹ç›¸é—œå•é¡Œç”Ÿæˆå°ˆæ¥­ç­”æ¡ˆï¼š

ã€å•é¡Œã€‘
{question}

ã€åˆ†é¡ã€‘
{category}

ã€æ¥­å‹™ç¯„åœã€‘
- ç§Ÿå±‹ç®¡ç†ç³»çµ±
- æœå‹™å°è±¡ï¼šç§Ÿå®¢
- å¸¸è¦‹å ´æ™¯ï¼šç§Ÿé‡‘ã€ç¶­ä¿®ã€åˆç´„ã€è¨­æ–½ä½¿ç”¨ã€ç¤¾å€è¦ç¯„

"""

        # åŠ å…¥å·²æœ‰çŸ¥è­˜ä½œç‚ºåƒè€ƒï¼ˆå¦‚æœæœ‰ï¼‰
        if context and context.get("related_knowledge"):
            related = context["related_knowledge"]
            if len(related) > 0:
                prompt += "\nã€ç›¸é—œç¾æœ‰çŸ¥è­˜ï¼ˆä¾›åƒè€ƒï¼Œä¸è¦ç…§æŠ„ï¼‰ã€‘\n"
                for k in related[:3]:
                    prompt += f"- Q: {k.get('question', 'N/A')}\n"
                    answer_preview = k.get('answer', '')[:150]
                    prompt += f"  A: {answer_preview}{'...' if len(k.get('answer', '')) > 150 else ''}\n\n"

        # å®‰å…¨æé†’
        if self.is_safe_category(category):
            prompt += "\nã€æé†’ã€‘æ­¤é¡åˆ¥ç‚ºå®‰å…¨é¡åˆ¥ï¼Œå¯ä»¥è‡ªç”±ç”Ÿæˆç­”æ¡ˆã€‚\n"
        else:
            prompt += "\nã€æé†’ã€‘æ­¤é¡åˆ¥å¯èƒ½æ¶‰åŠå°ˆæ¥­çŸ¥è­˜ï¼Œè«‹æ¨™è¨»ä¸ç¢ºå®šä¹‹è™•ã€‚\n"

        prompt += "\nè«‹ç”Ÿæˆç­”æ¡ˆï¼ˆJSON æ ¼å¼ï¼‰ï¼š"

        return prompt

    async def _call_openai_api(
        self,
        prompt: str,
        num_candidates: int
    ) -> List[Dict]:
        """å‘¼å« OpenAI API"""

        headers = {
            "Authorization": f"Bearer {self.openai_api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": self._get_system_prompt()},
                {"role": "user", "content": prompt}
            ],
            "temperature": float(os.getenv("KNOWLEDGE_GEN_TEMPERATURE", "0.7")),
            "n": num_candidates,  # ç”Ÿæˆå¤šå€‹å€™é¸
            "max_tokens": int(os.getenv("KNOWLEDGE_GEN_MAX_TOKENS", "800")),
            "response_format": {"type": "json_object"}  # å¼·åˆ¶ JSON è¼¸å‡ºï¼ˆGPT-4 æ”¯æ´ï¼‰
        }

        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                self.api_url,
                headers=headers,
                json=payload
            )

            if response.status_code != 200:
                raise Exception(f"OpenAI API éŒ¯èª¤: {response.status_code} - {response.text}")

            data = response.json()

            candidates = []
            for choice in data["choices"]:
                content = choice["message"]["content"]

                # è§£æ AI å›æ‡‰
                parsed = self._parse_ai_response(content)

                candidates.append(parsed)

            return candidates

    def _parse_ai_response(self, content: str) -> Dict:
        """è§£æ AI å›æ‡‰"""
        try:
            # å˜—è©¦è§£æ JSON
            data = json.loads(content)

            # é©—è­‰å¿…è¦æ¬„ä½
            if "answer" not in data:
                raise ValueError("ç¼ºå°‘ answer æ¬„ä½")

            # è£œå……é è¨­å€¼
            return {
                "answer": data["answer"],
                "confidence_score": float(data.get("confidence", 0.7)),
                "reasoning": data.get("reasoning", ""),
                "sources_needed": data.get("sources", []),
                "warnings": data.get("warnings", [])
            }

        except (json.JSONDecodeError, ValueError) as e:
            print(f"âš ï¸  AI å›æ‡‰è§£æå¤±æ•—: {str(e)}")
            print(f"   åŸå§‹å›æ‡‰: {content[:200]}...")

            # å¦‚æœè§£æå¤±æ•—ï¼Œå˜—è©¦æå–æ–‡å­—
            return {
                "answer": content,
                "confidence_score": 0.5,
                "reasoning": "AI å›æ‡‰æ ¼å¼éé æœŸï¼Œéœ€äººå·¥æª¢æŸ¥",
                "sources_needed": [],
                "warnings": ["éœ€è¦äººå·¥å¯©æ ¸æ ¼å¼å’Œå…§å®¹"]
            }

    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """
        è¨ˆç®—å…©æ®µæ–‡å­—çš„ç›¸ä¼¼åº¦ (0.0 ~ 1.0)

        ä½¿ç”¨ SequenceMatcher è¨ˆç®—å­—ç¬¦å±¤ç´šçš„ç›¸ä¼¼åº¦

        Args:
            text1: ç¬¬ä¸€æ®µæ–‡å­—
            text2: ç¬¬äºŒæ®µæ–‡å­—

        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•¸ (0.0 = å®Œå…¨ä¸åŒ, 1.0 = å®Œå…¨ç›¸åŒ)
        """
        if not text1 or not text2:
            return 0.0

        # ç§»é™¤å¤šé¤˜ç©ºç™½ä¸¦è½‰å°å¯«ä»¥æé«˜æ¯”è¼ƒæº–ç¢ºæ€§
        t1 = " ".join(text1.lower().split())
        t2 = " ".join(text2.lower().split())

        # ä½¿ç”¨ SequenceMatcher è¨ˆç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, t1, t2).ratio()

        return similarity

    def _deduplicate_candidates(
        self,
        candidates: List[Dict],
        similarity_threshold: float = 0.80
    ) -> List[Dict]:
        """
        å»é™¤ç›¸ä¼¼åº¦éé«˜çš„å€™é¸ç­”æ¡ˆ

        ç­–ç•¥ï¼š
        1. ä¿ç•™ç¬¬ä¸€å€‹å€™é¸
        2. å°æ–¼å¾ŒçºŒå€™é¸ï¼Œåªæœ‰ç•¶å…¶èˆ‡æ‰€æœ‰å·²ä¿ç•™å€™é¸çš„ç›¸ä¼¼åº¦éƒ½ä½æ–¼é–¾å€¼æ™‚æ‰ä¿ç•™
        3. ç›¸ä¼¼åº¦é–¾å€¼é»˜èª 0.80 (80%)

        Args:
            candidates: å€™é¸åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é–¾å€¼ï¼Œè¶…éæ­¤å€¼è¦–ç‚ºé‡è¤‡

        Returns:
            List[Dict]: å»é‡å¾Œçš„å€™é¸åˆ—è¡¨
        """
        if len(candidates) <= 1:
            return candidates

        deduplicated = [candidates[0]]  # ä¿ç•™ç¬¬ä¸€å€‹å€™é¸

        for i, candidate in enumerate(candidates[1:], start=1):
            answer = candidate.get("answer", "")

            # æª¢æŸ¥èˆ‡æ‰€æœ‰å·²ä¿ç•™å€™é¸çš„ç›¸ä¼¼åº¦
            is_duplicate = False
            for kept_candidate in deduplicated:
                kept_answer = kept_candidate.get("answer", "")
                similarity = self._calculate_text_similarity(answer, kept_answer)

                if similarity >= similarity_threshold:
                    print(f"   âš ï¸  å€™é¸ #{i+1} èˆ‡å€™é¸ #{deduplicated.index(kept_candidate)+1} ç›¸ä¼¼åº¦éé«˜ ({similarity:.2%})ï¼Œå·²ç§»é™¤")
                    is_duplicate = True
                    break

            if not is_duplicate:
                deduplicated.append(candidate)
                print(f"   âœ… å€™é¸ #{i+1} å·²ä¿ç•™ï¼ˆèˆ‡ç¾æœ‰å€™é¸å·®ç•°è¶³å¤ ï¼‰")

        return deduplicated

    async def batch_generate(
        self,
        test_scenarios: List[Dict],
        max_concurrent: int = 3
    ) -> Dict[int, List[Dict]]:
        """
        æ‰¹é‡ç”ŸæˆçŸ¥è­˜å€™é¸ï¼ˆç”¨æ–¼å¤šå€‹æ¸¬è©¦æƒ…å¢ƒï¼‰

        Args:
            test_scenarios: æ¸¬è©¦æƒ…å¢ƒåˆ—è¡¨ [{"id": 1, "question": "...", "category": "..."}]
            max_concurrent: æœ€å¤§ä¸¦ç™¼æ•¸ï¼ˆé¿å… API é™æµï¼‰

        Returns:
            Dict[int, List[Dict]]: {scenario_id: [candidates]}
        """

        semaphore = asyncio.Semaphore(max_concurrent)

        async def generate_with_limit(scenario):
            async with semaphore:
                try:
                    candidates = await self.generate_knowledge_candidates(
                        test_question=scenario["question"],
                        intent_category=scenario["category"],
                        num_candidates=2  # æ‰¹é‡æ™‚æ¸›å°‘å€™é¸æ•¸
                    )
                    return scenario["id"], candidates
                except Exception as e:
                    print(f"âŒ ç‚ºæƒ…å¢ƒ #{scenario['id']} ç”Ÿæˆå¤±æ•—: {str(e)}")
                    return scenario["id"], []

        tasks = [generate_with_limit(s) for s in test_scenarios]
        results = await asyncio.gather(*tasks)

        return dict(results)


# å–®ä¾‹æ¨¡å¼ï¼ˆå¯é¸ï¼‰
_generator_instance = None


def get_knowledge_generator() -> KnowledgeGenerator:
    """ç²å–çŸ¥è­˜ç”Ÿæˆå™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹ï¼‰"""
    global _generator_instance
    if _generator_instance is None:
        _generator_instance = KnowledgeGenerator()
    return _generator_instance
