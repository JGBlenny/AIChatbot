"""
çŸ¥è­˜åŒ¯å…¥æœå‹™
çµ±ä¸€è™•ç†å„ç¨®æ ¼å¼çš„çŸ¥è­˜åŒ¯å…¥ï¼ŒåŒ…æ‹¬æª”æ¡ˆè§£æã€å‘é‡ç”Ÿæˆã€è³‡æ–™åº«å„²å­˜
"""
import os
import json
import asyncio
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import pandas as pd
import asyncpg
from asyncpg.pool import Pool
from openai import AsyncOpenAI
import time


class KnowledgeImportService:
    """çŸ¥è­˜åŒ¯å…¥æœå‹™"""

    def __init__(self, db_pool: Pool):
        """
        åˆå§‹åŒ–çŸ¥è­˜åŒ¯å…¥æœå‹™

        Args:
            db_pool: è³‡æ–™åº«é€£æ¥æ± 
        """
        self.db_pool = db_pool
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.embedding_model = "text-embedding-3-small"
        self.llm_model = "gpt-4o-mini"

    async def process_import_job(
        self,
        job_id: str,
        file_path: str,
        vendor_id: Optional[int],
        import_mode: str,
        enable_deduplication: bool,
        user_id: str = "admin"
    ) -> Dict:
        """
        è™•ç†çŸ¥è­˜åŒ¯å…¥ä½œæ¥­ï¼ˆä¸»è¦å…¥å£ï¼‰

        æ‰€æœ‰åŒ¯å…¥çš„çŸ¥è­˜éƒ½æœƒå…ˆé€²å…¥å¯©æ ¸ä½‡åˆ—ï¼Œ
        ç¶“éäººå·¥å¯©æ ¸é€šéå¾Œæ‰æœƒåŠ å…¥æ­£å¼çŸ¥è­˜åº«ã€‚

        Args:
            job_id: ä½œæ¥­ ID
            file_path: ä¸Šå‚³çš„æª”æ¡ˆè·¯å¾‘
            vendor_id: æ¥­è€… IDï¼ˆå¯é¸ï¼‰
            import_mode: åŒ¯å…¥æ¨¡å¼ï¼ˆappend/replace/mergeï¼‰
            enable_deduplication: æ˜¯å¦å•Ÿç”¨å»é‡
            user_id: åŸ·è¡Œè€… ID

        Returns:
            åŒ¯å…¥çµæœçµ±è¨ˆ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“¦ é–‹å§‹è™•ç†çŸ¥è­˜åŒ¯å…¥ä½œæ¥­")
        print(f"   ä½œæ¥­ ID: {job_id}")
        print(f"   æª”æ¡ˆ: {file_path}")
        print(f"   æ¥­è€…: {vendor_id or 'é€šç”¨çŸ¥è­˜'}")
        print(f"   æ¨¡å¼: {import_mode}")
        print(f"{'='*60}\n")

        try:
            # 1. æ›´æ–°ä½œæ¥­ç‹€æ…‹ç‚ºè™•ç†ä¸­
            await self.update_job_status(job_id, "processing", progress={"current": 0, "total": 100})

            # 2. åµæ¸¬æª”æ¡ˆé¡å‹ä¸¦é¸æ“‡è§£æå™¨
            file_type = self._detect_file_type(file_path)
            print(f"ğŸ“„ æª”æ¡ˆé¡å‹: {file_type}")

            # 3. è§£ææª”æ¡ˆ
            await self.update_job_status(job_id, "processing", progress={"current": 10, "total": 100, "stage": "è§£ææª”æ¡ˆ"})
            knowledge_list = await self._parse_file(file_path, file_type)

            if not knowledge_list:
                raise Exception("æœªèƒ½å¾æª”æ¡ˆä¸­æå–ä»»ä½•çŸ¥è­˜")

            print(f"âœ… è§£æå‡º {len(knowledge_list)} æ¢çŸ¥è­˜")

            # 4. é å…ˆå»é‡ï¼ˆæ–‡å­—å®Œå…¨ç›¸åŒï¼‰- åœ¨ LLM å‰åŸ·è¡Œï¼Œç¯€çœæˆæœ¬
            if enable_deduplication:
                await self.update_job_status(job_id, "processing", progress={"current": 20, "total": 100, "stage": "æ–‡å­—å»é‡"})
                original_count = len(knowledge_list)
                knowledge_list = await self._deduplicate_exact_match(knowledge_list)
                text_skipped = original_count - len(knowledge_list)
                print(f"ğŸ” æ–‡å­—å»é‡: è·³é {text_skipped} æ¢å®Œå…¨ç›¸åŒçš„é …ç›®ï¼Œå‰©é¤˜ {len(knowledge_list)} æ¢")

            # 5. ç”Ÿæˆå•é¡Œæ‘˜è¦ï¼ˆä½¿ç”¨ LLMï¼‰- åªè™•ç†å»é‡å¾Œçš„çŸ¥è­˜
            await self.update_job_status(job_id, "processing", progress={"current": 35, "total": 100, "stage": "ç”Ÿæˆå•é¡Œæ‘˜è¦"})
            await self._generate_question_summaries(knowledge_list)

            # 6. ç”Ÿæˆå‘é‡åµŒå…¥ - åªè™•ç†å»é‡å¾Œçš„çŸ¥è­˜
            await self.update_job_status(job_id, "processing", progress={"current": 55, "total": 100, "stage": "ç”Ÿæˆå‘é‡åµŒå…¥"})
            await self._generate_embeddings(knowledge_list)

            # 7. èªæ„å»é‡ï¼ˆä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦ï¼‰- äºŒæ¬¡éæ¿¾
            if enable_deduplication:
                await self.update_job_status(job_id, "processing", progress={"current": 70, "total": 100, "stage": "èªæ„å»é‡"})
                semantic_original = len(knowledge_list)
                knowledge_list = await self._deduplicate_by_similarity(knowledge_list)
                semantic_skipped = semantic_original - len(knowledge_list)
                print(f"ğŸ” èªæ„å»é‡: è·³é {semantic_skipped} æ¢èªæ„ç›¸ä¼¼çš„é …ç›®ï¼Œå‰©é¤˜ {len(knowledge_list)} æ¢")
                print(f"ğŸ“Š ç¸½è¨ˆè·³é: {text_skipped + semantic_skipped} æ¢ï¼ˆæ–‡å­—: {text_skipped}, èªæ„: {semantic_skipped}ï¼‰")

            # 8. æ¨è–¦æ„åœ–ï¼ˆä½¿ç”¨ LLM æˆ–åˆ†é¡å™¨ï¼‰
            await self.update_job_status(job_id, "processing", progress={"current": 76, "total": 100, "stage": "æ¨è–¦æ„åœ–"})
            await self._recommend_intents(knowledge_list)

            # 9. å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°ï¼ˆéœ€æ±‚ 2ï¼šé‡å° B2C çŸ¥è­˜ï¼‰
            await self.update_job_status(job_id, "processing", progress={"current": 78, "total": 100, "stage": "å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°"})
            test_scenario_count = await self._create_test_scenario_suggestions(knowledge_list, vendor_id)

            # 10. åŒ¯å…¥åˆ°å¯©æ ¸ä½‡åˆ—ï¼ˆéœ€æ±‚ 3ï¼šæ‰€æœ‰çŸ¥è­˜éƒ½éœ€è¦å¯©æ ¸ï¼‰
            # çŸ¥è­˜æœƒå…ˆé€²å…¥ ai_generated_knowledge_candidates è¡¨
            # äººå·¥å¯©æ ¸é€šéå¾Œæ‰æœƒåŠ å…¥æ­£å¼çš„ knowledge_base è¡¨
            await self.update_job_status(job_id, "processing", progress={"current": 85, "total": 100, "stage": "åŒ¯å…¥å¯©æ ¸ä½‡åˆ—"})
            result = await self._import_to_review_queue(
                knowledge_list,
                vendor_id=vendor_id,
                created_by=user_id
            )
            result['test_scenarios_created'] = test_scenario_count

            # 10. æ›´æ–°ä½œæ¥­ç‹€æ…‹ç‚ºå®Œæˆ
            await self.update_job_status(
                job_id,
                "completed",
                progress={"current": 100, "total": 100},
                result=result
            )

            print(f"\n{'='*60}")
            print(f"âœ… åŒ¯å…¥å®Œæˆï¼ˆå·²é€²å…¥å¯©æ ¸ä½‡åˆ—ï¼‰")
            print(f"   åŒ¯å…¥çŸ¥è­˜: {result['imported']} æ¢")
            print(f"   è·³é: {result.get('skipped', 0)} æ¢")
            print(f"   éŒ¯èª¤: {result.get('errors', 0)} æ¢")
            if result.get('test_scenarios_created', 0) > 0:
                print(f"   æ¸¬è©¦æƒ…å¢ƒå»ºè­°: {result['test_scenarios_created']} å€‹")
            print(f"   âš ï¸  æ‰€æœ‰çŸ¥è­˜éœ€ç¶“äººå·¥å¯©æ ¸å¾Œæ‰æœƒæ­£å¼åŠ å…¥çŸ¥è­˜åº«")
            print(f"{'='*60}\n")

            return result

        except Exception as e:
            # æ›´æ–°ä½œæ¥­ç‹€æ…‹ç‚ºå¤±æ•—
            error_message = str(e)
            print(f"\nâŒ åŒ¯å…¥å¤±æ•—: {error_message}")
            await self.update_job_status(
                job_id,
                "failed",
                error=error_message
            )
            raise

    def _detect_file_type(self, file_path: str) -> str:
        """
        åµæ¸¬æª”æ¡ˆé¡å‹

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘

        Returns:
            æª”æ¡ˆé¡å‹ï¼ˆexcel, pdf, txt, jsonï¼‰
        """
        suffix = Path(file_path).suffix.lower()

        if suffix in ['.xlsx', '.xls']:
            return 'excel'
        elif suffix == '.pdf':
            return 'pdf'
        elif suffix == '.txt':
            return 'txt'
        elif suffix == '.json':
            return 'json'
        else:
            return 'unknown'

    async def _parse_file(self, file_path: str, file_type: str) -> List[Dict]:
        """
        è§£ææª”æ¡ˆ

        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            file_type: æª”æ¡ˆé¡å‹

        Returns:
            çŸ¥è­˜åˆ—è¡¨
        """
        if file_type == 'excel':
            return await self._parse_excel(file_path)
        elif file_type == 'txt':
            return await self._parse_txt(file_path)
        elif file_type == 'json':
            return await self._parse_json(file_path)
        elif file_type == 'pdf':
            return await self._parse_pdf(file_path)
        else:
            raise Exception(f"ä¸æ”¯æ´çš„æª”æ¡ˆé¡å‹: {file_type}")

    async def _parse_excel(self, file_path: str) -> List[Dict]:
        """
        è§£æ Excel æª”æ¡ˆ

        æ”¯æ´æ ¼å¼ï¼š
        - æ¬„ä½: å•é¡Œ / question / å•é¡Œæ‘˜è¦
        - æ¬„ä½: ç­”æ¡ˆ / answer / å›è¦†
        - æ¬„ä½: åˆ†é¡ / category (å¯é¸)
        - æ¬„ä½: å°è±¡ / audience (å¯é¸)
        - æ¬„ä½: é—œéµå­— / keywords (å¯é¸)

        Args:
            file_path: Excel æª”æ¡ˆè·¯å¾‘

        Returns:
            çŸ¥è­˜åˆ—è¡¨
        """
        print(f"ğŸ“– è§£æ Excel æª”æ¡ˆ: {file_path}")

        df = pd.read_excel(file_path, engine='openpyxl')
        print(f"   è®€å– {len(df)} è¡Œè³‡æ–™")
        print(f"   æ¬„ä½: {list(df.columns)}")

        # æ¬„ä½æ˜ å°„ï¼ˆæ”¯æ´å¤šç¨®æ¬„ä½åç¨±ï¼‰
        question_cols = ['å•é¡Œ', 'question', 'å•é¡Œæ‘˜è¦', 'question_summary', 'title', 'æ¨™é¡Œ']
        answer_cols = ['ç­”æ¡ˆ', 'answer', 'å›è¦†', 'response', 'content', 'å…§å®¹']
        category_cols = ['åˆ†é¡', 'category', 'é¡åˆ¥', 'type']
        audience_cols = ['å°è±¡', 'audience', 'å—çœ¾']
        keywords_cols = ['é—œéµå­—', 'keywords', 'æ¨™ç±¤', 'tags']

        # æ‰¾åˆ°å°æ‡‰çš„æ¬„ä½
        question_col = next((col for col in df.columns if col in question_cols), None)
        answer_col = next((col for col in df.columns if col in answer_cols), None)
        category_col = next((col for col in df.columns if col in category_cols), None)
        audience_col = next((col for col in df.columns if col in audience_cols), None)
        keywords_col = next((col for col in df.columns if col in keywords_cols), None)

        if not answer_col:
            raise Exception(f"æ‰¾ä¸åˆ°ç­”æ¡ˆæ¬„ä½ã€‚æ”¯æ´çš„æ¬„ä½åç¨±: {', '.join(answer_cols)}")

        knowledge_list = []
        current_category = None

        for idx, row in df.iterrows():
            # å¦‚æœæœ‰åˆ†é¡æ¬„ä½ä¸”è©²è¡Œæœ‰å€¼ï¼Œæ›´æ–°ç•¶å‰åˆ†é¡
            if category_col and pd.notna(row[category_col]):
                potential_category = str(row[category_col]).strip()
                # éæ¿¾æ‰éåˆ†é¡çš„æè¿°æ€§æ–‡å­—
                if potential_category and len(potential_category) < 50:
                    current_category = potential_category

            # è§£æç­”æ¡ˆï¼ˆå¿…å¡«ï¼‰
            answer = row.get(answer_col)
            if pd.isna(answer) or not str(answer).strip() or len(str(answer).strip()) < 10:
                continue

            answer = str(answer).strip()

            # è§£æå•é¡Œï¼ˆå¯é¸ï¼Œå¦‚æœæ²’æœ‰æœƒç”¨ LLM ç”Ÿæˆï¼‰
            question = None
            if question_col and pd.notna(row[question_col]):
                question = str(row[question_col]).strip()

            # è§£æå°è±¡
            audience = 'ç§Ÿå®¢'  # é è¨­
            if audience_col and pd.notna(row[audience_col]):
                audience = str(row[audience_col]).strip()

            # è§£æé—œéµå­—
            keywords = []
            if keywords_col and pd.notna(row[keywords_col]):
                keywords_str = str(row[keywords_col])
                keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]

            knowledge_list.append({
                'question_summary': question,  # å¯èƒ½ç‚º Noneï¼Œå¾ŒçºŒç”¨ LLM ç”Ÿæˆ
                'answer': answer,
                'category': current_category or 'ä¸€èˆ¬å•é¡Œ',
                'audience': audience,
                'keywords': keywords,
                'source_file': Path(file_path).name
            })

        print(f"   âœ… è§£æå‡º {len(knowledge_list)} å€‹æœ‰æ•ˆçŸ¥è­˜é …ç›®")
        return knowledge_list

    async def _parse_txt(self, file_path: str) -> List[Dict]:
        """
        è§£æç´”æ–‡å­—æª”æ¡ˆï¼ˆä½¿ç”¨ LLM æå–çŸ¥è­˜ï¼‰

        Args:
            file_path: æ–‡å­—æª”æ¡ˆè·¯å¾‘

        Returns:
            çŸ¥è­˜åˆ—è¡¨
        """
        print(f"ğŸ“– è§£æ TXT æª”æ¡ˆ: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        if len(content) < 50:
            raise Exception("æª”æ¡ˆå…§å®¹éçŸ­ï¼Œç„¡æ³•æå–çŸ¥è­˜")

        # ä½¿ç”¨ LLM æå–çŸ¥è­˜
        print("ğŸ¤– ä½¿ç”¨ LLM æå–çŸ¥è­˜...")

        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„çŸ¥è­˜åº«åˆ†æå¸«ã€‚
å¾æä¾›çš„æ–‡å­—å…§å®¹ä¸­æå–å®¢æœå•ç­”çŸ¥è­˜ã€‚

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼š
{
  "knowledge_list": [
    {
      "question_summary": "å•é¡Œæ‘˜è¦",
      "answer": "å®Œæ•´ç­”æ¡ˆ",
      "category": "åˆ†é¡",
      "audience": "ç§Ÿå®¢|æˆ¿æ±|ç®¡ç†å¸«",
      "keywords": ["é—œéµå­—1", "é—œéµå­—2"]
    }
  ]
}

æ³¨æ„ï¼š
- åªæå–æ¸…æ™°ã€å®Œæ•´çš„çŸ¥è­˜
- å•é¡Œæ‘˜è¦è¦ç°¡æ½”ï¼ˆ15å­—ä»¥å…§ï¼‰
- ç­”æ¡ˆè¦å®Œæ•´ä¸”å¯¦ç”¨
- é¿å…åŒ…å«ç§äººè³‡è¨Š
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model=self.llm_model,
                temperature=0.3,
                response_format={"type": "json_object"},
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"è«‹å¾ä»¥ä¸‹å…§å®¹æå–çŸ¥è­˜ï¼š\n\n{content[:4000]}"}
                ]
            )

            result = json.loads(response.choices[0].message.content)
            knowledge_list = result.get('knowledge_list', [])

            # æ·»åŠ ä¾†æºè³‡è¨Š
            for knowledge in knowledge_list:
                knowledge['source_file'] = Path(file_path).name

            print(f"   âœ… æå–å‡º {len(knowledge_list)} å€‹çŸ¥è­˜é …ç›®")
            return knowledge_list

        except Exception as e:
            print(f"   âš ï¸ LLM æå–å¤±æ•—: {e}")
            raise Exception(f"ç„¡æ³•å¾æ–‡å­—æª”æ¡ˆæå–çŸ¥è­˜: {e}")

    async def _parse_json(self, file_path: str) -> List[Dict]:
        """
        è§£æ JSON æª”æ¡ˆ

        é æœŸæ ¼å¼ï¼š
        {
          "knowledge": [
            {
              "question": "å•é¡Œ",
              "answer": "ç­”æ¡ˆ",
              "category": "åˆ†é¡",
              "audience": "å°è±¡",
              "keywords": ["é—œéµå­—"]
            }
          ]
        }

        Args:
            file_path: JSON æª”æ¡ˆè·¯å¾‘

        Returns:
            çŸ¥è­˜åˆ—è¡¨
        """
        print(f"ğŸ“– è§£æ JSON æª”æ¡ˆ: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # æ”¯æ´å¤šç¨®æ ¼å¼
        if 'knowledge' in data:
            raw_list = data['knowledge']
        elif 'knowledge_list' in data:
            raw_list = data['knowledge_list']
        elif isinstance(data, list):
            raw_list = data
        else:
            raise Exception("ç„¡æ³•è­˜åˆ¥ JSON æ ¼å¼ã€‚é æœŸåŒ…å« 'knowledge' æˆ– 'knowledge_list' æ¬„ä½")

        knowledge_list = []
        for item in raw_list:
            # æ¬„ä½æ˜ å°„
            question = item.get('question') or item.get('question_summary') or item.get('title')
            answer = item.get('answer') or item.get('response') or item.get('content')

            if not answer or len(str(answer).strip()) < 10:
                continue

            knowledge_list.append({
                'question_summary': question,
                'answer': str(answer).strip(),
                'category': item.get('category', 'ä¸€èˆ¬å•é¡Œ'),
                'audience': item.get('audience', 'ç§Ÿå®¢'),
                'keywords': item.get('keywords', []),
                'source_file': Path(file_path).name
            })

        print(f"   âœ… è§£æå‡º {len(knowledge_list)} å€‹çŸ¥è­˜é …ç›®")
        return knowledge_list

    async def _parse_pdf(self, file_path: str) -> List[Dict]:
        """
        è§£æ PDF æª”æ¡ˆ

        Args:
            file_path: PDF æª”æ¡ˆè·¯å¾‘

        Returns:
            çŸ¥è­˜åˆ—è¡¨
        """
        # TODO: å¯¦ä½œ PDF è§£æï¼ˆéœ€è¦å®‰è£ PyPDF2 æˆ– pdfplumberï¼‰
        raise Exception("PDF æ ¼å¼æš«ä¸æ”¯æ´ï¼Œè«‹å…ˆè½‰æ›ç‚º Excel æˆ– TXT")

    async def _generate_question_summaries(self, knowledge_list: List[Dict]):
        """
        ç‚ºæ²’æœ‰å•é¡Œæ‘˜è¦çš„çŸ¥è­˜ç”Ÿæˆå•é¡Œ

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨ï¼ˆæœƒç›´æ¥ä¿®æ”¹ï¼‰
        """
        need_generation = [k for k in knowledge_list if not k.get('question_summary')]

        if not need_generation:
            print("âœ… æ‰€æœ‰çŸ¥è­˜éƒ½å·²æœ‰å•é¡Œæ‘˜è¦")
            return

        print(f"ğŸ¤– ç‚º {len(need_generation)} æ¢çŸ¥è­˜ç”Ÿæˆå•é¡Œæ‘˜è¦...")

        for knowledge in need_generation:
            try:
                prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç­”æ¡ˆï¼Œç”Ÿæˆä¸€å€‹ç°¡æ½”çš„å•é¡Œæ‘˜è¦ï¼ˆ15å­—ä»¥å…§ï¼‰ã€‚

åˆ†é¡ï¼š{knowledge['category']}
ç­”æ¡ˆï¼š{knowledge['answer'][:200]}

åªè¼¸å‡ºå•é¡Œæ‘˜è¦ï¼Œä¸è¦åŠ å…¶ä»–èªªæ˜ã€‚"""

                response = await self.openai_client.chat.completions.create(
                    model=self.llm_model,
                    temperature=0.3,
                    max_tokens=50,
                    messages=[{"role": "user", "content": prompt}]
                )

                question_summary = response.choices[0].message.content.strip()
                knowledge['question_summary'] = question_summary

                # é¿å… rate limit
                await asyncio.sleep(0.1)

            except Exception as e:
                print(f"   âš ï¸ ç”Ÿæˆå•é¡Œå¤±æ•—: {e}")
                # å‚™ç”¨æ–¹æ¡ˆ
                knowledge['question_summary'] = f"{knowledge['category']}ç›¸é—œå•é¡Œ"

        print(f"   âœ… å•é¡Œæ‘˜è¦ç”Ÿæˆå®Œæˆ")

    async def _generate_embeddings(self, knowledge_list: List[Dict]):
        """
        ç‚ºçŸ¥è­˜ç”Ÿæˆå‘é‡åµŒå…¥

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨ï¼ˆæœƒç›´æ¥ä¿®æ”¹ï¼‰
        """
        print(f"ğŸ”® ç‚º {len(knowledge_list)} æ¢çŸ¥è­˜ç”Ÿæˆå‘é‡åµŒå…¥...")

        for idx, knowledge in enumerate(knowledge_list, 1):
            try:
                # çµ„åˆæ–‡å­—ï¼ˆå•é¡Œ + ç­”æ¡ˆå‰æ®µï¼‰
                text = f"{knowledge['question_summary']} {knowledge['answer'][:200]}"

                response = await self.openai_client.embeddings.create(
                    model=self.embedding_model,
                    input=text
                )

                knowledge['embedding'] = response.data[0].embedding

                if idx % 10 == 0:
                    print(f"   é€²åº¦: {idx}/{len(knowledge_list)}")

                # é¿å… rate limit
                await asyncio.sleep(0.05)

            except Exception as e:
                print(f"   âš ï¸ ç”Ÿæˆå‘é‡å¤±æ•— (ç¬¬ {idx} æ¢): {e}")
                raise Exception(f"å‘é‡ç”Ÿæˆå¤±æ•—: {e}")

        print(f"   âœ… å‘é‡åµŒå…¥ç”Ÿæˆå®Œæˆ")

    async def _deduplicate_exact_match(self, knowledge_list: List[Dict]) -> List[Dict]:
        """
        å»é™¤æ–‡å­—å®Œå…¨ç›¸åŒçš„çŸ¥è­˜ï¼ˆç²¾ç¢ºåŒ¹é…ï¼‰
        åœ¨ LLM å‰åŸ·è¡Œï¼Œç¯€çœ OpenAI token æˆæœ¬

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨

        Returns:
            å»é‡å¾Œçš„çŸ¥è­˜åˆ—è¡¨
        """
        print(f"ğŸ” åŸ·è¡Œæ–‡å­—å»é‡ï¼ˆç²¾ç¢ºåŒ¹é…ï¼‰...")

        async with self.db_pool.acquire() as conn:
            unique_list = []

            for knowledge in knowledge_list:
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçš„å•é¡Œå’Œç­”æ¡ˆï¼ˆåŒæ™‚æª¢æŸ¥æ­£å¼çŸ¥è­˜åº«ã€å¯©æ ¸ä½‡åˆ—å’Œæ¸¬è©¦æƒ…å¢ƒï¼‰
                exists = await conn.fetchval("""
                    SELECT COUNT(*) FROM (
                        SELECT 1 FROM knowledge_base
                        WHERE question_summary = $1 AND answer = $2
                        UNION ALL
                        SELECT 1 FROM ai_generated_knowledge_candidates
                        WHERE question = $1 AND generated_answer = $2
                        UNION ALL
                        SELECT 1 FROM test_scenarios
                        WHERE test_question = $1
                    ) AS combined
                """, knowledge.get('question_summary'), knowledge['answer'])

                if exists == 0:
                    unique_list.append(knowledge)
                else:
                    print(f"   è·³éé‡è¤‡: {knowledge.get('question_summary', 'ç„¡å•é¡Œ')[:50]}...")

        return unique_list

    async def _deduplicate_by_similarity(
        self,
        knowledge_list: List[Dict],
        threshold: float = 0.85
    ) -> List[Dict]:
        """
        ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦å»é‡ï¼ˆèªæ„å»é‡ï¼‰
        æª¢æŸ¥çŸ¥è­˜åº«ã€å¯©æ ¸ä½‡åˆ—å’Œæ¸¬è©¦æƒ…å¢ƒä¸­æ˜¯å¦å·²æœ‰èªæ„ç›¸ä¼¼çš„çŸ¥è­˜

        é‡ç”¨ unclear_questions çš„ç›¸ä¼¼åº¦æ©Ÿåˆ¶ï¼š
        - é–¾å€¼ï¼š0.85ï¼ˆèˆ‡ unclear_questions ä¸€è‡´ï¼‰
        - ä½¿ç”¨è³‡æ–™åº«å‡½æ•¸ check_knowledge_exists_by_similarity()

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨ï¼ˆå¿…é ˆå·²æœ‰ embeddingï¼‰
            threshold: ç›¸ä¼¼åº¦é–¾å€¼ï¼ˆé è¨­ 0.85ï¼‰

        Returns:
            å»é‡å¾Œçš„çŸ¥è­˜åˆ—è¡¨
        """
        print(f"ğŸ” åŸ·è¡Œèªæ„å»é‡ï¼ˆç›¸ä¼¼åº¦é–¾å€¼: {threshold}ï¼‰...")

        unique_list = []

        async with self.db_pool.acquire() as conn:
            for idx, knowledge in enumerate(knowledge_list, 1):
                embedding = knowledge.get('embedding')

                if not embedding:
                    print(f"   âš ï¸  çŸ¥è­˜ç¼ºå°‘ embeddingï¼Œè·³éèªæ„æª¢æŸ¥: {knowledge.get('question_summary', 'ç„¡å•é¡Œ')[:50]}")
                    unique_list.append(knowledge)
                    continue

                # å°‡ embedding è½‰æ›ç‚º PostgreSQL vector æ ¼å¼
                vector_str = '[' + ','.join(str(x) for x in embedding) + ']'

                # ä½¿ç”¨è³‡æ–™åº«å‡½æ•¸æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼çŸ¥è­˜
                result = await conn.fetchrow("""
                    SELECT * FROM check_knowledge_exists_by_similarity($1::vector, $2)
                """, vector_str, threshold)

                if result and (result['exists_in_knowledge_base'] or result['exists_in_review_queue'] or result.get('exists_in_test_scenarios', False)):
                    # æ‰¾åˆ°ç›¸ä¼¼çŸ¥è­˜ï¼Œè·³é
                    source = result['source_table']
                    matched_q = result['matched_question']
                    sim_score = result['similarity_score']

                    print(f"   è·³éèªæ„ç›¸ä¼¼ (ç›¸ä¼¼åº¦: {sim_score:.4f}, ä¾†æº: {source})")
                    print(f"      æ–°å•é¡Œ: {knowledge['question_summary'][:50]}...")
                    print(f"      ç›¸ä¼¼å•é¡Œ: {matched_q[:50]}...")
                else:
                    # æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼çŸ¥è­˜ï¼Œä¿ç•™
                    unique_list.append(knowledge)

                # æ¯è™•ç† 10 æ¢é¡¯ç¤ºé€²åº¦
                if idx % 10 == 0:
                    print(f"   é€²åº¦: {idx}/{len(knowledge_list)}")

        return unique_list

    async def _recommend_intents(self, knowledge_list: List[Dict]):
        """
        ç‚ºçŸ¥è­˜æ¨è–¦åˆé©çš„æ„åœ–

        ä½¿ç”¨ LLM æ ¹æ“šå•é¡Œå’Œç­”æ¡ˆå…§å®¹æ¨è–¦æœ€åˆé©çš„æ„åœ–
        æ¨è–¦çµæœå„²å­˜åˆ° knowledge['recommended_intent']

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨ï¼ˆæœƒç›´æ¥ä¿®æ”¹ï¼‰
        """
        print(f"ğŸ¯ ç‚º {len(knowledge_list)} æ¢çŸ¥è­˜æ¨è–¦æ„åœ–...")

        # 1. å–å¾—æ‰€æœ‰å¯ç”¨çš„æ„åœ–
        async with self.db_pool.acquire() as conn:
            intents = await conn.fetch("""
                SELECT id, name, description
                FROM intents
                ORDER BY id
            """)

        if not intents:
            print("   âš ï¸  æ‰¾ä¸åˆ°ä»»ä½•æ„åœ–ï¼Œè·³éæ¨è–¦")
            return

        # å»ºç«‹æ„åœ–æ¸…å–®æ–‡å­—
        intent_list = "\n".join([
            f"- {intent['id']}: {intent['name']} ({intent['description']})"
            for intent in intents
        ])

        # 2. ç‚ºæ¯æ¢çŸ¥è­˜æ¨è–¦æ„åœ–
        for idx, knowledge in enumerate(knowledge_list, 1):
            try:
                prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹å•ç­”å…§å®¹ï¼Œå¾æ„åœ–æ¸…å–®ä¸­é¸æ“‡æœ€åˆé©çš„æ„åœ–ã€‚

å•é¡Œï¼š{knowledge['question_summary']}
ç­”æ¡ˆï¼š{knowledge['answer'][:200]}
åˆ†é¡ï¼š{knowledge.get('category', 'æœªåˆ†é¡')}

å¯ç”¨çš„æ„åœ–æ¸…å–®ï¼š
{intent_list}

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "intent_id": æ¨è–¦çš„æ„åœ– IDï¼ˆæ•¸å­—ï¼‰,
  "intent_name": æ„åœ–åç¨±,
  "confidence": ä¿¡å¿ƒåº¦ï¼ˆ0.0-1.0ï¼‰,
  "reasoning": æ¨è–¦ç†ç”±ï¼ˆç°¡çŸ­èªªæ˜ï¼‰
}}

åªè¼¸å‡º JSONï¼Œä¸è¦åŠ å…¶ä»–èªªæ˜ã€‚"""

                response = await self.openai_client.chat.completions.create(
                    model=self.llm_model,
                    temperature=0.3,
                    response_format={"type": "json_object"},
                    messages=[{"role": "user", "content": prompt}]
                )

                result = json.loads(response.choices[0].message.content)

                # å„²å­˜æ¨è–¦çµæœ
                knowledge['recommended_intent'] = {
                    'intent_id': result.get('intent_id'),
                    'intent_name': result.get('intent_name'),
                    'confidence': result.get('confidence', 0.8),
                    'reasoning': result.get('reasoning', '')
                }

                if idx <= 3:  # åªé¡¯ç¤ºå‰ 3 æ¢çš„æ¨è–¦
                    print(f"   âœ… {knowledge['question_summary'][:40]}... â†’ {result.get('intent_name')} (ä¿¡å¿ƒåº¦: {result.get('confidence', 0):.2f})")

                # é¿å… rate limit
                await asyncio.sleep(0.1)

            except Exception as e:
                print(f"   âš ï¸  æ„åœ–æ¨è–¦å¤±æ•— (ç¬¬ {idx} æ¢): {e}")
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨é è¨­æ„åœ–
                knowledge['recommended_intent'] = {
                    'intent_id': 4,  # æœå‹™èªªæ˜
                    'intent_name': 'æœå‹™èªªæ˜',
                    'confidence': 0.5,
                    'reasoning': 'ç„¡æ³•è‡ªå‹•æ¨è–¦ï¼Œä½¿ç”¨é è¨­æ„åœ–'
                }

        print(f"   âœ… æ„åœ–æ¨è–¦å®Œæˆ")

    async def _clear_vendor_knowledge(self, vendor_id: int):
        """
        æ¸…é™¤æ¥­è€…çš„ç¾æœ‰çŸ¥è­˜ï¼ˆç”¨æ–¼ replace æ¨¡å¼ï¼‰

        Args:
            vendor_id: æ¥­è€… ID
        """
        print(f"ğŸ—‘ï¸  æ¸…é™¤æ¥­è€… {vendor_id} çš„ç¾æœ‰çŸ¥è­˜...")

        async with self.db_pool.acquire() as conn:
            deleted_count = await conn.fetchval("""
                DELETE FROM knowledge_base
                WHERE vendor_id = $1
                RETURNING COUNT(*)
            """, vendor_id)

            print(f"   âœ… å·²åˆªé™¤ {deleted_count or 0} æ¢èˆŠçŸ¥è­˜")

    async def _import_to_database(
        self,
        knowledge_list: List[Dict],
        vendor_id: Optional[int],
        import_mode: str,
        created_by: str
    ) -> Dict:
        """
        åŒ¯å…¥çŸ¥è­˜åˆ°è³‡æ–™åº«

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨
            vendor_id: æ¥­è€… ID
            import_mode: åŒ¯å…¥æ¨¡å¼
            created_by: å»ºç«‹è€…

        Returns:
            åŒ¯å…¥çµæœçµ±è¨ˆ
        """
        print(f"ğŸ’¾ åŒ¯å…¥ {len(knowledge_list)} æ¢çŸ¥è­˜åˆ°è³‡æ–™åº«...")

        imported = 0
        skipped = 0
        errors = 0

        async with self.db_pool.acquire() as conn:
            # å–å¾—é è¨­æ„åœ– ID
            default_intent_id = await conn.fetchval("""
                SELECT id FROM intents
                WHERE name IN ('ä¸€èˆ¬çŸ¥è­˜', 'å…¶ä»–', 'general')
                ORDER BY id
                LIMIT 1
            """)

            if not default_intent_id:
                print("âš ï¸ æ‰¾ä¸åˆ°é è¨­æ„åœ–ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹æ„åœ–")
                default_intent_id = await conn.fetchval("SELECT id FROM intents ORDER BY id LIMIT 1")

            for idx, knowledge in enumerate(knowledge_list, 1):
                try:
                    await conn.execute("""
                        INSERT INTO knowledge_base (
                            intent_id,
                            vendor_id,
                            title,
                            category,
                            question_summary,
                            answer,
                            audience,
                            keywords,
                            source_file,
                            source_date,
                            embedding,
                            scope,
                            priority,
                            created_by,
                            created_at,
                            updated_at
                        ) VALUES (
                            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14,
                            CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
                        )
                    """,
                        default_intent_id,
                        vendor_id,
                        knowledge['question_summary'],  # title
                        knowledge['category'],
                        knowledge['question_summary'],
                        knowledge['answer'],
                        knowledge['audience'],
                        knowledge['keywords'],
                        knowledge['source_file'],
                        datetime.now().date(),
                        knowledge['embedding'],
                        'global' if not vendor_id else 'vendor',
                        0,  # priority
                        created_by
                    )

                    imported += 1

                    if idx % 10 == 0:
                        print(f"   é€²åº¦: {idx}/{len(knowledge_list)}")

                except Exception as e:
                    print(f"   âš ï¸ åŒ¯å…¥å¤±æ•— (ç¬¬ {idx} æ¢): {e}")
                    errors += 1

        return {
            "imported": imported,
            "skipped": skipped,
            "errors": errors,
            "total": len(knowledge_list)
        }

    async def _create_test_scenario_suggestions(
        self,
        knowledge_list: List[Dict],
        vendor_id: Optional[int]
    ) -> int:
        """
        ç‚º B2C å°è±¡çš„çŸ¥è­˜å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°ï¼ˆéœ€æ±‚ 2ï¼‰

        B2C å°è±¡åŒ…æ‹¬ï¼šç§Ÿå®¢ã€æˆ¿æ±ï¼ˆæ‰€æœ‰å¤–éƒ¨æ¥­å‹™ç¯„åœï¼‰

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨
            vendor_id: æ¥­è€… ID

        Returns:
            å»ºç«‹çš„æ¸¬è©¦æƒ…å¢ƒæ•¸é‡
        """
        print(f"ğŸ§ª æª¢æŸ¥ B2C çŸ¥è­˜ä¸¦å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°...")

        # B2C å°è±¡åˆ—è¡¨
        b2c_audiences = ['ç§Ÿå®¢', 'æˆ¿æ±', 'tenant', 'landlord']

        created_count = 0

        async with self.db_pool.acquire() as conn:
            for knowledge in knowledge_list:
                audience = knowledge.get('audience', '').lower()

                # æª¢æŸ¥æ˜¯å¦ç‚º B2C å°è±¡
                if not any(b2c in audience.lower() for b2c in b2c_audiences):
                    continue

                question = knowledge['question_summary']

                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸ä¼¼çš„æ¸¬è©¦æƒ…å¢ƒ
                exists = await conn.fetchval("""
                    SELECT COUNT(*)
                    FROM test_scenarios
                    WHERE test_question = $1
                """, question)

                if exists > 0:
                    continue

                # å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°
                try:
                    await conn.execute("""
                        INSERT INTO test_scenarios (
                            test_question,
                            expected_category,
                            difficulty,
                            status,
                            source,
                            created_at
                        ) VALUES ($1, $2, $3, $4, $5, CURRENT_TIMESTAMP)
                    """,
                        question,
                        knowledge.get('category', 'ä¸€èˆ¬å•é¡Œ'),
                        'medium',  # é è¨­é›£åº¦
                        'pending_review',  # å¾…å¯©æ ¸ç‹€æ…‹
                        'imported'
                    )

                    created_count += 1

                except Exception as e:
                    print(f"   âš ï¸ å»ºç«‹æ¸¬è©¦æƒ…å¢ƒå¤±æ•—: {e}")

        if created_count > 0:
            print(f"   âœ… å»ºç«‹äº† {created_count} å€‹æ¸¬è©¦æƒ…å¢ƒå»ºè­°")
        else:
            print(f"   â„¹ï¸ æ²’æœ‰éœ€è¦å»ºç«‹æ¸¬è©¦æƒ…å¢ƒçš„ B2C çŸ¥è­˜")

        return created_count

    async def _import_to_review_queue(
        self,
        knowledge_list: List[Dict],
        vendor_id: Optional[int],
        created_by: str
    ) -> Dict:
        """
        å°‡çŸ¥è­˜åŒ¯å…¥åˆ°å¯©æ ¸ä½‡åˆ—ï¼ˆéœ€æ±‚ 3ï¼šæ··åˆæ¨¡å¼ï¼‰

        çŸ¥è­˜æœƒå…ˆé€²å…¥ ai_generated_knowledge_candidates è¡¨ï¼Œ
        äººå·¥å¯©æ ¸é€šéå¾Œæ‰æœƒåŠ å…¥æ­£å¼çš„ knowledge_base

        Args:
            knowledge_list: çŸ¥è­˜åˆ—è¡¨
            vendor_id: æ¥­è€… ID
            created_by: å»ºç«‹è€…

        Returns:
            åŒ¯å…¥çµæœçµ±è¨ˆ
        """
        print(f"ğŸ“‹ å°‡ {len(knowledge_list)} æ¢çŸ¥è­˜åŒ¯å…¥å¯©æ ¸ä½‡åˆ—...")

        imported = 0
        errors = 0

        async with self.db_pool.acquire() as conn:
            for idx, knowledge in enumerate(knowledge_list, 1):
                try:
                    question = knowledge['question_summary']
                    answer = knowledge['answer']

                    # 1. å…ˆæª¢æŸ¥æˆ–å»ºç«‹å°æ‡‰çš„æ¸¬è©¦æƒ…å¢ƒ
                    test_scenario_id = await conn.fetchval("""
                        SELECT id FROM test_scenarios
                        WHERE test_question = $1
                        ORDER BY created_at DESC
                        LIMIT 1
                    """, question)

                    # å¦‚æœæ²’æœ‰æ¸¬è©¦æƒ…å¢ƒï¼Œå…ˆå»ºç«‹ä¸€å€‹
                    if not test_scenario_id:
                        test_scenario_id = await conn.fetchval("""
                            INSERT INTO test_scenarios (
                                test_question,
                                expected_category,
                                difficulty,
                                status,
                                source,
                                created_at
                            ) VALUES ($1, $2, $3, $4, $5, CURRENT_TIMESTAMP)
                            RETURNING id
                        """,
                            question,
                            knowledge.get('category', 'ä¸€èˆ¬å•é¡Œ'),
                            'medium',
                            'pending_review',
                            'imported'
                        )

                    # 2. æº–å‚™ generation_reasoningï¼ˆåŒ…å«æ„åœ–æ¨è–¦ï¼‰
                    recommended_intent = knowledge.get('recommended_intent', {})
                    reasoning = f"""åˆ†é¡: {knowledge.get('category')}, å°è±¡: {knowledge.get('audience')}, é—œéµå­—: {', '.join(knowledge.get('keywords', []))}

ã€æ¨è–¦æ„åœ–ã€‘
æ„åœ– ID: {recommended_intent.get('intent_id', 'æœªæ¨è–¦')}
æ„åœ–åç¨±: {recommended_intent.get('intent_name', 'æœªæ¨è–¦')}
ä¿¡å¿ƒåº¦: {recommended_intent.get('confidence', 0)}
æ¨è–¦ç†ç”±: {recommended_intent.get('reasoning', 'ç„¡')}"""

                    # 3. å°‡ embedding è½‰æ›ç‚º PostgreSQL vector æ ¼å¼
                    embedding = knowledge.get('embedding')
                    embedding_str = None
                    if embedding:
                        embedding_str = '[' + ','.join(str(x) for x in embedding) + ']'

                    # 4. å»ºç«‹çŸ¥è­˜å€™é¸è¨˜éŒ„ï¼ˆå« embeddingï¼‰
                    await conn.execute("""
                        INSERT INTO ai_generated_knowledge_candidates (
                            test_scenario_id,
                            question,
                            generated_answer,
                            question_embedding,
                            confidence_score,
                            generation_prompt,
                            ai_model,
                            generation_reasoning,
                            suggested_sources,
                            warnings,
                            status,
                            created_at,
                            updated_at
                        ) VALUES ($1, $2, $3, $4::vector, $5, $6, $7, $8, $9, $10, $11, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP)
                    """,
                        test_scenario_id,
                        question,
                        answer,
                        embedding_str,  # å‘é‡åµŒå…¥ï¼ˆå­—ä¸²æ ¼å¼ï¼‰
                        0.95,  # åŒ¯å…¥çš„çŸ¥è­˜çµ¦äºˆè¼ƒé«˜çš„ä¿¡å¿ƒåˆ†æ•¸
                        f"å¾æª”æ¡ˆåŒ¯å…¥: {knowledge.get('source_file', 'unknown')}",
                        'knowledge_import',  # æ¨™è¨˜ç‚ºçŸ¥è­˜åŒ¯å…¥ä¾†æº
                        reasoning,  # åŒ…å«æ¨è–¦æ„åœ–çš„è©³ç´°è³‡è¨Š
                        [knowledge.get('source_file', 'imported_file')],
                        [],  # ç„¡è­¦å‘Š
                        'pending_review'  # å¾…å¯©æ ¸ç‹€æ…‹
                    )

                    imported += 1

                    if idx % 10 == 0:
                        print(f"   é€²åº¦: {idx}/{len(knowledge_list)}")

                except Exception as e:
                    print(f"   âš ï¸ åŒ¯å…¥åˆ°å¯©æ ¸ä½‡åˆ—å¤±æ•— (ç¬¬ {idx} æ¢): {e}")
                    errors += 1

        return {
            "imported": imported,
            "skipped": 0,
            "errors": errors,
            "total": len(knowledge_list),
            "mode": "review_queue"
        }

    async def update_job_status(
        self,
        job_id: str,
        status: str,
        progress: Optional[Dict] = None,
        result: Optional[Dict] = None,
        error: Optional[str] = None
    ):
        """
        æ›´æ–°ä½œæ¥­ç‹€æ…‹

        æ³¨æ„ï¼šjob è¨˜éŒ„å¿…é ˆåœ¨å‘¼å«æ­¤æ–¹æ³•å‰å·²å­˜åœ¨ï¼ˆç”±ä¸Šå‚³ç«¯é»å»ºç«‹ï¼‰

        Args:
            job_id: ä½œæ¥­ ID
            status: ç‹€æ…‹ï¼ˆprocessing/completed/failedï¼‰
            progress: é€²åº¦è³‡è¨Š
            result: çµæœçµ±è¨ˆ
            error: éŒ¯èª¤è¨Šæ¯
        """
        async with self.db_pool.acquire() as conn:
            import uuid
            # æ›´æ–°ä½œæ¥­ç‹€æ…‹ï¼ˆå‡è¨­ job è¨˜éŒ„å·²å­˜åœ¨ï¼‰
            # å…ˆé€²è¡ŒåŸºæœ¬æ›´æ–°
            updated = await conn.fetchval("""
                UPDATE knowledge_import_jobs
                SET status = $1::varchar,
                    progress = $2::jsonb,
                    result = $3::jsonb,
                    error_message = $4,
                    updated_at = CURRENT_TIMESTAMP,
                    completed_at = CASE WHEN $1::varchar IN ('completed', 'failed')
                                        THEN CURRENT_TIMESTAMP
                                        ELSE completed_at
                                   END
                WHERE job_id = $5
                RETURNING job_id
            """, status, json.dumps(progress) if progress else None,
                json.dumps(result) if result else None, error, uuid.UUID(job_id))

            if not updated:
                raise Exception(f"Job {job_id} not found in database")

            # å¦‚æœæœ‰ result ä¸”ç‹€æ…‹æ˜¯ completedï¼Œæ›´æ–°è¨ˆæ•¸æ¬„ä½
            if result and status == 'completed':
                await conn.execute("""
                    UPDATE knowledge_import_jobs
                    SET imported_count = ($1::jsonb->>'imported')::integer,
                        skipped_count = ($1::jsonb->>'skipped')::integer,
                        error_count = ($1::jsonb->>'errors')::integer
                    WHERE job_id = $2
                """, json.dumps(result), uuid.UUID(job_id))
